[
    {
        "id": 1,
        "instruction": "Quantifying and Mitigating the Impact of Label Errors on Model Disparity Metrics: Errors in labels obtained via human annotation adversely affect a trained model's performance. Existing approaches propose ways to mitigate the effect of label error on a model's downstream accuracy, yet little is known about its impact on a model's group-based disparity metrics\\footnote{Group-based disparity metrics like subgroup calibration, false positive rate, false negative rate, equalized odds, and equal opportunity are more often known, colloquially, as \\textit{fairness metrics} in the literature. We use the term group-based disparity metrics in this work.}. Here we study the effect of label error on a model's group-based disparity metrics like group calibration. We empirically characterize how varying levels of label error, in both training and test data, affect these disparity metrics. We find that group calibration and other metrics are sensitive to train-time and test-time label error---particularly for minority groups. For the same level of label error, the percentage change in group calibration error for the minority group is on average 1.5 times larger than the change for the majority group. Towards mitigating the impact of training-time label error, we present an approach to estimate how changing a single training input's label affects a model's group disparity metric on a test set. We empirically assess the proposed approach on a variety of datasets and find a 10-40\\% improvement, compared to alternative approaches, in identifying training inputs that improve a model's disparity metric. The proposed approach can help surface training inputs that may need to be corrected for improving a model's group-based disparity metrics.",
        "reference": "This paper studies the effect of label error on the model\u2019s disparity metrics (e.g., calibration, FPR, FNR) on both the training and test set. Empirically, the authors have found that label errors have a larger influence on minority groups than on majority groups. To mitigate the impact of label errors, The authors have proposed a method to estimate the influence of changing a single training input\u2019s label on a model\u2019s group disparity metric. Strength:  The research problems are important and may have many practical applications. The real-world machine learning dataset can easily contain label errors. Improving the robustness of learning models trained on noisy data is important. Existing methods mainly focus on downstream accuracy, but group-based disparity metrics have been ignored which are also important for designing a robust algorithm.  The proposed method is well-motivated. Estimating the influence of a single training input on a model\u2019s group disparity metric is important for confident example selection and dataset purification.  Weakness:  The technical insight may not be enough. The authors have empirically illustrated that minority groups are more sensitive to label errors than majority groups. To make the conclusion more meaningful and practical, I think it would be great to add some theoretical analysis on the influence of label errors with different minority and majority group sizes.  The proposed method for estimating the \u2018influence\u2019 of perturbing a training point\u2019s label on a disparity metric may not practical. The computational cost of the method seems very expensive and needs a lot of retraining processes to detect the effect of all training inputs, which can be hard to apply to a dataset with high-dimensional features. In addition, to demonstrate the performance of the proposed methods, some SOTA methods should be compared (e.g., JoCoR, CVPR\u201920; DivideMix, CVPR\u201920; MEIDTM, CVPR\u201922). The benchmark datasets such as CIFAR10 and CIFAR100 with different types of synthetic noise should also be compared.  The experiment setting is not clear to me. For example, it is not clear how the minority group and majority group in Fig. 1 and Fig.2 are obtained. I think the authors may also need to discuss that how to apply the convolutional network Resnet-18 to tabular and text datasets. This paper generally is well-written and easy to follow, but most discussions are based on experimental results obtained from a few datasets. The experimental settings and comparison should be more detailed and comprehensive. For me, the motivation and research problems of this paper are strong and important. My major concerns are that the technical contribution may not that strong, and the proposed method may not practical and hard to be applied to real-world machine learning datasets. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 1: The contributions are neither significant nor novel. NO.Details Of Ethics Concerns: I have not found any ethics concerns. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 2,
        "instruction": "Factorized Fourier Neural Operators: We propose the Factorized Fourier Neural Operator (F-FNO), a learning-based approach for simulating partial differential equations (PDEs). Starting from a recently proposed Fourier representation of flow fields, the F-FNO bridges the performance gap between pure machine learning approaches to that of the best numerical or hybrid solvers. This is achieved with new representations \u2013 separable spectral layers and improved residual connections \u2013 and a combination of training strategies such as the Markov assumption, Gaussian noise, and cosine learning rate decay. On several challenging benchmark PDEs on regular grids, structured meshes, and point clouds, the F-FNO can scale to deeper networks and outperform both the FNO and the geo-FNO, reducing the error by 83% on the Navier-Stokes problem, 31% on the elasticity problem, 57% on the airfoil flow problem, and 60% on the plastic forging problem. Compared to the state-of-the-art pseudo-spectral method, the F-FNO can take a step size that is an order of magnitude larger in time and achieve an order of magnitude speedup to produce the same solution quality.",
        "reference": "In this work, the authors proposed a novel neural operator architecture that factorizes the convolution on Fourier space into separate dimensions. Consequentially, the F-FNO model can scale up to a higher number of layers and achieve smaller errors. The paper has a comprehensive numerical study on multiple types of partial differential equations, considering chaotic systems and complex geometries. It also has a careful comparison with numerical solvers on the trade off between speed and accuracy. Strength:  The work shows significant improvement on previous methods. The paper has a comprehensive study on many pde problems. It has a careful cost-accuracy study with the numerical solver. I especially love figure 4.  weak:  The author improve FNO++ with many tricks. I assume F-FNO is also equipped with these tricks. It could be better to clearly list what these tricks are, and to what aspect they contribute to the overall improvement. It would be better to provide some intuition why the factorized structure help F-FNO scale with more layers. The paper is clearly written. It has good quality. The technical novelty is not very strong. Overall I find this paper interesting. It has a concrete contribution to the community. I recommend acceptance. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 4: The contributions are significant, and do not exist in prior works. NO. 8: accept, good paper 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 3,
        "instruction": "DFPC: Data flow driven pruning of coupled channels without data.: Modern, multi-branched neural network architectures often possess complex interconnections between layers, which we call coupled channels (CCs). Structured pruning of CCs in these multi-branch networks is an under-researched problem, as most existing works are typically designed for pruning single-branch models like VGG-nets. While these methods yield accurate subnetworks, the improvements in inference times when applied to multi-branch networks are comparatively modest, as these methods do not prune CCs, which we observe contribute significantly to inference time. For instance, layers with CCs as input or output take more than 66% of the inference time in ResNet-50. Moreover, pruning in the data-free regime, where data is not used for pruning, is gaining traction owing to privacy concerns and computational costs associated with fine-tuning. Motivated by this, we study the problem of pruning CCs in the data-free regime. To facilitate the development of algorithms to prune CCs, we define Data Flow Couplings (DFCs) to enumerate the layers that constitute coupled connections and the associated transformation. Additionally, saliencies for pruning CCs cannot be gauged in isolation, as there may be discrepancies among the layerwise importance of CCs using conventional scoring strategies. This necessitates finding grouped saliencies to gauge the importance of all corresponding coupled elements in a network. We thus propose the Backwards Graph-based Saliency Computation (BGSC) algorithm, a data-free method that computes saliencies by estimating an upper bound to the reconstruction error of intermediate layers; we call this pruning strategy Data Flow driven Pruning of Coupled channels (DFPC). Finally, we show the efficacy of DFPC for models trained on standard datasets. Since we pruned coupled channels, we achieve up to 1.66x improvements in inference time for ResNet-101 trained on CIFAR-10 with a 5% accuracy drop without fine-tuning. With access to the ImageNet training set, we achieve significant improvements over the data-free method and see an improvement of at least 47.1% in speedup for a 2.3% accuracy drop for ResNet-50 against our baselines.",
        "reference": "This paper tackles an important problem of neural network pruning. Specifically, the authors of the paper propose a novel method to prune coupled channels in neural networks. For instance, the layers with skip connections in the ResNet model are considered to be coupled channels. This is an under-considered problem, and most of the previously proposed pruning methods neglect these coupled channels. Nevertheless, coupled channels consist of a significant portion of many modern neural network architectures. in essence, the proposed approach involves traversing through all the paths between any pair of such couple channels, and aggregating the overall pruning scores for all of these individual paths. Then, an overall score can be determined and such couple channels can be effectively pruned. The authors of the paper conduct careful experiments to demonstrate the effectiveness of the proposed method. Strength:   The problem considered is an important one, and the proposed method is technically sound.  Experiments conducted show that the proposed method achieves good empirical performance.  The paper is well written in general.  Weaknesses:  The proposed BGSC algorithm seems quite computationally expensive. While there is an analysis of the time complexity, it would be nice to also report the practical time taken to run the algorithm in practice. Does the proposed method take up significantly more time? I feel like the BGSC algorithm needs more explanation. While the overview gave a good insight into the algorithm, there are a lot of technical details in the algorithm that needs to be further discussed and elaborated. In general, the paper is clearly written and enough details are provided to reproduce the experiments. All in all, I have some minor concerns regarding the computational cost of the proposed method. nevertheless, I think this is a good paper and think that it should be accepted. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 4,
        "instruction": "TVSPrune - Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning: Achieving structured, data-free sparsity of deep neural networks (DNNs) remains an open area of research.  In this work, we address the challenge of pruning filters without access to the original training set or loss function. We propose the discriminative filters hypothesis, that well-trained models possess discriminative filters, and any non-discriminative filters can be pruned without impacting the predictive performance of the classifier. Based on this hypothesis, we propose a new paradigm for pruning neural networks: distributional pruning, wherein we only require access to the distributions that generated the original datasets. Our approach to solving the problem of formalising and quantifying the discriminating ability of filters is through the total variation (TV) distance between the class-conditional distributions of the filter outputs. We present empirical results that, using this definition of discriminability, support our hypothesis on a variety of datasets and architectures. Next, we define the LDIFF score, a heuristic to quantify the extent to which a layer possesses a mixture of discriminative and non-discriminative filters. We empirically demonstrate that the LDIFF score is indicative of the performance of random pruning for a given layer, and thereby indicates the extent to which a layer may be pruned. Our main contribution is a novel one-shot pruning algorithm, called TVSPrune, that identifies non-discriminative filters for pruning. We extend this algorithm to IterTVSPrune, wherein we iteratively apply TVSPrune, thereby enabling us to achieve greater sparsity. Last, we demonstrate the efficacy of the TVSPrune on a variety of datasets, and show that in some cases, we can prune up to 60% of parameters with only a 2% loss of accuracy without any fine-tuning of the model, beating the nearest baseline by almost 10%.",
        "reference": "In this paper authors propose a mechanism to prune a convolutional neural network model in a relatively data-free manner i.e., they do not utilize training data or loss function for retraining the pruned model. However unlike the actual data-free pruning techniques they assume the availability of moments of class-conditional distributions of the activations. They make a critical assumption that these distributions are Gaussian to exploit cheaply computable sufficient statistics in deriving tractable bounds which are used to guide the pruning process. They propose a pruning method which exploits the proposed metric to both decide the extent of pruning for a given layer and the actual filters to prune in a given layer, without measuring the impact on the down-stream layer outputs either in terms of the deviations in the metric or in terms of actual performance. This is a reasonable operation under the fundamental hypothesis informing this paper, i.e., discriminability of a filter strongly correlates with performance impact on pruning it. However they do design an iterative version of their pruning method which explicitly measures the cumulative impact of pruning at all the layers on the over all model. They show appreciable reductions in performance degradations for a given pruning budget compared to other data-free pruning methods. Strengths The paper clearly calls out the hypothesis and constantly justifies the algorithmic decisions in the context of this hypothesis.  It performs intermediate validation exercises for their hypothesis, which motivate the reader and guide them through the author's intuitions. The supplementary material is very exhaustive and helpful in further clarifying the details of the proposed algorithm/metrics. Weaknesses The motivation for data free pruning is not clearly described in the paper. The readers are forced to rely on the references. It would be very informative to the reader to compare the sparsification potential of this technique to a pruning method which exploits fine-tuning post compression. The paper is very clear. It explicitly calls out the hypothesis under which the proposed method is designed and provides reasonable intermediary validation steps. The novelty of the paper lies in designing a cheap pruning method which is cheap due to well validated/referenced assumptions such as Gaussian nature of intermediate activations or strong correlation between discriminability of a layer's output and its prunability.  Authors made appreciable effort to improve reproducibility of the results in the paper. They provide a link to their implementation. The authors describe the algorithm in great detail. They utilize standard architectures to demonstrate their pruning method. They utilize open datasets for experimental validation. This paper guides the reader through the design of the algorithm and motivates the design decisions with well validated/referenced hypotheses. It addresses a problem of critical interest to this community. The writing style is clear. The references are more than adequate. The proposed technique is has limited but sufficient experimental validation. Though there are minor possible improvements the paper in its current form is already useful to the readers. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 5,
        "instruction": "Finding Actual Descent Directions for Adversarial Training: Adversarial Training using a strong first-order adversary (PGD) is the gold standard for training Deep Neural Networks that are robust to adversarial examples. We show that, contrary to the general understanding of the method, the gradient at an optimal adversarial example may increase, rather than decrease, the adversarially robust loss. This holds independently of the learning rate. More precisely, we provide a counterexample to a corollary of Danskin's Theorem presented in the seminal paper of Madry et al. (2018) which states that a solution of the inner maximization problem can yield a descent direction for the adversarially robust loss. Based on a correct interpretation of Danskin's Theorem, we propose Danskin's Descent Direction (DDi) and we verify experimentally that it provides better directions than those obtained by a PGD adversary. Using the CIFAR10 dataset we further provide a real world example showing that our method achieves a steeper increase in robustness levels in the early stages of training, and is more stable than the PGD baseline. As a limitation, PGD training of ReLU+BatchNorm networks still performs better, but current theory is unable to explain this.\n",
        "reference": "This paper pays attention to the computation of adversarial training, by pointing out that even in the simple case (nonsmooth), the descent direction is not given by the worst-case perturbation, as opposed to common practice in the AT community. The paper then proposes a method that, under the assumption of a finite number of worst-case perturbations, computes the correct descent direction. Strength: I appreciate the conceptual idea of this paper a lot. The starting point of the paper is the observation that the descent direction not necessarily corresponds to the worst-case perturbation. Given the fact that this is typically assumed always be the case in the AT community, this paper has provided a refreshing perspective on how should we do AT correctly. The constructed counterexamples are also easy to understand.  Weakness: The potential downside of this paper is that the proposed method (DDD) is inherently computationally expensive. As the authors advocate the usage of multiple worst-case perturbations, adopting DDD would require at least several times of computational effort compared to the already expensive standard AT procedure.  Another concern that I have is the similar final performance achieved by both AT and DDD. It seems that their performance differs by some margin only in the initial stage of the training, but is getting much closer to the final stage. This to some extent, has limited the potential practical impact of the proposed method. Quality:  The paper is very well written. Delivers a simple yet interesting observation clearly. The observation is to the best of my knowledge novel in the literature of AT.  Clarity:  The paper is well organized. The constructive counter-examples are well explained and the authors have done a good job maintaining their simplicity.  Originality: The main claim and the proposed methods are novel. I appreciate the paper being bold and challenging the mainstream practices. This paper focuses on the bring forward the issue that (a single) adversarial example might not give the descent direction, challenging a common belief adopted in the practice of this field. It did this with simple examples making its arguments. This is the strongest point of this paper. The proposed method, on the other hand, is computationally expensive compared to the original AT, and the final performance is comparable to AT, which is to some extent surprising given the fact that AT is supposedly not a correct method, the main theoretical claim made by this paper. The practical side of the claim and method proposed in this paper seems a little bit limited by its current shape. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 6,
        "instruction": "Learning Continuous Normalizing Flows For Faster Convergence To Target Distribution via Ascent Regularizations: Normalizing flows (NFs) have been shown to be advantageous in modeling complex distributions and improving sampling efficiency for unbiased sampling. In this work, we propose a new class of continuous NFs, ascent continuous normalizing flows (ACNFs), that makes a base distribution  converge faster to a target distribution. As solving such a flow is non-trivial and barely possible, we propose a practical implementation to learn flexibly parametric ACNFs via ascent regularization and apply it in two learning cases: maximum likelihood learning for density estimation and minimizing reverse KL divergence for unbiased sampling and variational inference. The learned ACNFs demonstrate faster convergence towards the target distributions, therefore, achieving better density estimations, unbiased sampling and variational approximation at lower computational costs. Furthermore, the flows show to stabilize themselves to mitigate performance deterioration and are less sensitive to the choice of training flow length $T$.\n",
        "reference": "This paper discusses ascent regularization for training continuous normalizing flows (CNFs). This is motivated from Wasserstein gradient flows and results in an interesting regularization that encourages the learned model to be similar to the target distribution around a large interval of time values. I find this to be an interesting alternative to optimal transport-based approach for reducing the compute cost to transport samples between a base distribution and a target distribution. Experiments include maximum likelihood on tabular data and as a variational inference model within a VAE. Strengths:  The proposed approach is interesting, as a way to mimic Wasserstein gradient flow paths. Using the time evolution of the score function is interesting as well. The algorithm works well at regularizing the path and seems to help accelerate convergence towards the target distribution. This is shown across both MLE and VI settings, as well as in a toy setting where it is shown to improve upon MCMC in terms of the number of function evaluations.  Weaknesses:  I found the writing poor and can be significantly improved. Specifically, there seems to be multiple concepts being introduced \\tilde{p}_t (Eq 4) and V (Eq 7) that do not appear in the experiments. See clarity section below for more comments. If I am mistaken, please correct me. Novelty:  Combining a score function estimator with the Wasserstein gradient flow for regularizing CNFs seems novel and works well in practice.  Quality:  ACNF compares favorably to CNF and RNODE (an OT-based regularization for CNFs) on a variety of settings.  Being able to get close to the T=1 distribution with a shorter time interval is interesting.  Clarity:  Eq (4), the \"estimated log-likelihood\" warrants more clarification. From what I understand, it is an approximation to p_0(x) where instead of integrating all the way from 0 to T, this integrates only up to t and directly assumes the base distribution at time t. So it is equal to p_0(x) when t=T, but otherwise is not really related to p_t. The sentence before Eq (4) makes it seem like \\tilde{p}_t is approximating p_t, but this isn't the case right?  It also isn't clear why \\tilde{p}_t is being introduced. The sentence before Eq (4) states this is to avoid extra integration steps, but both Algorithms 1 and 2 seem to integrate all the way from 0 to T regardless. And the \\tilde{p}_T in the training objective can be replaced by p_0. Can the authors clarify the importance of introducing \\tilde{p}_t ? Eq (7) seems out of place. I did not understand how this equation contributes to the paper but this takes up half of the section on ACNF. It might be better to have a dedicated Related Works section to discuss connections to other works. On the contrary, Eq (8) is interesting and should be emphasized more strongly I feel. How are the NFEs computed? Is the ODE being solved with an adaptive solver? Why is the model regularized for t > T? I did not understand why this is the case, since the model does not regularize beyond [0, T]. Also, is regularizing t > T useful for anything? I think this paper is interesting and has potential. However, I feel hesitant to recommend acceptance in its current form, particularly since multiple concepts and equations are introduced but I did not understand how these connect with the final training algorithm. My understanding is that the main contribution seems to be using the time evolution of the score function for regularizing the ODE to model the steepest descent direction on the KL, but the connection to \\tilde{p}_t is unclear to me. If the authors can clarify this, I would be willing to change my rating. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 7,
        "instruction": "Softened Symbol Grounding for Neuro-symbolic Systems: Neuro-symbolic learning generally consists of two separated worlds, i.e., neural network training and symbolic constraint solving, \nwhose success hinges on symbol grounding, a fundamental problem in AI. This paper presents a novel, softened symbol grounding process, bridging the gap between the two worlds, and resulting in an effective and efficient neuro-symbolic learning framework. Technically, the framework features (1) modeling of symbol solution states as a Boltzmann distribution, which avoids expensive state searching and facilitates mutually beneficial interactions between network training and symbolic reasoning; (2) a new MCMC technique leveraging projection and SMT solvers, which efficiently samples from disconnected symbol solution spaces; (3) an annealing mechanism that can escape from sub-optimal symbol groundings. Experiments with three representative neuro-symbolic learning tasks demonstrate that, owing to its superior symbol grounding capability, our framework successfully solves problems well beyond the frontier of the existing proposals. ",
        "reference": "This paper presents a neuro-symbolic learning framework with an explicit design for addressing the symbol grounding problem.  The key idea is softening symbol grounding by using a Boltzmann distribution to represent the entire symbol space, rather than a specific symbol grounding. Such a design achieves an efficient interaction between neural perception and symbolic reasoning. Then, a novel MCMC sampling method combined with SMT solving is proposed to facilitate learning the correct symbol grounding. Furthermore, an annealing mechanism with three different realizations is applied to avoid sub-optimal symbol groundings. Experimental evaluations show significant improvement over several state-of-the-art approaches. Strengths:  the paper is very well-written; all necessary background  (either classic or recent work) is carefully introduced, and related work is organized in a systematic and insightful manner. the paper addresses a fundamental problem in AI from a novel and promising angle -- view symbol grounding distribution as a mixed strategy in game playing; the combination of the MCMC sampling with SMT solving is both novel and elegant the paper has extensive experimental evaluations and achieves superior performance over many state-of-the-art approaches the paper also presents a formal analysis of the convergence guarantee (with some mild assumptions)  Weaknesses:  the projection and inverse operations may be quite sensitive and require some non-trivial domain expertise the visual Sudoku classification task seems to be a simplified version of the original one. The 4-by-4 Sudoku puzzle used in the evaluation is much simpler than a standard 9-by-9 Sudoku. SATNet should be the state-of-the-art for the Sudoku task, which is however not included Each individual technique used in this paper is standard, but the particular combination has great novelty and elegance, especially the interaction between MCMC sampling and SMT solving. The symbol grounding problem studied in this paper is fundamental, and the result  achieved in this paper significantly outperforms several state-of-the-art approaches, which are highly non-trivial. Detailed implementations and evaluation datasets have been shared publicly, so the results should be easily reproducible. This paper makes significant progress toward addressing a fundamental problem in neuro-symbolic AI. The presented approach is novel, effective, and elegant, which is likely to have a great influence on future neuro-symbolic system design. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 10: strong accept, should be highlighted at the conference 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 8,
        "instruction": "Encoding Recurrence into Transformers: This paper novelly breaks down with ignorable loss an RNN layer into a sequence of simple RNNs, each of which can be further rewritten into a lightweight positional encoding matrix of a self-attention, named the Recurrence Encoding Matrix (REM). Thus, recurrent dynamics introduced by the RNN layer can be encapsulated into the positional encodings of a multihead self-attention, and this makes it possible to seamlessly incorporate these recurrent dynamics into a Transformer, leading to a new module, Self-Attention with Recurrence (RSA). The proposed module can leverage the recurrent inductive bias of REMs to achieve a better sample efficiency than its corresponding baseline Transformer, while the self-attention is used to model the remaining non-recurrent signals. The relative proportions of these two components are controlled by a data-driven gated mechanism, and the effectiveness of RSA modules are demonstrated by four sequential learning tasks.",
        "reference": "The paper tackles the problem of endowing Transformers with the ability to encode information about the past via recurrence. The proposed architecture can leverage the recurrent connections to improve the sample efficiency while maintaining expressivity due to the use of self-attention. Strengths:  The paper is easy to read, and generally well written. The paper evaluates the proposed method on various different tasks such as time-series forecasting, code and language modelling. The paper augments the proposed method to various different transformer variants and compares the performance with respect to the unmodified baseline.  Weakness:  The problem of integrating recurrence and self-attention is an important research problem. There exists some existing ways on how to augment transformers with recurrence such as Temporal Latent Bottleneck [1] and Block-Recurrent Transformers [2]. The idea behind TLB is to \"divide\" the sequence into chunks, and within a chunk use self-attention and to access information across chunks the model needs to use recurrence. It would be useful to compare the proposed method to these variants to futher analyse the pros and cons.  It may also be useful to study the proposed method by varying the capacity of the network to see how well the underlying idea scales.  [1] Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning, https://arxiv.org/abs/2205.14794 (NeurIPS'22) [2] Block Recurrent Transformers, https://arxiv.org/abs/2203.07852 (NeurIPS'22) Clarity: The paper is easy to read. Quality: The paper tries to tackle an important problem. Novelty: Even though the problem is not \"new\" per se, but the underlying idea is interesting. Reproducibility: The paper should be easy to reproduce. The paper proposes a way to incorporate recurrence and self-attention by modifying the positional encoding. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 9,
        "instruction": "Human-Guided Fair Classification for Natural Language Processing: Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3's zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. We then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. Finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models. ",
        "reference": "This paper introduces a workflow/methodology to generate pairs of similar sentences that differ only wrt target/protected populations such as gender or race. The methodology inclues increasingly sophisticated steps, such as word replacement, unsupervised style transfer and generation using GPT3. Crowdsourcing and active learning is used to filter the generated pairs. The methodology is demonstrated in the context of toxic text prediction. Using the pair-wise sentences that vary only along the protected groups, a RoBERTa model is trained by enforcing the logits for the two pieces of text to be similar (since the toxic text prediction should not depend on the protected subgroup used in the sentence). Strengths:  The methodlogy could be used in different contexts for generating pairs of similar senteces that differ only in the protected group Lots of experiments to support the methodology  Weakness:  Some ad-hoc decisions (e.g., shortening the max sequence size) that are not well explained Lots of experimental results that are not well explained In general, I enjoyed reading the paper\tand learning about the proposed methodlogy. I had a hard time processing the multitude of experimental results. In particular, it's not clear what the numbers represent in each table. I would clearly describe in the caption of each figure what the rows/columns represent and what metric is used. In particular, the paper uses \"fairness\" without clearly explaining how fairness is measured. Some of\tthe design decisions are not well explained. In\tparticular, I am curious about the maximum sequence size of 64 (which is quite limiting). Is that to ensure that the style transfer and the GPT3 generation produce quality results? Not clear what WR50 means. In Table 1, all columns except for BA represent some measure of fairness? If that's the case and if I understand the results WR (which I'm guessing stands for word replacement) seems quite performant. How do you justify the complexity of adding the style transfer and GPT3 on top. What is the real advantage of the full C dataset? The fairness metric used seemed a bit ad hoc. One of the more standard metrics could be use (see Fairness Definitions Explained and the following paper for an example of using equalized odds: Your Fairness May Vary: Pretrained Language Model Fairness in Toxic Text Classification - analysis of many LMs wrt fairness and model size/training size/random seed (in the context of toxic text prediction)). Last but not least, I think \"fairness specifications\" is misleading and quite an overblown term for what the paper is about; it made me think of some theoretical formalism/specification. Similar pairs of text across protected groups is a much more honest description of what the paper generates. This paper introduces a methodology to generate pairs of similar sentences that differ only wrt protected group. The pairs generated can be used to train fairer classifiers by imposing similar logits for similar sentences. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 10,
        "instruction": "Mini-batch $k$-means terminates within $O(d/\\epsilon)$ iterations: We answer the question: \"Does \\emph{local} progress (on batches) imply \\emph{global} progress (on the entire dataset) for mini-batch $k$-means?\". Specifically, we consider mini-batch $k$-means which terminates only when the improvement in the quality of the clustering on the sampled batch is below some threshold.\n\nAlthough at first glance it appears that this algorithm might execute forever, we answer the above question in the affirmative and show that if the batch is of size $\\tilde{\\Omega}((d/\\epsilon)^2)$, it must terminate within $O(d/\\epsilon)$ iterations with high probability, where $d$ is the dimension of the input, and $\\epsilon$ is a threshold parameter for termination. This is true \\emph{regardless} of how the centers are initialized. When the algorithm is initialized with the $k$-means++ initialization scheme, it achieves an approximation ratio of $O(\\log k)$ (the same as the full-batch version). \n\nFinally, we show the applicability of our results to the mini-batch $k$-means algorithm implemented in the scikit-learn (sklearn) python library. ",
        "reference": "The paper analyzes the convergence rate of mini-batch k-means, namely, running Lloyd's iteration with a uniform sample of points from the data set, rather than using the entire set in each iteration. It gives strong results: with a sample size nearly quadratic in the dimension, the number of steps needed is linear in the dimension (and independent of the size of the data set). This requires a stopping condition that deviates from practice, and somewhat weaker bounds are shown for the conventional stopping condition. This is an appealing result, the best in my small pile, and should definitely be accepted. The paper is clearly written and easy to follow. It uses standard notation. The convergence bound is strong, and the paper actually indicates a modification in the standard implementation that could result is superior performance in practice. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. Not applicable NO. 10: strong accept, should be highlighted at the conference 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 11,
        "instruction": "Learning Uncertainty for Unknown Domains with Zero-Target-Assumption: We introduce our Maximum-Entropy Rewarded Reinforcement Learning (MERRL) framework that selects training data for more accurate Natural Language Processing (NLP). Because conventional data selection methods select training samples based on the test domain knowledge and not on real life data,  they frequently fail in unknown domains like patent and Twitter. \nOur approach selects training samples that maximize information uncertainty measured by entropy, including observation entropy like empirical Shannon entropy, Min-entropy, R\\'enyi entropy, and prediction entropy using mutual information, to cover more possible queries that may appear in unknown worlds. Our MERRL using regularized A2C and SAC achieves up to -99.7 perplexity decrease (-43.4\\% relatively) in language modeling, +25.0 accuracy increase (+40.0\\% relatively) in sentiment analysis, and +5.0 F1 score increase (+30.8\\% relatively) in named entity recognition over various domains, demonstrating strong generalization power on unknown test sets.",
        "reference": "In this paper, the authors propose to use a Maximum-Entropy Rewarded Reinforcement Learning framework to select training data for NLP tasks, the goal of which is to maximize generalization. The authors experiment with A2C and SAC and experimental results show that the proposed framework could outperform several baseline approaches. Strength: In general this paper is clearly written and easy to follow. The experimental results seem to confirm the validity of proposed method. Weakness: I suggest the authors include more justifications when selecting certain models of NLP tasks. For example, in sentiment analysis experiments, why the authors choose to use a CNN classifier as the model? This paper is easy to follow. The authors provide the detailed information for reproduce the experiments. I think the authors propose a novel method of using entropy in selecting data. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 12,
        "instruction": "Transformer-based model for symbolic regression via joint supervised learning: Symbolic regression (SR) is an important technique for discovering hidden mathematical expressions from observed data. Transformer-based approaches have been widely used for machine translation due to their high performance, and are recently highly expected to be used for SR. They input the data points, then output the expression skeleton, and finally optimize the coefficients. However, recent transformer-based methods for SR focus more attention on large scale training data and ignore the ill-posed problem: the lack of sufficient supervision, i.e., expressions that may be completely different have the same supervision because of their same skeleton, which makes it challenging to deal with data that may be from the same expression skeleton but with different coefficients. Therefore, we present a transformer-based model for SR with the ability to alleviate this problem. Specifically, we leverage a feature extractor based on pure residual MLP networks to obtain more information about data points. Furthermore, the core idea is that we propose a joint learning mechanism combining supervised contrastive learning, which makes features of data points from expressions with the same skeleton more similar so as to effectively alleviates the ill-posed problem. The benchmark results show that the proposed method is up to 25% higher with respect to the recovery rate of skeletons than typical transformer-based methods. Moreover, our method outperforms state-of-the-art SR methods based on reinforcement learning and genetic programming in terms of the coefficient of determination ($R^2$).",
        "reference": "This paper proposes a novel transformer-based model for symbolic regression (SR), which produces mathematical expression skeletons from data points. In the proposed method, a feature extractor using pointMLP is added and jointly trained using contrastive loss to realize efficient training. The experimental evaluation using benchmark functions demonstrates that the proposed method exhibits higher recovery rates and R2 scores compared to existing SR methods. Strengths  Introducing the idea of feature extractor and contrastive learning into the transformer-based symbolic regression model is convincing. Although each component of the proposed method was previously proposed in the context of other tasks, incorporating such methods into the SR model seems to be novel. The numerical experiment supports the effectiveness of the proposed method.  Weaknesses  In the experiment, the same dataset is used for training and testing. I am not confident whether such a setting is reasonable. From the machine learning perspective, the obtained model should be evaluated on test datasets.  Comments  It would be nice if the proposed method was evaluated using recent benchmark datasets such as SRBench (https://github.com/cavalab/srbench). In Figure 2, the redder color might indicate the cosine similarity is closer to 0. How is the computational cost of the proposed method compared to other SR methods? This paper is well-written. The motivation and contribution are easily understood. Although the components of the proposed method are existing ideas, introducing such promising techniques into the transformer-based SR model is novel. The code is provided and the detailed setting of the experiment is reported. Basically, this paper is well-written, and the motivation and contribution are clear. The effectiveness of the proposed method is supported by the numerical evaluation. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 13,
        "instruction": "QAID: Question Answering Inspired Few-shot Intent Detection: Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance.",
        "reference": "The paper proposes QAID, Question Answering inspired Intent Detection system, which models the intention detection classification as a question-answering task. The model uses two stages of training: a pretraining for better query representation and finetuning on few-shot labels of query and answers (name of intents). Certain choices of model, such as token-level similarity, batched contrastive learning, and inference with answers only, are verified by detailed ablation studies. The proposed method achieves SOTA  performance on three intention detection dataset from DialoGLUE. Strength:   The paper uses and adapts techniques from QA and answer retrieval to the task of ID. The method is effective and efficient on three benchmark datasets when compared with various baselines. The detailed ablation study verifies the necessity of stages and component of proposed framework. The paper is well-written and easy to follow.  Weakness:  Given the number of intents being only a few hundreds at most, is Faiss really necessary for inference? n has two meanings in section 2.1: one for batch size and another for number of tokens. Clarity: The paper is clearly written. Quality: The statements are well supported. The empirical results are solid. Novelty: The paper adapts the method from question answering to intent detection. The adaptation itself is novel although the proposed methods are a combination of existing techniques. Reproducibility: It requires some effort to reproduce the results. The paper is inspired by the development of ColBERT in QA task and propose a variant of the model for the task of intent detection. The proposed model makes several adaptations including batch contrastive loss and  signal from the intent names. The proposed method achieves SOTA performance on three few-shot ID benchmarks. And the ablation study proves the necessity of various components in the system. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 14,
        "instruction": "Solving stochastic weak Minty variational inequalities without increasing batch size: This paper introduces a family of stochastic extragradient-type algorithms for a class of nonconvex-nonconcave problems characterized by the weak Minty variational inequality (MVI). Unlike existing results on extragradient methods in the monotone setting, employing diminishing stepsizes is no longer possible in the weak MVI setting. This has led to approaches such as increasing batch sizes per iteration which can however be prohibitively expensive. In contrast, our proposed methods involves two stepsizes and only requires one additional oracle evaluation per iteration. We show that it is possible to keep one fixed stepsize while it is only the second stepsize that is taken to be diminishing, making it interesting even in the monotone setting. Almost sure convergence is established and we provide a unified analysis for this family of schemes which contains a nonlinear generalization of the celebrated primal dual hybrid gradient algorithm.\n",
        "reference": "For the first time, the authors introduces a family of stochastic extragradient-type algorithms that positively solves a class of nonconvex-nonconcave problems which can be cast as stochastic weak Minty variational inequality (MVI). In the monotone setting, extragradient methods adopt constant stepsizes and bounded batchsizes (both of which are critical in practical performances), and when extending to the weak MVI setting, only theories adopting expensive increasing batch sizes per iteration approaches are available. Strength This work answers affirmatively an open problem by proposing a bias-corrected stochastic extragradient (BCSEG+) algorithm that solves stochastic weak Minty variational inequalities without increasing the batch size. As the authors indicated, Pethick et al. (2022) \"su\ufb03ces in the special case of unconstrained quadratic games but can fail even in the monotone case ...\". Also, earlier works such as Hsieh et al. (2020) adopt diminishing but larger exploration stepsize and smaller updating stepsize. Weakness There is not much from my perspective, as long as the proof is correct (which I took a high-leve look at but did not go into all details). Two small comments: --MVI can be short for \"monotone\" variational inequality instead of \"Minty\" variational inequality. Adopting this shorthand as in some earlier work might cause unnecessary confusion. Therefore, I would suggest the authors avoid this shorthand as much as possible. --The authors should do more literature reviews. Missing reference includes but not limited to \"Bot et al., Minibatch Forward-Backward-Forward Methods for Solving Stochastic Variational Inequalities, 2021\" The authors did a good job in all these given aspects. This paper is theoretically strong in the sense that it made an important step in fixing SEG with constant extrapolation stepsize and diminishing updating stepsize. This makes an important step upon existing work (e.g. Hsieh et al., 2020; Diakonikolas et al., 2021; Pethick et al., 2022) even for the monotone case. This supports my high rating of this work. It would be even better if the authors have a chance to include richer empirical findings relative to this work. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 15,
        "instruction": "Curriculum-based Co-design of Morphology and Control of Voxel-based Soft Robots: Co-design of morphology and control of a Voxel-based Soft Robot (VSR) is challenging due to the notorious bi-level optimization. In this paper, we present a Curriculum-based Co-design (CuCo) method for learning to design and control VSRs through an easy-to-difficult process. Specifically, we expand the design space from a small size to the target size gradually through a predefined curriculum. At each learning stage of the curriculum, we use reinforcement learning to simultaneously train the design policy and the control policy, which is enabled by incorporating the design process into the environment and using differentiable policy representations. The converged morphology and the learned policies from last stage are inherited and then serve as the starting point for the next stage. In empirical studies, we show that CuCo is more efficient in creating larger robots with better performance by reusing the practical design and control patterns learned within each stage, in comparison to prior approaches that learn from scratch in the space of target size.",
        "reference": "This paper introduces a new curriculum-based method for co-designing morphology and control of voxel-based soft robots. This curriculum-based method expands the design space from a small size to the target size using reinforcement learning with a predefined curriculum. To address incompatible state-action spaces, local observations of robot voxels are modeled as a sequence and self-attention is used to control the voxels. Strength:   The paper addresses an important problem on co-design of morphology and control with the potential to greatly impact voxel-based soft robots.  The paper identifies several interesting challenges for soft robot co-design, such as the high dimensionality of the joint design and control space and the difficulty in generalizability. Methods are designed to address these specific challenges.  The paper is well organized and well written.   Weakness:  The goal of the paper is the co-design of morphology and control. However, the approach assumes that \"The morphology of the robot is unchangeable during the interaction with the environment\". While the control policy uses the learned morphology, but how does the control inform or help morphology design?  In the related work section, the paper argues that methods such as Transform2Act by Yuan et al (2022) have \"the limitation of aggregating multi-hop information\". In the experiment section, the paper states that CuCo-NCU (CuCo without the curriculum component) is similar to Transform2Act. How is this multi-hop information aggregated by CuCo. Other than the curriculum, what are the key differences between CuCo and Transform2Act?  While simulators such as Evolution Gym and MuJoCo are helpful to evaluate theories, out of curiosity, how are these simulators, the proposed method, and the experimental results applicable to real soft robots? For example, what are the existing prototypes of real voxel-based soft robots that can use the proposed method for co-design? This paper introduces a new solution for co-design of morphology and control for soft robots, and originally addresses several key challenges. The paper is well written, and the ideas are clearly justified. The experimental results could be reproduced using the Evolution Gym simulator. Although several technical details need to be clarified, this paper proposes a new method and addresses several specific challenges for morphology and control of simulated voxel-based soft robots. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 16,
        "instruction": "WiNeRT: Towards Neural Ray Tracing for Wireless Channel Modelling and Differentiable Simulations: In this paper, we work towards a neural surrogate to model wireless electro-magnetic propagation effects in indoor environments.\nSuch neural surrogates provide a fast, differentiable, and continuous representation of the environment and enables end-to-end optimization for downstream tasks (e.g., network planning). Specifically, the goal of the paper is to render the wireless signal (e.g., time-of-flights, power of each path) in an environment as a function of the sensor's spatial configuration (e.g., placement of transmit and receive antennas). NeRF-based approaches have shown promising results in the visual setting (RGB image signal, with a camera sensor), where the key idea is to algorithmically evaluate the 'global' signal (e.g., using volumetric rendering) by breaking it down in a sequence of 'local' evaluations (e.g., using co-ordinate neural networks). In a similar spirit, we model the time-angle channel impulse response (the global wireless signal) as a superposition of multiple paths. The wireless characteristics (e.g., power) of each path is a result of multiple evaluations of a neural network that learns implicit ray-surface interaction properties. We evaluate our approach in multiple indoor scenarios and demonstrate that our model achieves strong performance (e.g., $<$0.33ns error in time-of-flight predictions). Furthermore, we demonstrate that our neural surrogate whitens the `black-box' wireless simulators, and thus enables inverse rendering applications (e.g., user localization).",
        "reference": "This paper proposes a neural network based solution to heuristically solve a wireless signal (physics) rendering problem. Given the environment set-up and configurations of the transmitter and receivers, the pre-trained network is able to simulate the wireless signal propagation in the confined environment instead of physically computing the ray-tracing function, which costs the computational power extremely. In addition, this approach enables the reverse rendering applications. Strengths  This paper creates a new area that is simulating / predicting wireless signal propagation problem (or more generally: a ray-tracing simulation). This method explores a new topic and expands the boundary of computer vision applications.  The proposed paper has thorough study with signal propagation problem. It provides detailed mathematical definition of the problem, casts the physics ray-tracing computation to a network simulation problem. It also come with solidate experiments to demonstrate the effectiveness of the proposed method.  The paper also proposes two new databases / datasets that allow the community to benchmark further new methods. This could bring more impact to the community of computer vision.  Weaknesses  This paper does not discuss about the non-linear surface or interactions of wireless signals. It assumes the operations are all linear. The authors are suggested to provide a limitation of the method In the introduction part, the goal of the proposed method is to reduced the inference time. The authors are suggested to make a comparison between the current method and ray-tracing computation. This paper has clearly described the motivation of wireless signal propagation, the necessity of simulating wireless signals. It also has provided a comprehensive literature review, although no previous research in this line.  The method is clearly written with the aid of using mathematical formulas. The novelty is quite strong because this is a new topic with no pre-existing solutions.   The code is provided and the reproducibility and originality of the work is good. In summary, this paper proposes a new topic of simulating wireless signal propagation in a confined configurable environment. The paper systematically defines and solves the problem with comprehensive experiments. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. 4: The contributions are significant, and do not exist in prior works. NO. 8: accept, good paper 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 17,
        "instruction": "LS-IQ: Implicit Reward Regularization for Inverse Reinforcement Learning: Recent methods for imitation learning directly learn a $Q$-function using an implicit reward formulation rather than an explicit reward function. However, these methods generally require implicit reward regularization to improve stability and often mistreat absorbing states. Previous works show that a squared norm regularization on the implicit reward function is effective, but do not provide a theoretical analysis of the resulting properties of the algorithms. In this work, we show that using this regularizer under a mixture distribution of the policy and the expert provides a particularly illuminating perspective: the original objective can be understood as squared Bellman error minimization, and the corresponding optimization problem minimizes a bounded $\\chi^2$-Divergence between the expert and the mixture distribution. This perspective allows us to address instabilities and properly treat absorbing states. We show that our method, Least Squares Inverse Q-Learning (LS-IQ), outperforms state-of-the-art algorithms, particularly in environments with absorbing states. Finally, we propose to use an inverse dynamics model to learn from observations only. Using this approach, we retain performance in settings where no expert actions are available.",
        "reference": "The paper studies the problem of imitation learning, building on the recent IQ-learn framework. Instead of an adversarial reward-policy loss like GAIL, IQ-learn instead parameterizes the Q-function so that the policy can be directly extracted. While IQ-learn works fine, the paper notes that some of the practical tricks don't match the analysis, e.g. regularizing both the expert and the imitator (which also should prevent direct extraction of policy in theory). In this paper, the authors present Least Squares Inverse Q Learning (LS-IQ). LS-IQ patches some of the gap described above with IQ-learn.   First, LS-IQ shows that the mixture regularizer is, naturally, regularizing a chi-squared divergence between the expert and the mixture of expert/policy occupancies. In practice this is good because the mixture ensures that the divergence is bounded. Second, they use a least-squares RL perspective to figure out how to properly treat absorbing states. Third, they propose using a regularization critic, to account for the additional regularizer term in the objective. A few more tips and tricks, e.g., replacing bootstrapping target with fixed target, learning from observations (with no action information) by training an IDM. Strengths  the biggest strength of the paper is that it is very practical: it identifies many tips and tricks for making IQ-learn better. This is a great contribution, given the already strong results of the base IQ-learn algorithm. some of these tricks are somewhat theoretically grounded. Though (as the paper notes itself) some parts are not super convincing, e.g. fixed Q target, it is still useful for future work. the experiments have ablations of using different subsets of the proposed tricks.  Weaknesses The first few pages were find to read, but the rest of the paper (esp section 3) was poorly structured in my opinion. It reads like a laundry list A, B, C, and the reader has no idea what to anticipate next. Some of the insights feel like offhand remarks that have little relevance to the rest of the paper, and are hard to distinguish from the actual important insights. I would try to restructure the paper so that the most important parts are emphasized (even repeated), and move more minor details to the appendix. Clarity is subpar. Quality, novelty, and reproducibility are great. In summary the authors improve on IQ-learn, the current SOTA imitation learning algorithm on many tasks. The paper presents a laundry list of tips and tricks, which was hard to read, but shows good practical improvements that should be very valuable to the community.  Update: I have read the author response, and am keeping my evaluation. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 18,
        "instruction": "Humanly Certifying Superhuman Classifiers: This paper addresses a key question in current machine learning research: if we believe that a model's predictions might be better than those given by human experts, how can we (humans) verify these beliefs? In some cases, this ``superhuman'' performance is readily demonstrated; for example by defeating top-tier human players in traditional two player games. On the other hand, it can be challenging to evaluate classification models that potentially surpass human performance. Indeed, human annotations are often treated as a ground truth, which implicitly assumes the superiority of the human over any models trained on human annotations. In reality, human annotators are subjective and can make mistakes. Evaluating the performance with respect to a genuine oracle is more objective and reliable, even when querying the oracle is more expensive or sometimes impossible. In this paper, we first raise the challenge of evaluating the performance of both humans and models with respect to an oracle which is $\\textit{unobserved}$. We develop a theory for estimating the accuracy compared to the oracle, using only imperfect human annotations for reference. Our analysis provides an executable recipe for detecting and certifying superhuman performance in this setting, which we believe will assist in understanding the stage of current research on classification. We validate the convergence of the bounds and the assumptions of our theory on carefully designed toy experiments with known oracles. Moreover, we demonstrate the utility of our theory by meta-analyzing large-scale natural language processing tasks, for which an oracle does not exist, and show that under our mild assumptions a number of models from recent years have already achieved superhuman performance with high probability---suggesting that our new oracle based performance evaluation metrics are overdue as an alternative to the widely used accuracy metrics that are naively based on imperfect human annotations.",
        "reference": "This paper proposes an approach to certify whether a given machine learning model achieves super-human performance when the dataset labels are (possibly erroneous) human annotations and not (unobserved) ground-truth labels. The proposed approached relies on proving the following results (given K human annotators and infinite data):  (Theorem 1) The probability that a randomly selected human annotator labels all samples correctly is bounded above by the root mean squared inter-annotator agreement.  (Theorem 3) The probability that the machine learning model predicts the labels of all samples correctly is bounded below by the probability that the machine learning model predicts labels that match the aggregated human-annotated labels (aggregated via majority voting, for example).   Note that the lower bound in Theorem 3 is basically the machine learning model accuracy with respect to the human annotations, as is traditionally reported. The paper also proves variants of the theorems above for finite data samples. The theorems above rely on the following assumptions:  Human annotators are not independent; their annotations are \"positively correlated\". Even if the aggregated human annotation is incorrect, the machine learning model is more likely to predict the correct label than the incorrect human annotation.  The proposed approach essentially compares the upper bound for human annotators (Theorem 1) with the lower bound for the machine learning model (Theorem 3). The paper also provides a way to construct confidence intervals on the difference between these two bounds. The paper concludes with an empirical evaluation of the proposed theory. This paper has several strengths. It considers an important problem: how can we theoretically guarantee that a given model exceeds human annotation performance? More importantly, it enables qualifying claims of superhuman performance by quantifying whether the observed/reported superhuman performance is statistically significant for a given number of annotators with a specific inter-annotator agreement. The paper approaches this problem in a principled manner, by deriving upper bounds on human performance and lower bounds on model performance (without needing access to ground truth labels). The paper also derives finite sample variants of these bounds that are practically useful, and derives confidence intervals on the difference between these two bounds. All assumptions are clearly stated. The paper concludes with an empirical evaluation of the proposed theory, which covers several important aspects of the theory such as the validity of the assumptions, whether the bounds are valid empirically, and how they vary as the number of annotators increases. I have a few concerns about the clarity and quality of this paper, enumerated below. 1. Unclear random vs. deterministic quantities in the problem statement I think the distinction between what is random and what is deterministic in the problem statement could be clearer. Since P(li=l\u2217) is defined as \"the ratio of matched labels\" in Section 2.1, I believe that the dataset of N points is fixed and not a random sample. Hence, l\u2217 and each li is deterministic, and P(li=l\u2217) is a deterministic ratio. lK, in contrast, is random. Hence, P(lK=l\u2217) is random. The usage of P for both random and deterministic quantities makes following the problem statement a bit difficult. Later in Theorem 1, P(li=lj) is treated like a probabilistic quantity, which suggests that P(li=lj) is the probability that i and j agree on all N labels for a randomly sampled dataset of N points. However, this contradicts Section 2.1 which defines P(li=l\u2217) as \"the ratio of matched labels\". Finally in Theorem 5, P(N)(li=lj) is defined as the empirical fraction of N data points where i and j agree on the label, which suggests that P(li=l\u2217) is the \"the ratio of matched labels\" assuming N\u2192\u221e. It would help if the notation made the appropriate interpretation of each probability unambiguous. 2. Unclear meaning of \"P(li=lj) is overestimated as 1\" Based on Section 2.1, li is given for i=1,\u2026,K. Hence, P(li=lj)=1 when i=j, and P(li=lj) is a deterministic quantity (the fraction of labels over N data points on which humans i and j agree). What does it mean to \"estimate\" or \"overestimate\" P(li=lj)? 3. Clarity and restrictiveness of the assumption in Lemma 2 Lemma 2 assumes that P(li=lj)\u22651/Nc. Should this be for every pair of humans i and j? I am also unclear on how to interpret this assumption. One interpretation is, assuming the dataset is a random sample, that P(li=lj is the probability that i and j agree on all labels in the dataset. This will likely be pretty low, so the assumption is unlikely to hold in practice (given that Nc is usually not very large). Another interpretation is that P(li=lj) is the fraction of labels on with i and j agree on a fixed dataset, in which case this assumption is not restrictive (annotator agreement rates are typically upwards of 70% in practice). **4. How are the bounds in Figure 3 calculated for the case of just one annotator?\" The formula for the lower bound does not make this clear. 5. Possible conflict between the assumptions of Theorem 1 and 3 Theorem 1 assumes that the human annotators are positively correlated. Theorem 3 assumes that even when the aggregated human annotation is incorrect, it is possible for the machine learning model to be predict the label correctly. Given that the machine learning model is trained to mimic human annotations, and that these annotations are correlated, it seems that these 2 assumptions are in conflict. Considering an extreme case (eg. humans are completely incorrect and strongly correlated, completely incorrect and weakly correlated, etc.) may help illuminate these assumptions better, and help evaluate whether they are applicable to a specific setting in practice. I believe (but am not sure), taken together, the proposed theorems rely on the human annotations being reasonably accurate and reasonably correlated (but not too correlated).   6. Minor Grammatical/Spelling Errors: \"Within this setting provide...\" \"along some labels\" \"for all of a pairs\" \"affects their decisions, and etc.\" \"we introduce another assumption equation\" \"in that as even\" This paper considers the important problem\u00a0of certifying whether an ML model achieves superhuman performance, and provides a principled approach to doing so. The proposed approach is clear and transparent in its assumptions. However, some of the notation makes it difficult to follow the paper, and the restrictiveness of the assumptions in practice is insufficiently explored. While both these drawbacks are significant, I believe they are addressable. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. Not applicable NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 19,
        "instruction": "Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning: Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy.  Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free.  Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it).  We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \\DPFEDREP\\ converges to a ball centered around the \\emph{global optimal} solution at a linear rate, and the radius of the ball is proportional to the reciprocal of the privacy budget.  With this novel utility analysis, we improve the SOTA utility-privacy trade-off for this problem by a factor of $\\sqrt{d}$, where $d$ is the input dimension.  We empirically evaluate our method with the image classification task on CIFAR10, CIFAR100, and EMNIST, and observe a significant performance improvement over the prior work under the same small privacy budget. The code can be found in this link, https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.",
        "reference": "The paper presents a new algorithm for DP federated learning (with trusted central server) under model personalization assumptions. The algorithm (CENTAUR) works by having the clients only send a subset of their parameters to the central server to be trained privately. The final (classification) layer is trained on each client non-privately. The authors provide rigorous analysis for the specific problem of linear representation learning (LRL) and further support the efficacy of their algorithm empirically with experiments. Strengths:  The paper provides a substantial rate improvement over existing work for the problem of LRL The paper provides sound empirical reasoning for their method (i.e. that data representations are less prone to distribution shift) The paper supports its results empirically  Weakness:  The O(d/n) rate is stated is for the matrix 2-norm but Jain et al. 2021 provide convergence guarantees for the Frobenious norm, making the comparison to existing work less clear. The proposed algorithm is not incredibly novel compared to prior work.  The presentation of the analysis for the theoretical results is a bit verbose. Theorem 5.1 and 5.2 are not easily interpreted on their own and it seems more useful instead to provide a more direct statement of Corollary 5.1. Given the rate improvement, an investigation into what DP lower bounds suggest is necessary would be nice. The paper makes precise statements and provides adequately detailed proofs. The experiments are likewise well documented. The paper makes progress for the problem model personalization under user level differential privacy and federated learning constraints. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 20,
        "instruction": "EquiMod: An Equivariance Module to Improve Visual Instance Discrimination: Recent self-supervised visual representation methods are closing the gap with supervised learning performance. Most of these successful methods rely on maximizing the similarity between embeddings of related synthetic inputs created through data augmentations. This can be seen as a task that encourages embeddings to leave out factors modified by these augmentations, i.e. to be invariant to them. However, this only considers one side of the trade-off in the choice of the augmentations: they need to strongly modify the images to avoid simple solution shortcut learning (e.g. using only color histograms), but on the other hand, augmentations-related information may be lacking in the representations for some downstream tasks (e.g. literature shows that color is important for bird and flower classification). Few recent works proposed to mitigate this problem of using only an invariance task by exploring some form of equivariance to augmentations. This has been performed by learning additional embeddings space(s), where some augmentation(s) cause embeddings to differ, yet in a non-controlled way. In this work, we introduce EquiMod a generic equivariance module that structures the learned latent space, in the sense that our module learns to predict the displacement in the embedding space caused by the augmentations. We show that applying that module to state-of-the-art invariance models, such as BYOL and SimCLR, increases the performances on the usual CIFAR10 and ImageNet datasets. Moreover, while our model could collapse to a trivial equivariance, i.e. invariance, we observe that it instead automatically learns to keep some augmentations-related information beneficial to the representations. ",
        "reference": "This paper proposes an equivariance regularizer as a modification to the usual invariance-inducing self-supervised losses. This is an interesting approach to enabling equivariance as there is no need to have a special architecture as prior work. The authors are able to train multiple self-supervised losses on a standard ResNet-50 and show good linear probe improvements over invariant baselines. Strengths:  A novel approach to enforcing equivariance is presented. The regularizer and modification to regular architectures is simple and effective.  The performance of the technique is shown on multiple losses and   Weaknesses/Questions:  In Eq. (4) why is j not shown on the numerator?   Is the only difference between the equivariance projection head and equivariant predictor the fact that the augmentation parameters are fed into the predictor? Otherwise equivariance projection head and predictor could have just been merged? Is equivariance projection head used elsewhere?  In Figure 3 and 4, what is the value of these measures for a regularly trained (invariant) model? This will tell us the benefit of adding the regularizer.   What is the reason that equivariance is stronger for color compared to other augmentations for ImageNet? Any insight into this behavior?  Did you try any finetuning experiments? The paper is easy to follow, clearly written, and the method is novel. I imagine that it will be easy to reproduce given the details in the paper. Given the overall novelty and clean/simple idea which seems to work well, I recommend acceptance. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 21,
        "instruction": "Task-Aware Information Routing from Common Representation Space in Lifelong Learning: Intelligent systems deployed in the real world suffer from catastrophic forgetting when exposed to a sequence of tasks. Humans, on the other hand, acquire, consolidate, and transfer knowledge between tasks that rarely interfere with the consolidated knowledge.  Accompanied by self-regulated neurogenesis, continual learning in the brain is governed by the rich set of neurophysiological processes that harbor different types of knowledge which are then integrated by the conscious processing. Thus, inspired by Global Workspace Theory of conscious information access in the brain, we propose TAMiL, a continual learning method that entails task-attention modules to capture task-specific information from the common representation space. We employ simple, undercomplete autoencoders to create a communication bottleneck between the common representation space and the global workspace, allowing only the task-relevant information to the global workspace, thereby greatly reducing task interference. Experimental results show that our method outperforms state-of-the-art rehearsal-based and dynamic sparse approaches and bridges the gap between fixed capacity and parameter isolation approaches while being scalable. We also show that our method effectively mitigates catastrophic forgetting while being well-calibrated with reduced task-recency bias.",
        "reference": "In this paper, the authors introduced TAMiL, a continual-learning model inspired by the global workspace theory that can learn multiple tasks without catastrophic forgetting by constructing a common representation space across tasks. By combining previous approaches on self-regulated neurogenesis and experience replay, TAMiL outperformed current state-of-the-art rehearsal-based methods as well as popular regularization-based methods on Seq-CIFAR10, Seq-CIFAR 100 and Seq-TinyImageNet, both in Class-Incremental Learning setting and and Task-incremental Learning setting. The basic unit of TAMiL, TAM, can also be flexibly augmented to previous rehearsal-based methods to boost performance. Strengths:  This paper is well-written and the figures are easily digestible. The baseline models included a wide range of selections, and varied in buffer sizes. TAMiL applies the global workspace theory, a longstanding neuroscience theory for consciousness, to the continual learning setting, which is quite a novel approach.  Weaknesses:  The concept of global workspace is influential to the field of cognitive neuroscience, and this paper shows great novelty by taking inspiration from it. However, exactly how the global workspace is mathematically defined, constructed and used was not explained well enough in this paper, unlike the common representation space which the author explains in great detail. Moreover, since the global workspace theory has been linked to many neuroscience findings (Mashour et al., 2020), it would be interesting to draw potential connections between TAMiL and the neural circuits underlying the ignition event.  Questions:  Figure 1 bottom: is Lp the same as Lpd, i.e. the pairwise discrepancy loss? What are the transformation coefficients mentioned in section 3.4 second paragraph, and where does it fit in Figure 1? The work is mostly clearly communicated, though it would be even better if Figure 1 could be referred to more frequently in Section 3 of the main text. For example, the color coding in Figure 1 wasn\u2019t very clear to me and I couldn\u2019t find much detail about it in the main text. The work combines two common approaches in continual learning, namely replay and regularization, thus is quite novel. The training details are provided in the appendix, thus the work should be reproducible upon code release. I lean slightly towards accepting this paper for ICLR: the proposed model, inspired by the global workspace theory, robustly outperforms state-of-the-art continual learning models in many settings. Ablation experiments also provided insights into the importance of each of the components of the model. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 22,
        "instruction": "CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code: Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account source code specifics. We propose subtokenziation that reduces average length by 17--40% without downstream performance drop, and show that a carefully chosen subtokenization may improve  quality by 0.5-2%, possibly with some length increase.",
        "reference": "The authors of this paper propose various subtokenization strategies affecting input string length in a way so as to improve the efficiency of LLMs when trained on source code and when applied to downstream tasks such as code generation, code summarization and code clone detection. Authors propose a strategy the restricts vocabulary size as well as compresses lengths of inputs without affecting performance via several combination strategies in the UnigramLM and BPE tokenization schemas. The main contribution of this paper is to study the impact of tokenization for source code based applications. Strengths  this paper studies an interesting problem. authors consider code from multiple programming languages.  Weaknesses  this paper is a little hard to follow at times, writing could be clear and so could the organization of sections in the paper. the paper primarily compares UnigramLM and BPE, with the authors claiming in the motivations that such tokenizers have demonstrated improvements in NL tasks. However, the authors fail to provide qualitative examples explaining why similar strategies achieve improvements on source code. while authors use the Sentencepiece vocabulary, they do not compare against Sentencepiece. I am not sure why the comparisons were restricted to UnigramLM and BPE alone. The paper is hard to read and follow in some instances. While it is known that vocabulary size and combination steps in BPE style algorithms impact the performance of a LM, it is hard to fully understand why it is the case for source code and exactly what gap are these general NL techniques addressing on source code. Experiments on their own seem easy to reproduce. While the problem is interesting, the presented analysis fails to provide insight into how proposed subtokenization strategies exactly help with source code. Furthermore, I find it a little puzzling why comparisons against Wordpiece were omitted. Similarly while the point was to demonstrate the strengths of subword tokenizers, commentary on how they compare against other tokenization schema, i.e whitespace etc would be nice to see given that the vocabulary of source code is fairly limited. On the whole while I see the experiments as thorough I am still unsure as to why these results need to be considered as novel. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 23,
        "instruction": "Few-Shot Domain Adaptation For End-to-End Communication: The problem of end-to-end learning of a communication system using an autoencoder -- consisting of an encoder, channel, and decoder modeled using neural networks -- has recently been shown to be an effective approach. A challenge faced in the practical adoption of this learning approach is that under changing channel conditions (e.g. a wireless link), it requires frequent retraining of the autoencoder in order to maintain a low decoding error rate. Since retraining is both time consuming and requires a large number of samples, it becomes impractical when the channel distribution is changing quickly. We propose to address this problem using a fast and sample-efficient (few-shot) domain adaptation method that does not change the encoder and decoder networks. Different from conventional training-time unsupervised or semi-supervised domain adaptation, here we have a trained autoencoder from a source distribution that we want to adapt (at test time) to a target distribution using only a small labeled dataset, and no unlabeled data. We focus on a generative channel model based on the Gaussian mixture density network (MDN), and propose a regularized, parameter-efficient adaptation of the MDN using a set of affine transformations. The learned affine transformations are then used to design an optimal transformation at the decoder input to compensate for the distribution shift, and effectively present to the decoder inputs close to the source distribution. Experiments on many simulated distribution changes common to the wireless setting, and a real mmWave FPGA testbed demonstrate the effectiveness of our method at adaptation using very few target domain samples~\\footnote{Code for our work: \\url{https://github.com/jayaram-r/domain-adaptation-autoencoder}}.",
        "reference": "The paper addresses the problem of handling domain-shifts that arises in generative learnt channel models in E2E communication systems in a few-shot setting. The proposed domain adaptation approach is tailored around a Mixture Density Network (MDN) representing the channel model. In here, the approach: learns an adapter layer, which models an affine transform of the original conditional channel distribution introduces an additional regularization objective to ensure the adapter doesn't converge to bad/degenerate solutions presents a feature transformation formulation on the decoder side to aid learning on the domain-shifted distributions   The approach is evaluated extensively, covering multiple types of distribution changes in both synthethic settings as well on a high-resolution mmWave testbed. Strengths 1. Extensive evaluation  The approach is evaluated rigorously with well-suited baselines and a range of scenarios (e.g., multiple types of domain shifts, real-world evaluation). I especially appreciate evaluations studying when the (reasonable) assumptions are violated.  2. Motivation and relevant problem  While there has been a lot of attention on generative channel modelling recently, most works in my knowledge largely (and somewhat incorrectly) assume a stationary distribution. This paper takes a step in the right direction by addressing this pain-point.  3. Insightful approach  The approach overall is insightful and makes sense. By learning an adapter network and learning parameters relevant for the domain shifts (e.g., like FiLM modules), it makes few-shot domain-adaptation more tractable. Furthermore, I find the choice of the channel model representation (MDNs) to also be sufficiently appropriate for the task (as opposed to GANs) for this study.  Concerns 1. \"labeled set obtained for free\"  The paper at multiple times claims that few-shot learning is especially possible since we can get labeled dataset for free -- I find this slightly confusing. Wouldn't the labeled dataset be split between the encoder (transmitter) and decoder (receiver) devices? As a result, for a party to have the full labeled dataset, isn't a prerequisite communicating labels back to the other party?  2. Evaluation: Some observations unclear  I found some patterns in the evaluation was somewhat unclear and would appreciate the authors' answers on the questions below: (a) Oracle-approach gap in Figure 4/5: I'm slightly surprised that proposed approach's symbol error rate does not converge to the oracle with a reasonable number of additional examples (50 * 16-QAM classes = 800), given that there are 50 learnable parameters. Are the authors aware if convergence is possible with even higher examples? Morevover, what is the size of the source dataset? (b) Unchanged error rates in Figure 4/5 for many baselines: Are the authors aware of why the error rates of many baselines do not improve at all in spite of more training examples? Were the \"finetune\" baselines finedtuned only on the new data or a combination? In the case of combination, are domain-invariant features learnt?  (nitpick) Please summarize the performance degradation discussions in Ricean fading experiments in the main paper.  3. Evaluation: Performance under no distribution change  I appreciate that the authors also evaluate under a non-domain shifted dataset in Figure 10. Can the authors clarify why results drop in performance when there is no distribution change? Specifically, it appears that the adapter layers' parameters are initialized such that it produces a identity mapping (page 18), so I'm surprised that this nonetheless degrades performance.  4. SNR=14-20 dB  Can the authors comment whether a SNR of 14-20dB (which to me appears really large) is a reasonable setting? Did the authors also evaluate SNR vs. error rates for the approach and baselines? I wonder if the results shown here apply only in high SNR regimes. Clarity: Good. It was generally easy reading the paper, thanks to really crisp text and a comprehensive background section. The minor issue I found is that some patterns in the results are not discussed (see concern 2, 3) The only nitpick I have are the figures (esp. Figures 4-6) where legends are highly illegible. Quality: Good. While there are minor discrepancies the approach (e.g., performance slightly deteriorates when there is no distribution change, does not translate well to certain distribution changes), I think it can be overlooked in light of the remaining contributions. Novelty: Very good. The authors tackle a very well motivated problem (see strength 2) and propose an insightful approach to tackle it (see strength 3). Reproducibility: Very good. The main paper (esp. the large appendix) appears to contain many details of the approach. Additionally, the code is provided as well. I'm not sure if the authors plan to release the channels from the mmWave FPGA testbed. The paper tackles a relevant bottleneck in generative channel modelling for E2E communication systems (i.e., they are trained assuming a stationary distribution, but this isn't the typical case). The approach is novel and intuitive in my opinion, and is further evaluated extensively in both simulated and real conditions. While I have some minor concerns (e.g., can one really have a labelled dataset for this task in practise?), I don't think they significantly affect the paper's claims and contributions. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 4: The contributions are significant, and do not exist in prior works. 4: The contributions are significant, and do not exist in prior works. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 24,
        "instruction": "FairGBM: Gradient Boosting with Fairness Constraints: Tabular data is prevalent in many high-stakes domains, such as financial services or public policy. Gradient Boosted Decision Trees (GBDT) are popular in these settings due to their scalability, performance, and low training cost. While fairness in these domains is a foremost concern, existing in-processing Fair ML methods are either incompatible with GBDT, or incur in significant performance losses while taking considerably longer to train. We present FairGBM, a dual ascent learning framework for training GBDT under fairness constraints, with little to no impact on predictive performance when compared to unconstrained GBDT. Since observational fairness metrics are non-differentiable, we propose smooth convex error rate proxies for common fairness criteria, enabling gradient-based optimization using a ``proxy-Lagrangian'' formulation. Our implementation shows an order of magnitude speedup in training time relative to related work, a pivotal aspect to foster the widespread adoption of FairGBM by real-world practitioners.",
        "reference": "The paper proposed a new FairGBM method to train GBDT under fairness constraints that shows little impact to predictive performance but improved fairness metrics as compared to unconstrained GBDT. The major challenge in the Fairness constraint is from the non-differentiable property of fairness notion, which is resolved by the proxy Lagrangian in this paper. Afterwards, the problem is resolve under the two-player game formulation where a descent step optimizes the loss and a ascent step ensures the fairness. Numerical results on ACSIncome-Adult and AOF dataset show that FairGBM proposed from the paper have better trade-off among Fairness, Performance, and Efficiency. Strength [+] The problem is well motivated. Fairness is an important topics and designing a optimization framework in GBDT to ensure fairness is an interesting and important area. Weakness [-] FairGBM is a modified version of LightGBM with fairness constraint. From the experimental results in Table 2, it shows FairGBM is better than lightGBM in both performance and fairness. Could the author helps to clarify the source of improvement of performance? It seems contra intuitive that a fairness constraint algorithm outperforms a unconstraint algorithm that purely optimize for performance. [-] Could the author helps to confirm fairness metrics used in evaluation and model training (\\tiltle(c)_i in equation 7)? In figure 2, the fairness metrics is different across dataset, FNR for ACSIncome-Adult and FRR for Account Opening Fraud. Is different fairness constraint selected when algorithm is applied on different datasets? Overall, paper is good written and well motivated. Overall the paper proposed a fairGBM with a proxy Lagrangian formulation. The problem is well motivated and solution is reasonable sound. There are some doubts on the experimental results, while overall results demonstrate the efficiency of the new proposed algorithm. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 25,
        "instruction": "Online Bias Correction for Task-Free Continual Learning: Task-free continual learning is the machine-learning setting where a model is trained online with data generated by a nonstationary stream. Conventional wisdom suggests that, in this setting, models are trained using an approach called experience replay, where the risk is computed both with respect to current stream observations and to a small subset of past observations. In this work, we explain both theoretically and empirically how experience replay biases the outputs of the model towards recent stream observations. Moreover, we propose a simple approach to mitigate this bias online, by changing how the output layer of the model is optimized. We show that our approach improves significantly the learning performance of experience-replay approaches over different datasets. Our findings suggest that, when performing experience replay, the output layer of the model should be optimized separately from the preceding layers.",
        "reference": "Paper proposes a method to correct a recency bias in replay-based task-free continual learning, but separately optimising the final connected layer  of network from the rest of the network. Focuses on continual learning in vision with evaluations in the area. Strengths  A very simple but highly effective method for bias correction in task-free continual learning Ablates the effect of various components (e.g. utility of surrogate classifier). Well written paper with decent set of evaluations in the vision space. Good comparison to baselines and shows particularly good performance with respect to the bias metric proposed, and another metric proposed by Cai et al (2021).  Weaknesses  Lack of understanding into why various components work despite ablations. This makes it difficult to find as much value in the paper, given the focus on toy datasets. Concretely, in section 4.5 it details why a surrogate classifier is necessary. However, there is no exposition on why surrogate classifier is necessary. There is a reference to gradient flow and figure 1 but nothing beyond that.  Minor Comments & Questions  Provide a name for the bias metric, and then use that name in the tables Highlight performance of OBC in tables. Clear and should be easily easy to reproduce given the simplicity. A simple method to mitigate the effect of the recency bias in task-free continual learning. Well written paper. Accept. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 26,
        "instruction": "Don\u2019t fear the unlabelled: safe semi-supervised learning via debiasing: Semi-supervised learning (SSL) provides an effective means of leveraging unlabelled data to improve a model\u2019s performance. Even though the domain has received a considerable amount of attention in the past years, most methods present the common drawback of lacking theoretical guarantees. Our starting point is to notice that the estimate of the risk that most discriminative SSL methods minimise is biased, even asymptotically. This bias impedes the use of standard statistical learning theory and can hurt empirical performance. We propose a simple way of removing the bias. Our debiasing approach is straightforward to implement and applicable to most deep SSL methods.  We provide simple theoretical guarantees on the trustworthiness of these modified methods, without having to rely on the strong assumptions on the data distribution that SSL theory usually requires. In particular, we provide generalisation error bounds for the proposed methods. We evaluate debiased versions of different existing SSL methods, such as the Pseudo-label method and Fixmatch, and show that debiasing can compete with classic deep SSL techniques in various settings by providing better calibrated models. Additionally, we provide a theoretical explanation of the intuition of the popular SSL methods.  An implementation of a debiased version of Fixmatch is available at\nhttps://github.com/HugoSchmutz/DeFixmatch",
        "reference": "This work identifies that the existing approaches in semi-supervised learning minimize a biased risk and hence, are devoid of theoretical guarantees unless there are strong assumptions. This works gives a method to de-bias the loss with a simple estimator which also allows them to give theoretical guarantees on the risk. They also evaluate their debiased versions of existing semi supervised learning approaches on various datasets and show that they lead to improved calibration and improved accuracy in certain settings. Semi-supervised learning is an interesting area and coming up with theoretically sounds methods is an important problem. The idea of debasing the risk is simple and interesting and also leads to an unbiased estimator of the risk with theoretical guarantees. The toy example provided is appealing. Their method also leads to improved calibration and better accuracy on certain subgroups. The main weakness is that their method does not lead to improved accuracy on many of the datasets. It would be nice to have some discussion of the properties of the dataset which leads this method to outperform others in terms of accuracy.  They also show that the debiased version of Fixmatch leads to better accuracy but not the debased version of Pseudolabels. Is there some reason for why this is the case? It would also be good to discuss why their method leads to better accuracy on certain subgroups when using Fixmatch. The paper is well written and clear. They have included the relevant prior work. The idea of computing the entropy term on the labelled data as well to make the loss estimator unbiased is novel in this context to the best of my knowledge.  They have also released the code for their experiments. The idea is interesting, simple and natural with theoretical guarantees. The paper is very well written. But, I think the experimental section is weak as discussed above. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. 2: The contributions are only marginally significant or novel. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 27,
        "instruction": "Learning a Data-Driven Policy Network for Pre-Training Automated Feature Engineering: Feature engineering is widely acknowledged to be pivotal in tabular data analysis and prediction. Automated feature engineering (AutoFE) emerged to automate this process managed by experienced data scientists and engineers conventionally. In this area, most \u2014 if not all \u2014 prior work adopted an identical framework from the neural architecture search (NAS) method. While feasible, we posit that the NAS framework very much contradicts the way how human experts cope with the data since the inherent Markov decision process (MDP) setup differs. We point out that its data-unobserved setup consequentially results in an incapability to generalize across different datasets as well as also high computational cost. This paper proposes a novel AutoFE framework Feature Set Data-Driven Search (FETCH), a pipeline mainly for feature generation and selection. Notably, FETCH is built on a brand-new data-driven MDP setup using the tabular dataset as the state fed into the policy network. Further, we posit that the crucial merit of FETCH is its transferability where the yielded policy network trained on a variety of datasets is indeed capable to enact feature engineering on unseen data, without requiring additional exploration. To the best of our knowledge, this is a pioneer attempt to build a tabular data pre-training paradigm via AutoFE. Extensive experiments show that FETCH systematically surpasses the current state-of-the-art AutoFE methods and validates the transferability of AutoFE pre-training.",
        "reference": "This paper proposes a first of its kind architecture framework for automated feature engineering called Fetch, a system based on brand new data driven Markov decision process. The authors identify critical gaps by stating the current methods for AutoFe are insufficient when it comes resembling human effort in handling datasets as the underlying Markov decision setup is different i.e. more based on trial and error, which leads to poor generalization across datasets and higher computational costs. The new method also has a key element of transferability, which is basically the ability to enable feature engineering on new datasets using prior policy networks trained on previous datasets, without the need for additional data exploration. The authors present evidence that Fetch is superior to the existing state of the art automated feature engineering methods such as Random, DFS, AutoFeat, NFS, DIFER as well as AutoML methods such as AutoSklearn and AutoGluon. The method is also tested for transferability by application on several datasets.  The authors argue that the approach comes very close to mimicking human experts when it comes to handling new datasets when it comes to transferring experience. Strengths:   One of the key strengths of this paper is its ability to provide transferability between datasets which previous methods have not been able to provide.  The method is highly flexible as it has been applied on different datasets as well different ML models.  Overall there is sound discussion relevant work, gap analysis to find opportunity of development, strong mathematical rigour and experimental setup, evaluation and analysis.  Weaknesses:   The authors declare that to the best of their knowledge there aren't any autoFE/autoML workarounds for managing tabular data to accomplish transferability. However this could be supported by a stronger statement that can more concretely say if such methods exist or not by clearly stating a comprehensive survey of analysis did not yield any methods.  This method is specifically geared towards tabular datasets. While tabular datasets prevail in several key applications, it would be interesting to know how this method applies/does not apply to any other form of datasets (eg. images, speech, unstructured datasets). At least there could be some discussion on consideration, if not a full scale evaluation. Novelty:   Paper proposes a new form of markov decision process to generate features. Previous methods take number of features and only update using sequences of previous actions, iteratively. Whereas Fetch provides Feature engineering actions and constructions actions, instead of features, iteratively based on the newly generated datasets (features).  According to this paper, Previous methods for AutoFE have never explored transferability between datasets, which has been done by this method.  Clarity & Quality: The paper has high clarity with explicit background provided and methods explained with sufficient depth both in terms of mathematical equations, datasets and analysis of results. There is further in-depth explanation in the appendix section around usage of material and datasets which is crystal clear.  Reproducibility: The method has been extensively tested on publicly available datasets which are linked. Algorithm and methods have been provided as well. The paper states that all experiments have been run with open source code provided. I propose that we accept this paper on the basis of its merits around developing a new AutoFE method based on a novel markov decision process which not only is superior to the existing state of the art in terms of performance but also has a unique feature of transferability. Both strong points are well argued and evidenced in the paper throughout with sufficient gap analysis around opportunity areas, discussion of background and related work, strong mathematical rigor and sound experimental design setup, with thorough data analysis, discussion of evaluation and results. Although the work is limited to tabular data there is thoughtful coverage on several popular datasets including regression and classification examples and comparison to other AutoFE models and coverage across several ML models. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO.Details Of Ethics Concerns: None. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 28,
        "instruction": "Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples: The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https://github.com/qizhangli/MoreBayesian-attack. ",
        "reference": "This paper proposes to attack a Bayesian model for improving the transferability of black-box adversarial attacks. Specifically, the authors employ gradient-based attack algorithms on their constructed Bayesian models and expect the generated adversarial attacks can better fool other unseen models. Extensive experiments show that the proposed method surpassed baseline methods in terms of attack success rate. Strength  In general the paper is well-written. The authors conduct extensive experiments to validate the transferability of the proposed method by concerning many different neural architectures and multiple baselines. The proposed way to employ Bayesian estimation for improving the transferability of attacks looks novel to me.  Weaknesses  When both the substitute model and the victim model are adversarially trained (Table 6), the improvement is quite marginal. Therefore, the proposed method may only be employed to improve the transferability of attacks on nonrobust models, which weakens the empirical significance. In Eq. 6, \u2206w is sampled from the gaussian prior. Then why do we need p(\u2206w) \u2265 \u03b5? Plus, if p(\u2206w) \u2265 \u03b5 is important, then \u03b5 should be a very important hyper-parameter but there is no experiment showing the sensitivity, The Clarity is good. The proposed way to employ Bayesian estimation for attack transferability looks novel. The authors claimed that the code will be available. Given the strength and weaknesses, I tend to rate the paper as marginally above the acceptance. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 29,
        "instruction": "Learning Group Importance using the Differentiable Hypergeometric Distribution: Partitioning a set of elements into subsets of a priori unknown sizes is essential in many applications. These subset sizes are rarely explicitly learned - be it the cluster sizes in clustering applications or the number of shared versus independent generative latent factors in weakly-supervised learning. Probability distributions over correct combinations of subset sizes are non-differentiable due to hard constraints, which prohibit gradient-based optimization. In this work, we propose the differentiable hypergeometric distribution. The hypergeometric distribution models the probability of different group sizes based on their relative importance. We introduce reparameterizable gradients to learn the importance between groups and highlight the advantage of explicitly learning the size of subsets in two typical applications: weakly-supervised learning and clustering. In both applications, we outperform previous approaches, which rely on suboptimal heuristics to model the unknown size of groups.",
        "reference": "The paper proposes an approximation of the multivariate non-central hypergeometric distribution in which, first the multivariate distribution is expressed through the product rule as a product of conditional distributions, each of which can be treated as a univariate non-central hypergeometric distribution by grouping the remaining (un-accommodated/sampled) classes into one. Finally, these univariate non-central hypergreometric distributions are approximated using the Gumbel-Softmax approximation technique. This makes the differentiation of the joint distribution with respect to the class importance weights feasible. The paper concludes with some experiments; first showing the accuracy of the approximation through simulating from the true and approximate distributions, and then in two real applications where it shows superiority to some existing methods in learning numbers of shared and independent latent generative factors from coupled image observations and in clustering, where the proposed distribution is used as a tunable prior for a VAE. Strengths:  The work investigates a relatively under-studied topic in machine learning where alternatives perform modelling based on i.i.d. sampling and may, at best, use post-hoc assessment of group sizes or similar to infer importance weights. By building the importance weights into the estimation both inference and estimation can be improved. The empirical results show that the method can offer improvements on existing methods in some interesting and relevant problems.  Weaknesses:  The main weakness of the paper is that it expects a lot of knowledge from the reader, and many \"for details see...\" pieces of text which may make the work not stand well along as a single item of literature. There are also potential issues with clarity, where while the individual points are clear it is not immediately forthcoming how the optimisation rendered possible by the differentiability of the mass function is implemented. For the most part the paper is very readable and most individual points are reasonably clear, however, as hinted above the paper could benefit from an extremely simple application in which the optimisation which is rendered possible by the proposal is described explicitly.  Having said this, the actual description of the algorithm, and the sampling procedure for generating realisations from the distribution is clear and, having some experience with other machinery described in the experiments I believe the results are reproducible. The paper covers an interesting and under-explored topic in machine learning, and the proposed method seems to offer options for modelling, estimation and inference which are otherwise quite limited to heuristics. Although the paper is very readable and the details are well explained, as a reader one might feel still somewhat at a loss for some of the higher level practical aspects of the proposed approximation.  The experiments show that the proposed distribution has realisable benefits in some important and interesting applications over existing alternatives and more naive straightforward formulations. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 4: The contributions are significant, and do not exist in prior works. NO. 8: accept, good paper 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 30,
        "instruction": "Cross-Layer Retrospective Retrieving via Layer Attention: More and more evidence has shown that strengthening layer interactions can enhance the representation power of a deep neural network, while self-attention excels at learning interdependencies by retrieving query-activated information. Motivated by this, we devise a cross-layer attention mechanism, called multi-head recurrent layer attention (MRLA), that sends a query representation of the current layer to all previous layers to retrieve query-related information from different levels of receptive fields. A light-weighted version of MRLA is also proposed to reduce the quadratic computation cost. The proposed layer attention mechanism can enrich the representation power of many state-of-the-art vision networks, including CNNs and vision transformers. Its effectiveness has been extensively evaluated in image classification, object detection and instance segmentation tasks, where improvements can be consistently observed. For example, our MRLA can improve 1.6% Top-1 accuracy on ResNet-50, while only introducing 0.16M parameters and 0.07B FLOPs. Surprisingly, it can boost the performances by a large margin of 3-4% box AP and mask AP in dense prediction tasks. Our code is available at https://github.com/joyfang1106/MRLA.",
        "reference": "This paper proposes a novel method for cross-layer interaction, which complements current mainstream networks emphasizing the interaction within a layer. Taking advantage of the attention mechanism, the proposed method enhances the layer interaction via attention. An efficient implementation is also introduced to avoid the vanilla quadratic complexity. Experimental results demonstrate the effectiveness of the proposed method. [ Strength ]  The motivation is very attractive. It is common that most of existing networks, including Transformers, only focus on the interaction within a certain layer. Even though ResNet and DenseNet, as analyzed by the authors, put some emphasis on layer interaction, the way they used (i.e., addition and/or concatenation) is a little bit naive and hard. The proposed method takes advantage of the attention mechanism and makes the cross-layer interaction more technically rational.  The method is elegant and the presentation logic is clear. It is quite straightforward to understand each part of the method section. The proposed efficient implementation of layer attention, just like linear attention in efficient Transformers, is useful and necessary.  Sufficient experiments demonstrate the effectiveness of the proposed method. The performance on image classification, object detection and instance segmentation indicates that the method is indeed effective and efficient.  [ Weakness ]  On image classification, the input resolution is identically set as 224. What if enlarging the resolution? In fact, it is quite important to compare the accuracy and efficiency with various resolutions to reveal the robustness. Of course, it is necessary to check whether the complexity is linear (or nearly linear) to the input resolution. Visual comparison is missing. The proposed cross-layer attention can enhance the interaction between layers, but there is no strong evidence specially for this interaction. For example, are local features from shallow layers transferred to top layers? How do high-level features facilitate the feature representation in shallow layers? These would need visual comparison. The paper is of high quality including its clear motivation, good novelty and originality, and logical presentation. I think the paper is quite good, and I recommend to accept the paper if the authors can address my two primary concerns above. Also, I would like to have further communication with other reviewers and the AC. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 31,
        "instruction": "Decision S4: Efficient Sequence-Based RL via State Spaces Layers: Recently, sequence learning methods have been applied to the problem of off-policy\nReinforcement Learning, including the seminal work on Decision Transformers,\nwhich employs transformers for this task. Since transformers are parameter-heavy,\ncannot benefit from history longer than a fixed window size, and are not computed\nusing recurrence, we set out to investigate the suitability of the S4 family of\nmodels, which are based on state-space layers and have been shown to outperform\ntransformers, especially in modeling long-range dependencies. In this work, we\npresent two main algorithms: (i) an off-policy training procedure that works with\ntrajectories, while still maintaining the training efficiency of the S4 model. (ii) An\non-policy training procedure that is trained in a recurrent manner, benefits from\nlong-range dependencies, and is based on a novel stable actor-critic mechanism.\nOur results indicate that our method outperforms multiple variants of decision\ntransformers, as well as the other baseline methods on most tasks, while reducing\nthe latency, number of parameters, and training time by several orders of magnitude,\nmaking our approach more suitable for real-world RL",
        "reference": "This paper presents an offline reinforcement learning approach which is able to capture longer range dependencies than a traditional sequence modeling approaches such as a Decision Transformer (DT). Similar to DT, Decision S4 views RL as a sequence modeling problem, but using the implicit S4 model instead of attention blocks. The offline approach is trained by taking individual transitions and rewards to go, and predicting the actions. The paper also introduces an offline-to-online training approach by first freezing the actor and then the critic, as well as freezing the S4 kernel. Results show that a much smaller model can achieve a similar performance to DT on offline RL tasks. Strengths:   To my knowledge this paper presents a novel approach to tackle offline RL Applications of the S4/LSSM architectures to control and RL are under-explored and could be beneficial for the community  S4 does provide almost similar performance to DT with fewer parameters  Offline-to-online finetuning approach works quite well  paper is well written  Weaknesses:   I don't particularly understand why the offline training is capturing long range dependencies, if it is predicting a single action at a time conditioned on the current state and reward to go? Where is this captured? It's important to explain this well.  How long does it take to train the model vs DT and what is the inference time vs DT?  I would be willing to increase my rating if the questions above are answered.  I would be willing to incre Paper is clear, presents a novel approach and is well written. This paper provides an interesting use of the S4 parameterization and shows that the method works well compared to alternatives (such as Decision Transformer). Details and explanations about what the S4 blocks actually capture in the context of RL are missing which would make the claims in the paper stronger. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO.Details Of Ethics Concerns: N/A 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 32,
        "instruction": "Unveiling the sampling density in non-uniform geometric graphs: A powerful framework for studying graphs is to consider them as geometric graphs: nodes are randomly sampled from an underlying metric space, and any pair of nodes is connected if their distance is less than a specified neighborhood radius. Currently, the literature mostly focuses on uniform sampling and constant neighborhood radius. However, real-world graphs are likely to be better represented by a model in which the sampling density and the neighborhood radius can both vary over the latent space. For instance, in a social network communities can be modeled as densely sampled areas, and hubs as nodes with larger neighborhood radius. In this work, we first perform a rigorous mathematical analysis of this (more general) class of models, including derivations of the resulting graph shift operators. The key insight is that graph shift operators should be corrected in order to avoid potential distortions introduced by the non-uniform sampling. Then, we develop methods to estimate the unknown sampling density in a self-supervised fashion.\u00a0 Finally, we present exemplary applications in which the learnt density is used to 1) correct the graph shift operator and improve performance on a variety of tasks, 2) improve pooling, and 3) extract knowledge from networks. Our experimental findings support our theory and provide strong evidence for our model.",
        "reference": "This work considers geometric graphs having both non-uniform sampling density, as well varying neighborhood radius. Under this model, a GSO can be though of as a discretization of the latent continuous Laplacian. In order for this GSO to approximate the continuous laplacian, the adjacency matrix needs to be normalized according to the sampling density. The non-uniform geometric graph model considered here seems a plausible model for real-world graphs. Of course, though, the sampling density is not known in practice. For this reason, it estimates the sampling density using a NN in a self-supervised manner. Experiments on synthetic datasets, where the ground truth sampling density is known, show that indeed this NN approach can well approximate the true underlying sampling density. Finally, it concludes with a set of experiments that seem to validate the hypotheses stated earlier in this work. More precisely, performance is improved in classification tasks, while learning density values can be used as a way to assign importance scores to nodes. After the introduction of the non-uniform geometric graph model, the resulting GSO is properly derived with a bound on the convergence of it to the continuous laplacian. There is an admirable effort to justify the reasoning behind using the proposed geometric model through experiments where the decoder is restricted to be a geometric graph with hubs and with experiments that try to demonstrate the applicability of this work in practical tasks. However, as the work builds on the assumption of slowly changing density functions and piece-wise neighborhood radius, there should be a better discussion of it with other models, e.g. in statistical models there is a notion of popularity/expansiveness that differs for each node [1]. From a clarity perspective, this work is, in my opinion, weakly motivated. For example, the relevant section starts with a discussion on ways to compare graphs for similarity. It is not clear how this is connected with this work. Moreover, while GSO are central to this work, the definition on them is not repeated in order to start a discussion on how they are used and why this approach can be better than other approaches. In general, reading this work leaves you with a question-mark on what the main target of it is. Moreover, in the experimental section regarding classification accuracy there is no comparison with works that learn a parameterized form of GSO (like the one of dasoulas et al. cited in this work). It is a question of whether we could have both pgso and this work combined. Finally, for the learned 1/\u03c1, of the real-world networks, it would be interesting to see some plots and how they are different from let's say 1/deg.  [1] https://arxiv.org/pdf/0912.5410.pdf This work in general could be more clearly written, starting from motivating properly what this work is trying to do and why. For example, do we just want to derive a proper normalization of the adjacency matrix in a way that improves classification performance, or we want to explain in a theoretical grounded way why this normalization is necessary? Also, I think section 2 is quite verbose and each subsection does not seamlessly build on what is discussed in the previous one. E.g., section 2.2 starts by discussing again eq 1 with little reference to what was discussed right before (in deriving eq 2).  In terms of quality more could be done in the experimental section with respect to comparing with other methods for learning parameterized graph shift operators, or a discussion with other latent models that try to learn graphs and assume different levels of popularity for each node. In terms of originality of this work, it studies geometric graphs in a different context than it was done before. So far, geometric graphs were (mostly) assumed to be uniform as they were mostly studied as a theoretical model with concrete expressions on properties of them, e.g. degree distribution. Hence the uniformity assumption to derive such expression. From the GSO side, this work takes a rather different angle, rather than learning the parameters, it learns a way to normalize the adjacency matrix. I believe this work has some potential, though certain things need to be addressed. Mostly related to clarity, better motivating this work and focussing on the exact question/problem it tries to answer/solve. This will drive both the theoretical explanation and empirical evaluation in a more clear and concrete way. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 33,
        "instruction": "Boosting Causal Discovery via Adaptive Sample Reweighting: Under stringent model type and variable distribution assumptions, score-based causal discovery methods learn the directed acyclic graph (DAG) from observational data by evaluating candidate graphs over an averaged score function. Despite the great success in low-dimensional linear systems, it has been observed that these approaches overly exploits easier-to-fit samples, thus inevitably learning spurious edges. Worse still, the common homogeneity assumption of most causal discovery methods can be easily violated due to the widespread existence of heterogeneous data in the real world, resulting in performance vulnerability when noise distributions vary. We propose a simple yet effective model-agnostic framework to boost causal discovery performance by dynamically learning the adaptive weights for the Reweighted Score function, ReScore for short, where the learned weights tailors quantitatively to the important degree of each samples. Intuitively, we leverage the bilevel optimization scheme to alternatively train a standard DAG learner first, then upweight the samples that the DAG learner fails to fit well and downweight the samples that the DAG learner easily extracts the causation information from. Extensive experiments on both synthetic and real-world datasets are carried out to validate the effectiveness of ReScore. We observe consistent and significant boosts in structure learning performance. We further visualize that ReScore concurrently mitigates the influence of spurious edges and generalizes to heterogeneous data. Finally, we perform theoretical analysis to guarantee the structure identifiability and the weight adaptive properties of ReScore. Our codes are available at https://github.com/anzhang314/ReScore.",
        "reference": "This work addresses the problem of errors in structure learning algorithms.  The authors propose to reweight poorly fit samples in order to improve the efficacy of the underlying algorithm. A proof is shown under linear models for specific scoring rules in the asymptotic regime. Empirical results show the proposed method improving state of the art structure learning algorithms. Strengths:   The relative frailty of structure learning algorithms is a major roadblock to practical usage, so this task is very well motivated.  The authors present an algorithm which is simple and intuitive Empirical results are very compelling  Weaknesses:  The authors reweight samples with the intuition that these correspond to spurious edges, however it is not entirely clear to me why this should be limited to one reweighting step. Can the authors give some sort of intuition on why only one boosting step is used? What would happen if multiple reweighting steps were employed? Theorem one is incredibly limited in scope. I don't think this necessarily a problem but the authors should reconcile their claim that the approach is applicable to any score base learner with the assumptions that restrict the space of models and algorithms substantially.  The approach deals with recovering directed graphs, but the authors frame in terms of causal discovery. In the case of causal discovery, only an equivalence class is recovered, not a fully directed graph. Do the authors have suggestions or intuition regarding the modifications necessary to have the algorithm work when an algorithm (e.g., GES) returns an equivalence class after every step? Overall, I think the authors have written a clear and well organized paper. Outside of the issues that I raised I think the quality is quite good and the novelty is quite high, in my view. As I mentioned above, I think this is a well motivated and simple solution to a very compelling problem. I think the idea that the procedure can be applied a wide variety of algorithms is quite compelling. My reservations are listed above in the weaknesses, and is largely in the theoretical underpinnings and some of the details of the proposed approach. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 34,
        "instruction": "Iterative Circuit Repair Against Formal Specifications: We present a deep learning approach for repairing sequential circuits against formal specifications given in linear-time temporal logic (LTL). Given a defective circuit and its formal specification, we train Transformer models to output circuits that satisfy the corresponding specification. We propose a separated hierarchical Transformer for multimodal representation learning of the formal specification and the circuit. We introduce a data generation algorithm that enables generalization to more complex specifications and out-of-distribution datasets. In addition, our proposed repair mechanism significantly improves the automated synthesis of circuits from LTL specifications with Transformers. It improves the state-of-the-art by $6.8$ percentage points on held-out instances and $11.8$ percentage points on an out-of-distribution dataset from the annual reactive synthesis competition.",
        "reference": "Given a formal specification in LTL, this paper introduces a transformer architecture that aims to transform a defective circuit into a repaired one, in accordance to the spec.   The primary contribution is in the transformer neural architecture, which they call the separated hierarchical transformer since it has separate local layers that do not share model parameters.  In addition, they introduce a data generation procedure that models human errors to encourage generalization to more complex specifications. Main strengths:  Well motivated problem Mostly well written and easy to follow Comprehensive experimental analysis (with some caveats)  Main weaknesses:  Most of the value of this contribution rests upon whether the following causal claims are true and well-justified: the new architecture and/or the data-augmentation procedure caused the improvements in performance of state of the art.  Despite a number of different experimental analyses in the paper, determining this is not straightforward.  In particular, I cannot see where if anywhere the number of parameters are controlled for.  The paper does say that using separated heads leads to an increase in the number of parameters, but I do not see any evidence in this paper to suggest that performance increases over previous methods is not attributed simply to this network being larger.  Also, the experimental results do not allow us to distinguish whether improvements are from the model changes or from the data changes. I have some reservations about the use of Levenshtein distance as a metric for the quality of a synthesized circuit.  Obviously there is no \u201cright\u201d metric, and Levenshtein may be used in prior work, but a more semantic property could be used in addition to convey improvement.  For instance, one could try to look at (some approximation of) the increase or decrease in the number of satisfying traces The paper is mostly well written and straightforward to follow.  The exception are the figures, which would benefit from more information in the captions. The novelty of this paper is limited.  The main contributions are a modest variant on an existing hierarchical transformer architecture, and a straightforward data augmentation procedure.  Individually or together these do not seem substantially novel, but that by itself is not necessarily a problem. The description is precise enough to attempt to reproduce this work.  The authors state in several places that they will produce the source code for this work, which will improve reproducitibility further. Questions  Clarify \u201cfiltered out samples that exceed our model restrictions\u201d It\u2019s not clear whether \u201cafter last iteration\u201d in Table 1 means after exactly 2 iterations of after some arbitrary number. In Figure 4 has an \u201cerror\u201d label purple, but I do not see any corresponding purple in the graph itself.  Is this just too small to see on the graph? Clarify \u201caccuracy\u201d in Figures as semantic accuracy and provide explicit meaning to [0, 1] range. Well written paper with comprehensive experimental analysis Limited novelty Would like to see much more evidence that claimed contributions are responsible for improved performance 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 35,
        "instruction": "Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study: Large pre-trained language models have helped to achieve state of the art on a variety of NLP tasks, nevertheless, they still suffer from forgetting when incrementally learning a series of sequential tasks. To alleviate this problem, recent works propose several models enhanced by sparse experience replay and local adaption, which yield satisfactory performance. However, in this paper we find that pre-trained language models like BERT have a potential ability to learn sequentially, even without any sparse memory replay. To verify the ability of BERT to maintain old knowledge, we adopt and re-finetune single-layer probe networks with the parameters of BERT fixed. We investigate the models on two typical kinds of NLP tasks, text classification and extractive question answering. And our experiments reveal that BERT can actually generate high quality representations for previous tasks in a long term, under extremely sparse replay or even no replay. We further introduce a series of methods to interpret the mechanism of forgetting and how memory rehearsal plays a significant role in task incremental learning, which bridges the gap between our new discovery and previous studies about catastrophic forgetting. Additionally, we provide both quantified and visualized results demonstrating that the representation space of BERT is always topologically organised, which guarantees its performance.",
        "reference": "In this paper, the authors investigate whether or not pretrained language models like BERT have the ability to maintain previously learned knowledge in the long term. To do this, they track the encoding ability of BERT for specific tasks before, during, and after learning new tasks. They find that BERT can actually refrain from forgetting when learning a sequence of tasks, contrary to existing studies about catastrophic forgetting. The authors believe this is due to the fact that BERT has a strong potential to produce high-quality representations for previous tasks even without memory replay. They further investigate the topological structure of the learned representation sub-space within and among different tasks and find that forgetting can be interpreted as intra-task forgetting (forgetting what has been learned within a task) and inter-task forgetting (forgetting what has been learned across tasks). I like this paper, the results are neat, the exposition is clear (Section 4.2 could use some work as described later), and (almost) everything is easy to understand. But as I was reading this paper, I kept coming back to one paper [1] that I read last year which had done everything this paper is offering. They answered the same questions, they did an even more extensive set of experiments, and the only difference was in how they viewed why pretrained models maintain good representations even if the decoder needs to be retrained. I spent the last day reading both papers side by side and in my view, up until Section 4, there's hardly any difference between the two works. It appears that the authors may not be aware of [1] as it hasn't been cited in this paper so I invite the authors to read [1] and share if they disagree with my assessment. But in light of this, so far, I see the marginal value of this paper lying in Section 4 and would recommend the authors to amplify that as the core contribution of the paper (which is currently listed as their third contribution in Introduction), as the rest of their contributions (claimed contributions 1 and 2) have already been offered to the community previously. Additionally, please add standard errors in every result. [1] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. \"An empirical investigation of the role of pre-training in lifelong learning.\" arXiv preprint arXiv:2112.09153 (2021). Errata:  Introduction, first paragraph: \"learning the xxx one\". No idea what you meant to say here. Introduction, first paragraph: \"learning a sequecne of tasks\" ---> \"learning a sequence of tasks\" Section 3.2, second paragraph: Kingma and Ba (2015) should be \\citep not \\citet Section 4.1 \"decoders,pre-trained\" ---> \"decoders, pre-trained\" The paper is clear and easy to understand but Section 4.2 could use another pass (a strategy that works for me is to read every sentence out loud and ask what does it mean?). The novelty is marginal at best. I'd recommend the authors put a section in Appendix about implementation details and add standard errors to the results. I like the paper and found the results to be clear and easy to understand. However, my current view is that much of what was presented in this paper had already been answered in another paper last year which the authors may be unaware of (as it hasn't been cited in this work). The main difference between the two papers, in the my opinion, is in Section 4. I recommend that the authors emphasize Section 4 as the main contribution of the paper. 4: All of the claims and statements are well-supported and correct. 1: The contributions are neither significant nor novel. 1: The contributions are neither significant nor novel. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 36,
        "instruction": "Behavior Proximal Policy Optimization : Offline reinforcement learning (RL) is a challenging setting where existing off-policy actor-critic methods perform poorly due to overestimating of out-of-distribution state-action pairs. Thus, various additional augmentations are proposed to keep the learned policy close to the offline dataset (or the behavior policy). In this work, starting from the analysis of offline monotonic policy improvement, we reach a surprising conclusion that online on-policy algorithms are naturally able to solve offline RL. Specifically, the inherent conservatism of these on-policy algorithms is exactly what the offline RL method needs to overcome the overestimation. Based on this, we propose Behavior Proximal Policy Optimization (BPPO), which solves offline RL without any extra constraint or regularization introduced compared to PPO. Extensive experiments on the D4RL benchmark empirically show this extremely succinct method outperforms state-of-the-art offline RL algorithms. Our implementation is available at https://github.com/Dragon-Zhuang/BPPO.",
        "reference": "This paper addresses the problem of learning in the offline RL scenario. While existing approaches need to regularize objectives in order to learn a policy in proximity to the one used to collect data, the authors explore the use of online on-policy algorithms in order to solve such tasks. Their focus is on the policy improvement theorem, from which notable algorithms like TRPO or PPO can be derived. They first extend such a theorem to the offline setting, deriving a lower bound on the true performance that depends on additional terms related to the offline dataset. Then they extend such theorem to consider an improvement over an arbitrary learned policy. Based on these results, they derive an iterative algorithm that optimizes the PPO objective, but with a change in Importance Sampling ratios, where the denominator is constantly replaced by the current learned policy. To prevent this optimization process to diverge, they introduce a clipping decay mechanism. Experiments show that this approach is effective in solving offline tasks. Moreover, an ablation on different hyperparameters demonstrates the robustness of the method. Strengths:  The theoretical results seem correct and not trivial. The authors state the assumptions well before each theorem. The problem is significant. In particular, recent work showed that applying on-policy algorithms to offline RL problems can lead to surprising results. This paper goes one step further in such a direction. The experimental results show higher performance with respect to the baselines  Weaknesses:  Some parts of the paper seem written in a hurry and are unclear. For example, the first 5 lines after Eq. 11 are confusing. Other parts of the paper seem too colloquial. For instance \"simply augmenting TD3 (...) with behavior cloning (...) reminds us to revisit\", or \"the most tough thing is the existence of A\". There are claims that are not verified through experiments. In the abstract and in the introduction, it is stressed that existing off-policy actor-critic methods overestimate out-of-distribution actions. This motivates the use of a proximal objective. However, the paper is lacking an experiment showing that the proposed method does not suffer from this issue. I have a few concerns regarding the soundness of the method. The sequential optimization of bound (8) would lead to policy improvement. However, the approximation in Eq 13 has no guarantees to improve upon the behavioral policy. The only guarantees are given by Theorem 4, which does not involve the true objective. Can the authors clarify this? The comparison with Onestep BPPO, which consists of Behavioral Cloning + PPO, is not convincing. Did the authors take the tuned hyperparameters they found for their method and applied them to the Onestep method? The paper is lacking an important ablation on the clip coefficient decay and the asymmetric coefficient for the advantage: How does the method perform without such tricks? How does it perform with respect to Onestep PPO, when both methods are without tricks?  I kind of disagree with the authors in the main claim of the paper, which is that they discover that an online, on-policy method can solve offline RL problems. What the paper proposed, is instead an off-policy version of PPO that works for offline RL. Please note that PPO can already be considered an off-policy algorithm, since at each iteration, after the first policy gradient step, it is learning about a policy that was not used to collect the data. The clipped importance weight helps keep the optimization to be near-on-policy. BPPO further relaxes this constraint, letting the learned policy be able to be far from the behavioral policy.  Other questions:  The experiments involve continuous action spaces. How does this affect Assumption 1? I expect the value of \u03be to be 1. What is the value of the bounds proposed during training? What is the magnitude of each component in the bound during the experiments? At each iteration, the importance weight between the current policy and the previous one is clipped. Can the authors quantify instead the importance weight between the final learned policy and the behavioral policy? Can this value be arbitrarily large? The paper is mostly clear, apart from some sections that need to be further polished (see weaknesses). The theory seems of high quality, but the practical approximations make the proposed algorithm less sound. Further experiments are needed to improve the quality of the paper. To my knowledge, the results are novel. The authors specify the hyperparameters used in their implementation. This paper introduces an offline version of PPO that can be used to solve offline tasks. It is based on an offline version of the policy improvement theorem. Although the experiments show improvement, there are a few concerns regarding the soundness of the proposed approach and the experimental choice before acceptance. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 37,
        "instruction": "Actionable Neural Representations: Grid Cells from Minimal Constraints: To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an  `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.",
        "reference": "In this paper, the authors provided a solution of how to parametrically hard code grid cells with learnable parameters in order to be functional, actionable, and biological by minimizing the objective function. Under these three constraints, they showed that hexagonal grid cells are the optimal representation of locations in 1D angular and 2D euclidean space. Strength:  To-date, most computational models on grid cell emergence have focused on training a neural network to solve path-integration tasks and selecting grid cells based on their known neuroscientific properties. This creates a possibility, which also applies to other EC-HPC representations, that the neural representations are simply due to correlation with different behavioral variables (i.e. \u201cyou see what you\u2019re looking for\u201d). In this paper, the authors only imposed normative constraints rather than assumptions about known properties of grid cells, and still observed grid cells, which fills the gap in the literature on why EC-HPC representations exist in the first place.  Major weaknesses:  Though it provides a good model for grid cell emergence, this framework is somewhat isolated in that it doesn\u2019t consider the interactions between MEC (where grid cells are observed) and other regions in the EC-HPC circuit. For example, under this framework, do grid cells still help explain place cell remapping (Whittington et al., 2020)? The metric described in this paper (i.e. functional, biological, actionable) still does not account for whether the grid cells representations are useful for behavior, which in my opinion is what \u201cfunctional\u201d should mean. I think it would make the paper even stronger if you could show the connection between your model and path integration behavior, or some other behavioral task, eg. train model on path integration task and do lesion study. As raised in Schaeffer et al., 2022, grid cell emergence depends on hyperparameters and implementation choices. Is that also the case here? Eg. What happens when you increase the number of neurons? Or other hyperparameters listed in section B.3? It would be interesting to see whether the claim from Schaeffer et al. still holds when grid cells emerge from not path-integration but mathematical optimization. One can also see grid cells in non-spatial settings (Whittington et al., 2020), where a different set of constraints may apply (eg. the resolution of representation does not necessarily follow a Goldilocks of frequency). And grid cells have been shown to represent other behaviourally relevant variables besides location, such as time and distance (Kraus et al., 2015) . How can your model account for this? Or is it only applicable to spatial location?  Minor weaknesses:  Please remake all hand drawn figures, including the ones in appendix, with computer softwares Page 5 first paragraph: \u201c... and \u03b8 dependent parts (equation 6; Figure 3A)\u201d: 1) Adding a hyphen, i.e. \u03b8-dependent, might be helpful for readers. 2) Do you mean equation 5? Page 5 first paragraph: \u201c...This effect is limited by the firing rate bound: ||a0||2\u22122L0=N...\" Do you mean 1/2L0 ? Among the EC-HPC models, the paper is quite novel in terms of technicality. The ideas are clearly communicated, and as long as the simulation codes are released it should be reproducible. I\u2019d recommend accepting this paper for ICLR, as it provides a normative explanation for why the multi-module representation of grid cells is the optimal representation of space. The paper is well-written and clearly communicated, but also please remake your hand-drawn figures. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 4: The contributions are significant, and do not exist in prior works. Not applicable NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 38,
        "instruction": "Modeling content creator incentives on algorithm-curated platforms: Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by modern algorithms including factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices\u2014e.g., non-negative vs. unconstrained factorization\u2014significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models like ours for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools for numerically finding equilibria in exposure games, and illustrate results of an audit on the MovieLens and LastFM datasets. Among else, we find that the strategically produced content exhibits strong dependence between algorithmic exploration and content diversity, and between model expressivity and bias towards gender-based user and creator groups.",
        "reference": "The paper studied the interaction effects between content producers and consumers. The dynamics in the interactions are well formalized as an exposure game about incentives. They provided a comprehensive theoretical analysis of the properties of Nash equilibria in the games and proposed tools for numerically finding equilibria in exposure games. Both theory and empirical experiments verify the dependence between algorithmic exploration and content diversity, and between model expressivity and bias. Strengths:  The paper is well-written and flows very smoothly. Despite the paper's modest space, it provides the required context and necessary explanation in a good job.  Most parts of the paper have clear motivations. Many of my questions are well answered in the paper. Formulation and theoretical analysis of the exposure game in the recommender system is insightful. Most papers studied the recommender system in a fixed manner. I agree understanding the interaction effects/dynamics between the producer and customer is important and useful. The analysis of diversity and exploration is interesting. The paper mentioned the game may concentrate on uniform distribution, and exploration may incentivize content that is uniform and broadly appealing rather than diverse. From the recent papers, I also found deeper models can achieve diversity/coverage and accuracy at the same time. These two concepts may not be contradictory. If could further reveal the relationship between these two, it'll be very interesting. The paper identified several factors that can influence the recommendation seriously and provided a pre-deployment audit tool that can benefit the community.  Weaknesses/Questions:  In eq. (2), the paper introduced the temperature parameter \u03c4 to control the spread of exposure probabilities. Many of the follow-up analyses and experiments are built on this. However, most recommender systems didn't include this in their objectives. Could you please explain the rationale or connectivity between these? On page 3, I'm confused about the full control assumption. What's the difference between full contry and partial control and why it can abstract away the explicit model of producer actions? Is it possible to list some concrete examples?  For experiments, how are the producers defined? In my opinion, there are only user/item information in these data sets.  For the \u03f5-LNE solver in eq. (4), is this the paper originally proposed or adapted from others? I'm not familiar with the NE-based methods. It seems it's also updating the model with the gradient. Could you please illustrate a little more on the common points and difference between this one and the normal gradient descent method used to optimize ML model? The paper is mostly clear and well-written. The technical novelty is substantial as dynamics in the recommender system are well formalized and analyzed. The reproducibility is good, but the empirical evaluation and justification may not be comprehensive enough. In general, I think it's a good paper with clear novelty. Some of the descriptions and empirical experiments in the paper may require further explanations. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 4: The contributions are significant, and do not exist in prior works. 2: The contributions are only marginally significant or novel. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 39,
        "instruction": "Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules: Recent years have witnessed the prosperity of pre-training graph neural networks (GNNs) for molecules. Typically, atom types as node attributes are randomly masked, and GNNs are then trained to predict masked types as in AttrMask \\citep{hu2020strategies}, following the Masked Language Modeling (MLM) task of BERT~\\citep{devlin2019bert}. However, unlike MLM with a large vocabulary, the AttrMask pre-training does not learn informative molecular representations due to small and unbalanced atom `vocabulary'. To amend this problem, we propose a variant of VQ-VAE~\\citep{van2017neural} as a context-aware tokenizer to encode atom attributes into chemically meaningful discrete codes. This can enlarge the atom vocabulary size and mitigate the quantitative divergence between dominant (e.g., carbons) and rare atoms (e.g., phosphorus). With the enlarged atom `vocabulary', we propose a novel node-level pre-training task, dubbed Masked Atoms Modeling (\\textbf{MAM}), to mask some discrete codes randomly and then pre-train GNNs to predict them. MAM also mitigates another issue of AttrMask, namely the negative transfer. It can be easily combined with various pre-training tasks to improve their performance. Furthermore, we propose triplet masked contrastive learning (\\textbf{TMCL}) for graph-level pre-training to model the heterogeneous semantic similarity between molecules for effective molecule retrieval. MAM and TMCL constitute a novel pre-training framework, \\textbf{Mole-BERT}, which can match or outperform state-of-the-art methods in a fully data-driven manner. We release the code at \\textcolor{magenta}{\\url{https://github.com/junxia97/Mole-BERT}}.",
        "reference": "This paper presents a pre-training framework named Mole-BERT for learning molecular representation. The key components of Mole-BERT are 1) a variant of VQ-VAE for getting discrete tokenization result of atoms in molecular graphs, 2) a Masked Atoms Modeling strategy for predicting the masked atoms, and 3)  a graph-level contrastive learning task for adjusting the molecular representations. Authors evaluated Mole-BERT on molecular property prediction and drug-target affinity prediction, and compared with a number of baselines. Ablation study was conducted as well for showing the effectiveness of each component in the designed framework. The strengths of the paper are:  authors conducted extensive evaluation experiments on several datasets.  The pre-trained model was evaluated for molecular property prediction and drug-target affinity prediction.  the experimental results show that the proposed pre-trained model has better performance than the baselines.  The weaknesses of the paper are:  the proposed pre-training framework is a mixture of existing tools. The variant of VQ-VAE  is only a small change of VQ-VAE by organizing atoms in groups in the codebook (assigning fixed intervals to C, N O and other atoms). The masked modeling is a well developed idea, as well as the graph-level contrastive learning.   These existing tools are employed without any adaptation to the molecular graphs. For example, the MAM is used just with randomly mask its 15% atoms\u2019 tokens, and the graph augmentation in graph-level contrastive learning is just based on masking 15% vs masking 30%. Molecular graphs are different from other general graphs, because of their constraints on the graph validity. These  constraints should be considered in the design of self-supervising tasks. the paper has writing errors such as  \"an one\" The employed baselines were not introduced clearly. Some baselines are for general graphs. It is unclear how they were pre-trained by what datasets. GraphMVP is a pre-trained model on another molecular dataset with 3D geometry. Does Mole-BERT use much more larger dataset for pre-training than GraphMVP? is the 1.34% improvement due to the larger  dataset for pre-training in Mole-BERT ? There are other molecular pre-training models, such as GROVER (Rong et al., 2020a)  and (Liu et al., 2022a; Sta \u0308rk et al., 2021; Fang et al., 2022a; Zhu et al., 2022).  They should be included as baseline for comparison as well.  Another question about the results in Table 1 is the significance of performance improvement. The proposed Mole-BERT mostly outperforms the best baseline by around 1%. However, the std is also around 1.0. It is unknown how statistically significant the improvement is. In general, it is an interesting paper. However, the proposed model is weak on technique novelty. It is more like a model with mixed existing tools.  The experimental evaluation misses details about the baselines, and cannot support strongly the effectiveness of the proposed model. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 40,
        "instruction": "Concept-level Debugging of Part-Prototype Networks: Part-prototype Networks (ProtoPNets) are concept-based classifiers designed to achieve the same performance as black-box models without compromising transparency. ProtoPNets compute predictions based on similarity to class-specific part-prototypes learned to recognize parts of training examples, making it easy to faithfully determine what examples are responsible for any target prediction and why. However, like other models, they are prone to picking up confounders and shortcuts from the data, thus suffering from compromised prediction accuracy and limited generalization. We propose ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a human supervisor, guided by the model\u2019s explanations, supplies feedback in the form of what part-prototypes must be forgotten or kept, and the model is fine-tuned to align with this supervision. Our experimental evaluation shows that ProtoPDebug outperforms state-of-the-art debuggers for a fraction of the annotation cost. An online experiment with laypeople confirms the simplicity of the feedback requested to the users and the effectiveness of the collected feedback for learning confounder-free part-prototypes. ProtoPDebug is a promising tool for trustworthy interactive learning in critical applications, as suggested by a preliminary evaluation on a medical decision making task.",
        "reference": "This paper proposes ProtoPNets, a method to debug models at the concept level. Strengths  The paper makes good progress on debugging models using concepts. The authors write well: this makes it quite straightforward to see how this paper varies from existing work. The contributions are thus clear. The layperson validation of the method is quite convincing and shows the utility of ProtoPDebug. Figure 3, which should be Algorithm 1(?), is straightforward to implement  Weaknesses  Please explain what is meant by \"\u03b8' is consistent with F\" in Equation 10 It would be nice to make the the debugging process iterative (instead of one off to build up V or F); e.g., how many examples does it take before no debugging is needed? How much does debugging vary between participants Clarity: Adequate Quality: High Novelty: Adequate Reproducibility: Adequate. This is a strong paper that proposes an effective concept-level debugger that the authors validate not only using quantitative experiments but also using human-subject experiments. I foresee this paper garnering some attention for its simplicity and clear validation. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 41,
        "instruction": "Geometrically regularized autoencoders for non-Euclidean data: Regularization is almost {\\it de rigueur} when designing autoencoders that are sparse and robust to noise. Given the recent surge of interest in machine learning problems involving non-Euclidean data, in this paper we address the regularization of autoencoders on curved spaces. We show that by ignoring the underlying geometry of the data and applying standard vector space regularization techniques, autoencoder performance can be severely degraded, or worse, training can fail to converge. Assuming that both the data space and latent space can be modeled as Riemannian manifolds, we show how to construct regularization terms in a coordinate-invariant way, and develop geometric generalizations of the denoising autoencoder and reconstruction contractive autoencoder such that the essential properties that enable the estimation of the derivative of the log-probability density are preserved. Drawing upon various non-Euclidean data sets, we show that our geometric autoencoder regularization techniques can have important performance advantages over vector-spaced methods while avoiding other breakdowns that can result from failing to account for the underlying geometry.",
        "reference": "The authors propose geometrically regularized autoencoders for non-Euclidean data. The main contribution of the paper is adapting the regularization terms in the denoising autoencoder (DAE) and the reconstruction contractive autoencoder (RCAE) to the non-Euclidean setting. If the manifold from which the data are sampled are known a priori, the proposed autoencoders achieve improved reconstruction performance, and are able to estimate the score of the non-Euclidean data. Strength  The motivation of the paper is clear. It is also very well-organized. I especially like the different notations for points on the manifold and their coordinates. Theorem 1 suggests that the geometric score can be estimated from the trained auto-encoder (a generalization of the result by Alain & Bengio, 2014 in the Euclidean setting), and the numerical results are convincing.  Weakness  My biggest concern is the increased computational cost after taking into account the geometric information (exponential maps, geodesics, etc.) This seems to be buried under the rug of the paper. I would like to see come comparison of the computational time in training the vanilla autoencoder and the proposed models. The author seems to be using one single chart for a manifold. However, general manifolds are typically not parametrizable using one chart. What would happen if r(x) and x appear in two different charts? It might be a good idea to elaborate, in the main text, what \"\u2248\" means in equation (7). Also, is RCAE actively used in the community? There does not seem to be any reference on that in the paper. The paper is well-written and very easy to read. The idea of adapting the regularization to non-Euclidean data is novel and interesting. I think this is an interesting paper. I have some concern on the practical implementation and its implication on the computational cost. I am willing to adjust my rating based on the authors' response. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 42,
        "instruction": "A Message Passing Perspective on Learning Dynamics of Contrastive Learning: In recent years, contrastive learning achieves impressive results on self-supervised visual representation learning, but there still lacks a rigorous understanding of its learning dynamics. In this paper, we show that if we cast a contrastive objective equivalently into the feature space, then its learning dynamics admits an interpretable form. Specifically, we show that its gradient descent corresponds to a specific message passing scheme on the corresponding augmentation graph. Based on this perspective, we theoretically characterize how contrastive learning gradually learns discriminative features with the alignment update and the uniformity update. Meanwhile, this perspective also establishes an intriguing connection between contrastive learning and Message Passing Graph Neural Networks (MP-GNNs). This connection not only provides a unified understanding of many techniques independently developed in each community, but also enables us to borrow techniques from MP-GNNs to design new contrastive learning variants, such as graph attention, graph rewiring, jumpy knowledge techniques, etc. We believe that our message passing perspective not only provides a new theoretical understanding of contrastive learning dynamics, but also bridges the two seemingly independent areas together, which could inspire more interleaving studies to benefit from each other. The code is available at https://github.com/PKU-ML/Message-Passing-Contrastive-Learning.",
        "reference": "This paper revisits contrastive learning objectives from the feature propagation perspective. Specifically, by casting alignment and uniformity as two message propagation procedures on two respectively graphs, we can establish equivalence between contrastive learning and message passing graph neural networks. In this way, we can inherit existing techniques in graph neural networks to improve contrastive learning performance. Strengths:  This paper presents a very interesting perspective on understanding contrastive learning by formulating alignment and uniformity as message passing on graphs This paper is well-written and the idea is easy to follow The message passing formulation is supported by both theoretical analysis and empirical evidence  Weaknesses: I only have several minor points:  The second paragraph in the introduction section is a little bit confusing. I would like to suggest the authors to elaborate on \"model distribution\" and \"data distribution\". I wonder how we can understand the MoCo approach that maintains a memory bank for negatives by constructing similar message passing graphs? In Page 1, GAT is not cited correctly. In Page 5, reference (Eq. 30) should be (Eq. 11). This paper is clearly written. I do not spot any obvious errors. I think this paper makes a novel contribution both theoretically and empirically for understanding and improving contrastive learning. The authors do not provide source code so I cannot comment on the reproducibilty. I like the paper very much. I think the authors make a nontrivial contribution in understanding and improving contrastive learning. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. Not applicable NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 43,
        "instruction": "Zeroth-Order Optimization with Trajectory-Informed Derivative Estimation: Zeroth-order (ZO) optimization, in which the derivative is unavailable, has recently succeeded in many important machine learning applications. Existing algorithms rely on finite difference (FD) methods for derivative estimation and gradient descent (GD)-based approaches for optimization. However, these algorithms suffer from query inefficiency because many additional function queries are required for derivative estimation in their every GD update, which typically hinders their deployment in real-world applications where every function query is expensive. To this end, we propose a trajectory-informed derivative estimation method which only employs the optimization trajectory (i.e., the history of function queries during optimization) and hence can eliminate the need for additional function queries to estimate a derivative. Moreover, based on our derivative estimation, we propose the technique of dynamic virtual updates, which allows us to reliably perform multiple steps of GD updates without reapplying derivative estimation. Based on these two contributions, we introduce the zeroth-order optimization with trajectory-informed derivative estimation (ZoRD) algorithm for query-efficient ZO optimization. We theoretically demonstrate that our trajectory-informed derivative estimation and our ZoRD algorithm improve over existing approaches, which is then supported by our real-world experiments such as black-box adversarial attack, non-differentiable metric optimization, and derivative-free reinforcement learning.",
        "reference": "Based Gaussian process, this paper develops a zeroth-order optimization algorithm which requires fewer function queries to estimate the gradient than previous zeroth-order methods. Additionally, a so-called dynamic virtual update schemes is incorporated. Theoretically, the proposed method is shown to obtain gradient with exponentially diminishing error; and the convergence result of the algorithm is attained. Empirical studies with synthesized data and real-world data corroborate the theoretical analysis and demonstrated the superior performance of the presented approach over existing zeroth-order optimization approaches. Strength: 1.\tA novel algorithm is developed. 2.\tThe theoretical analysis is solid--- the bound of the gradient error is given, and the convergence rate is obtained. 3.\tEmpirical results show the proposed algorithm outperforms previous ones by a substantial margin. Weaknesses: 1.\tRelated studies on advanced zeroth-order methods that aim to reduce the gradient error or improve query efficiency are not introduced. The code to reproduce the numerical results is currently unavailable. For different problems, is there any guidance for the selection of confidence threshold c.     More intuitive explanations on why the gradient error decreases at exponential rate are expected. The code to reproduce the numerical results is currently unavailable. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 44,
        "instruction": "Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery: Deep Reinforcement Learning (RL) has emerged as a powerful paradigm for training neural policies to solve complex control tasks. However, these policies tend to be overfit to the exact specifications of the task and environment they were trained on, and thus do not perform well when conditions deviate slightly or when composed hierarchically to solve even more complex tasks. Recent work has shown that training a mixture of policies, as opposed to a single one, that are driven to explore different regions of the state-action space can address this shortcoming by generating a diverse set of behaviors, referred to as skills, that can be collectively used to great effect in adaptation tasks or for hierarchical planning. This is typically realized by including a diversity term - often derived from information theory - in the objective function optimized by RL. However these approaches often require careful hyperparameter tuning to be effective. In this work, we demonstrate that less widely-used neuroevolution methods, specifically Quality Diversity (QD), are a competitive alternative to information-theory-augmented RL for skill discovery. Through an extensive empirical evaluation comparing eight state-of-the-art algorithms (four flagship algorithms from each line of work) on the basis of (i) metrics directly evaluating the skills' diversity, (ii) the skills' performance on adaptation tasks, and (iii) the skills' performance when used as primitives for hierarchical planning; QD methods are found to provide equal, and sometimes improved, performance whilst being less sensitive to hyperparameters and more scalable. As no single method is found to provide near-optimal performance across all environments, there is a rich scope for further research which we support by proposing future directions and providing optimized open-source implementations.",
        "reference": "This paper proposes and integrates the usage of Quality-Diversity methods for skill discovery in RL. While prior methods largely rely on maximum-information (MI) RL, this paper makes a case for QD as an alternative approach when used with fast environment simulators. The authors provide a large number of experiments which show that QD-based methods are competitive with the MI RL approach and should be researched in more dept as future work. Strengths  The paper takes on an exciting direction, using QD for skill discovery. While the MI-RL line of work has shown great results in recent years, QD methods can provide an alternative, given the diversity element that is being explicitly optimized for. I like how the paper positions itself neutrally to both types of approaches, namely QD and MI-RL. After presenting all the experimental results, it comes to the conclusion that no method is optimal for all environments and lays down avenues for future work. It also accepts the pros and cons of both approaches, and analyses all methods without any biases, as far as I could tell. The paper includes extensive empirical evaluations and analysis, including experiments directly evaluating diversity, few-shot adaption and hierarchical learning. The list of baselines used in the experiments is fair, as far as I\u2019m concerned.  Weaknesses  One weakness of the work is that it always provides the median of the metric of interest over the seeds (e.g. Table 1, Figure 4). This can provide unfair results if there are outliers. I suggest that the authors provide several metrics of interest, such as mean, IQM, etc. rliable is a great tool for this. I didn\u2019t quite understand why haven\u2019t the authors include rewardless DIAYN and DADS results in their experiments. Throughout Section 5, they only use DIAYN/DADS+REWARDS as baselines. QD-based methods are obviously more expensive computationally, given they store separate weights for each policy/skill. The authors argue that this is okay if the simulator is very fast. I have no arguments against it, but it would be great to know how much more compute-heavy QD-based methods are. The authors only mention that everything is within 2 hours of training, but there can be a big difference within this range. Minor: please use the correct citation styles of prior work. See the ICLR instruction template for information on where to use \\citep and \\citet.  Note: My score is not final and can change (in either direction) based on the authors' responses or comments of other reviewers. Post rebuttal update I have increased my score based on the responses from the authors. Clarity: High. The paper is clear and easy to follow. The prior and related work is properly cited and helps the reader to understand the manuscript faster. Quality: High. The paper includes extensive experiments and a thorough analysis of prior work as well as new techniques that use QD for skill discovery. Novelty: High. While QD and skill discovery are mature fields at this point, this paper is the first attempt for using QD for skill discovery, as far as I\u2019m concerned. Reproducibility: High. The code is included alongside the submission. I have no worries regarding the reproducibility. Interesting work that combines QD and skill discovery in RL. Extensive experiments and thorough analysis of prior work are included in the manuscript. I have raised several questions and some concerns that I\u2019d like to be answered by the authors during the rebuttal. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 45,
        "instruction": "Uniform-in-time propagation of chaos for the mean-field gradient Langevin dynamics: The mean-field Langevin dynamics is characterized by a stochastic differential equation that arises from (noisy) gradient descent on an infinite-width two-layer neural network, which can be viewed as an interacting particle system. In this work, we establish a quantitative weak propagation of chaos result for the system, with a finite-particle discretization error of $\\mathcal{O}(1/N)$ \\textit{uniformly over time}, where $N$ is the width of the neural network. This allows us to directly transfer the optimization guarantee for infinite-width networks to practical finite-width models without excessive overparameterization. On the technical side, our analysis differs from most existing studies on similar mean field dynamics in that we do not require the interaction between particles to be sufficiently weak to obtain a uniform propagation of chaos, because such assumptions may not be satisfied in neural network optimization. Instead, we make use of a logarithmic Sobolev-type condition which can be verified in appropriate regularized risk minimization settings. ",
        "reference": "This paper proposed a uniform-in-time analysis of the propagation of chaos for the mean-field Langevin dynamics to conduct neural network optimization. The authors avoid the double-loop structure and enable to deal with convergence guarantee based on finite-width neural network with vanilla noisy gradient descent algorithms. Pros:  The first quantitative discretization error ensures analysis based on neural networks of finite/ limited neurons. The uniform nature ensures that the deviation of the output reduces rapidly. The analysis leverages the advantages of propagation of chaos and overcomes the bottleneck suffered by the Growall inequality in Mei'18 and the error bound remains stable when the time t is large.  Cons: The assumption of the super-quadratic tail of the regularization term is strong indeed. The authors clearly stated the contributions and limitations of the proof.  Novelty: I am not an expert in neural network theory and I am not able to evaluate the novelty. The technical tools based on gradient flow stuff seem standard and reasonable. Typos: \u03bct decreases L(\u03bct) unless \u03b4L(\u03bc)\u03b4\u03bc=0? Convergence analysis for stochastic gradient descent on a finite-width neural network where the upper bound is table w.r.t. time. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 4: The contributions are significant, and do not exist in prior works. 2: The contributions are only marginally significant or novel. NO.Details Of Ethics Concerns: NA 6: marginally above the acceptance threshold 1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
    },
    {
        "id": 46,
        "instruction": "Asynchronous Distributed Bilevel Optimization: Bilevel optimization plays an essential role in many machine learning tasks, ranging from hyperparameter optimization to meta-learning. Existing studies on bilevel optimization, however, focus on either centralized or synchronous distributed setting. The centralized bilevel optimization approaches require collecting massive amount of data to a single server, which inevitably incur significant communication expenses and may give rise to data privacy risks. Synchronous distributed bilevel optimization algorithms, on the other hand, often face the straggler problem and will immediately stop working if a few workers fail to respond. As a remedy, we propose Asynchronous Distributed Bilevel Optimization (ADBO) algorithm. The proposed ADBO can tackle bilevel optimization problems with both nonconvex upper-level and lower-level objective functions, and its convergence is theoretically guaranteed. Furthermore, it is revealed through theoretic analysis that the iteration complexity of ADBO to obtain the $\\epsilon$-stationary point is upper bounded by $\\mathcal{O}(\\frac{1}{{{\\epsilon ^2}}})$. Thorough empirical studies on public datasets have been conducted to elucidate the effectiveness and efficiency of the proposed ADBO.",
        "reference": "This paper proposes an algorithm that solves bilevel optimization problem in an asynchronous distributed manner. The iteration complexity for the algorithm to obtain \u03f5-stationary point is upper bounded by O(1\u03f52). Empirical results show that under asynchronous setting, the proposed algorithm outperforms state-of-art distributed bilevel optimization algorithm. Strength:  Strong empirical results with theoretical analysis  Weakness:  Redundant formulation compare to other bilevel paper, which makes it hard to read, especially when explaining how cutting plane works. Insufficient references. There are works on asynchronous distributed optimization that are not included in this paper. For example, https://ieeexplore.ieee.org/ielaam/6884276/8350883/7903733-aam.pdf. There are also many works asynchronous federated learning, and probably needed to be included here. What is the value of \u03c4 in the experiment? In the appendix, I don't quite follow the logic from eqn 57 to eqn 58. Can you please explain more? I don't understand from optimization perspective, why this algorithm can outperform other algorithms in asynchronous setting. From Theorem 2, the larger S is, then the less number of iterations is needed. For experiment, what if you run other synchronous algorithms (e.g. SDBO) in the same setting of ADBO, i.e. that the master problem will update its variable once it receives updates from S workers. Will ADBO still outperform SDBO? This paper is original and novel. However the written part needs to be improved. I think this paper is marginally below the acceptance threshold. Please see the weakness part for my reasoning. I think the authors need to revise the written to make the problem formulation/algorithm more neat and easier to follow. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 47,
        "instruction": "Confidence-Based Feature Imputation for Graphs with Partially Known Features: This paper investigates a missing feature imputation problem for graph learning tasks. Several methods have previously addressed learning tasks on graphs with missing features. However, in cases of high rates of missing features, they were unable to avoid significant performance degradation. To overcome this limitation, we introduce a novel concept of channel-wise confidence in a node feature, which is assigned to each imputed channel feature of a node for reflecting the certainty of the imputation. We then design pseudo-confidence using the channel-wise shortest path distance between a missing-feature node and its nearest known-feature node to replace unavailable true confidence in an actual learning process. Based on the pseudo-confidence, we propose a novel feature imputation scheme that performs channel-wise inter-node diffusion and node-wise inter-channel propagation. The scheme can endure even at an exceedingly high missing rate (e.g., 99.5\\%) and it achieves state-of-the-art accuracy for both semi-supervised node classification and link prediction on various datasets containing a high rate of missing features. Codes are available at https://github.com/daehoum1/pcfi.",
        "reference": "The authors tackle the problem of imputation in graphs, in which we want to predict missing node features in graph-structured data. Their approach uses a strategy involving pseudo-confidence (PC), where the PC of a given feature is proportional to alpha^S, where S is the corresponding node's nearest labeled neighbor. They combine this with a propagation strategy which shares information between features of the same node and between neighboring nodes.  The authors evaluate their method on a benchmark data set OGN. They follow (Rossi et al, 2021) in their evaluation. Their primary evaluation is on a data set from arXiv, in which nodes represent papers, edges denote citations between papers and node features are a representation of the words used in the paper's title and abstract. They hold out a subset of features of each node and impute these held-out features. They show that on these tasks their method outperforms existing methods including GCNMF and Feature Propagation. Overall, the paper is understandable, the method is reasonable and the experimental results are good. As described below, I am not sure about the methodological novelty. The application impact seems high, although this is also hard to tell because the benchmarks used are on relatively contrived tasks.  My main concern is that I could not tell what parts of the proposed method are novel. This is partly because I am not too familiar with graph imputation. The proposed method contains many components and all components are described as though they are novel. In particular, the authors indicate that their main contribution as \"pseudoconfidence\" as alpha^S, where S is the distance in the graph.  They should explain that this is a common strategy used in random walk methods, Monte Carlo reinforcement learning, and many other methods. Similarly, they should explain and cite the inspiration for each other part of their method. It seems likely that there are novel components and novel ways in which preexisting components are combined, but I could not work out what these are.  What real-world problems have 99% missing features? See above. See above. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 4: The contributions are significant, and do not exist in prior works. NO. 8: accept, good paper 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 48,
        "instruction": "LiftedCL: Lifting Contrastive Learning for Human-Centric Perception: Human-centric perception targets for understanding human body pose, shape and segmentation. Pre-training the model on large-scale datasets and fine-tuning it on specific tasks has become a well-established paradigm in human-centric perception. Recently, self-supervised learning methods have re-investigated contrastive learning to achieve superior performance on various downstream tasks. When handling human-centric perception, there still remains untapped potential since 3D human structure information is neglected during the task-agnostic pre-training. In this paper, we propose the Lifting Contrastive Learning (LiftedCL) to obtain 3D-aware human-centric representations which absorb 3D human structure information. In particular, to induce the learning process, a set of 3D skeletons is randomly sampled by resorting to 3D human kinematic prior. With this set of generic 3D samples, 3D human structure information can be learned into 3D-aware representations through adversarial learning. Empirical results demonstrate that LiftedCL outperforms state-of-the-art self-supervised methods on four human-centric downstream tasks, including 2D and 3D human pose estimation (0.4% mAP and 1.8 mm MPJPE improvement on COCO 2D pose estimation and Human3.6M 3D pose estimation), human shape recovery and human parsing.",
        "reference": "This paper proposes the Lifting Contrastive Learning (LiftedCL) workflow to train robust human-centric representations that are useful for downstream 2D and 3D human tasks such as pose estimation, shape prediction, and parsing. There are two core components. The contrastive learning part generates features that are invariant and equivariant (under inverse transforms). The \"lifting\" part generates 3D skeletons from the human-centric representations and is trained with a discriminator along with randomly sampled real 3D skeletons. Experiments show that the proposed training produces more useful features for downstream tasks than competitive alternatives such as MoCo-v2 and PixPro. Strength  The intuition of learning a robust representation that encodes 3D human information makes a lot of sense to me.  The designed pipeline is straightforward. It could inspire further development in this direction.  The invariance and equivariance training is novel in the context of 3D human feature learning tasks.   I like the idea of having unpaired randomly sampled 3D skeletons as auxiliary training data.  Experiments show the solid performance of the proposed method when trained on ImageNet + COCO: downstream tasks have significant improvement. That validates the idea of encoding 3D human structure information is useful for human-centric tasks.   Weakness  I think the paper could use a strong baseline in which all the 3D elements are replaced by 2D counterparts. For example, instead of lifting 2D features to 3D skeletons, the network will output 2D skeletons and a discriminator will compare those with randomly sampled real 2D skeletons. Other parts of the network will be kept as much as possible. If the proposed LiftedCL outperforms this 2D alternative on 2D human tasks such as pose estimation and parsing, then it is clear that 3D is essential to both 2D and 3D human tasks. I agree with the authors that 3D is important but it will be more convincing with such an experiment.  I might have missed it, but is there an ablation study on the effectiveness of the KCS layer? Clarity The paper is straightforward and easy to follow.  Quality The quality of the work is good. Novelty I think the idea of encoding 3D-aware representations in human-centric tasks via lifting-to-3D contrastive learning is novel and it can inspire future studies in this direction. Reproducibility There seems to be a sufficient amount of detail in the paper for reproducibility. It will be better if the authors plan to release their code. It is not currently mentioned in the submission. Overall, I like this paper because it studies an interesting and important problem of encoding 3D-aware representations in human-centric tasks and provides a viable solution. The design components in this workflow could inspire future research in this direction. The experiment results are solid on multiple human-centric tasks. The paper could be further strengthened by adding the 2D baseline mentioned above, but it is a good contribution to the community in its current form. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. Not applicable NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 49,
        "instruction": "Individual Privacy Accounting with Gaussian Differential Privacy: Individual privacy accounting enables bounding differential privacy (DP) loss individually for each participant involved in the analysis. This can be informative as often the individual privacy losses are considerably smaller than those indicated by the DP bounds that are based on considering worst-case bounds at each data access. In order to account for the individual losses in a principled manner, we need a privacy accountant for adaptive compositions of mechanisms, where the loss incurred at a given data access is allowed to be smaller than the worst-case loss. This kind of analysis has been carried out for the R\u00e9nyi differential privacy by Feldman and Zrnic (2021), however not yet for the so-called optimal privacy accountants. We make first steps in this direction by providing a careful analysis using the Gaussian differential privacy which gives optimal bounds for the Gaussian mechanism, one of the most versatile DP mechanisms. This approach is based on determining a certain supermartingale for the hockey-stick divergence and on extending the R\u00e9nyi divergence-based fully adaptive composition results by Feldman and Zrnic (2021). We also consider measuring the individual  $(\\varepsilon,\\delta)$-privacy losses using the so-called privacy loss distributions. Using the Blackwell theorem, we can then use the results of Feldman and Zrnic (2021) to construct an approximative individual $(\\varepsilon,\\delta)$-accountant. We also show how to speed up the FFT-based individual DP accounting using the Plancherel theorem.",
        "reference": "The paper extends previous results on individual privacy accounting from R\u00e9nyi DP to Gaussian DP. It also considers methods to maintain approximate privacy filters. Strengths  The paper has adequate review of the relevant literature, is mostly self-contained, and clearly states the prior results it builds upon. It provides a proof for a natural (and perhaps not surprising) extension of the prior individual privacy accounting in the RDP case.  Weaknesses  The technical contribution seems incremental (proof of Theorem 11), but I will leave the assessment of the paper's novelty to the experts in this particular topic. Experiments should better highlight the practical use of individual GDP accounting compared to RDP. Figure C.3 shows one example for private GD, with a fixed number of steps. Perhaps showing similar plots under different settings (e.g. different number of steps) can help. Experiments in Section 6 merit further discussion. Besides the disparate impact problem, the experiments suggest that even in terms of overall AUC (Figure 4), very soon after filtering starts, the overall AUC begins to drop, which suggest a limited benefit from individual accounting. Did the authors compare to not using individual accounting? The paper is well-written and clearly positions itself within the relevant literature. Code for the experiments is provided in the supplement. A nice extension of previous individual accounting results, from the R\u00e9nyi DP to the Gaussian DP case. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 50,
        "instruction": "Evolving Populations of Diverse RL Agents with MAP-Elites: Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such as hyperparameter sensitivity, high stochasticity as well as training instability, including when the population size increases as some components are shared across the population in recent approaches. Furthermore, existing approaches mixing ME with RL tend to be tied to a specific RL algorithm, which effectively prevents their use on problems where the corresponding RL algorithm fails. To address these shortcomings, we introduce a flexible framework that allows the use of any RL algorithm and alleviates the aforementioned limitations by evolving populations of agents (whose definition include hyperparameters and all learnable parameters) instead of just policies. We demonstrate the benefits brought about by our framework through extensive numerical experiments on a number of robotics control problems, some of which with deceptive rewards, taken from the QD-RL literature. We open source an efficient JAX-based implementation of our algorithm in the QDax library. ",
        "reference": "The paper presents a framework that allows using any reinforcement learning (RL) algorithm within a population of agents. The contribution is to use quality diversity methods to evolve populations and maintain their diversity. The most commonly used method is MAP-ELITES, but MAP-ELITES does not work well on high-dimensional search spaces. The contribution of the paper is the development of a version of MAP-ELITES, called PBT-MAP-ELITES, that does not depend on a specific RL agent, is robust to hyperparameter choices, and scales to large population sizes.  Some experimental results are included for an example problem. Strengths:  The changes made to MAP-ELITES are simple but seem to be effective. Weaknesses:   the PBT-MAP-ELITES algorithm is a variation of PGA-MAP-ELITES, which seems quite simple even though it appears to be effective.  There are many variations of MAP-ELITES that are used in the experiments but for which there is no explanation in the paper.  The experimental results included are relatively limited and are not described with enough details to be reproducible.  The paper assumes familiarity with all the specific robotics problems used to test the algorithm.   The figure with the experimental results (Fig.2) is hard to read. The paper is clear but not always precise and sufficiently detailed. I do not think is possible to replicate the results. Too many details are missing. The paper presents an algorithm that is a new variation of MAP-ELITES. It evolves a population of agents, scales well to large population sizes, and is robust to hyperparameter choices. The experimental results that are included in the paper show good performance compared to other algorithms, but they are described in a succinct way, making it very hard to replicate the results. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO.Details Of Ethics Concerns: None 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 51,
        "instruction": "Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data : The implicit biases of gradient-based optimization algorithms are conjectured to be a major factor in the success of modern deep learning.  In this work, we investigate the implicit bias of gradient flow and gradient descent in two-layer fully-connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal, a common property of high-dimensional data.  For gradient flow, we leverage recent work on the implicit bias for homogeneous neural networks to show that asymptotically, gradient flow produces a neural network with rank at most two.  Moreover, this network is an $\\ell_2$-max-margin solution (in parameter space), and has a linear decision boundary that corresponds to an approximate-max-margin linear predictor.  For gradient descent, provided the random initialization variance is small enough, we show that a single step of gradient descent suffices to drastically reduce the rank of the network, and that the rank remains small throughout training.  We provide experiments which suggest that a small initialization scale is important for finding low-rank neural networks with gradient descent. ",
        "reference": "This paper analyzes the implicit bias of gradient flow/descent on a two-layer leaky-ReLU network. Specifically, assuming the data points are nearly orthogonal, Theorem 3.2 gives an accurate characterization of the KKT point of the margin maximization problem: all data points are support vectors, the first layer is of rank 2, and a closed-form solution is given. In Theorem 3.4, it is further shown that gradient flow converges to this solution. Section 4 focuses on gradient descent, and shows that with nearly orthogonal data and smoothed leaky-ReLU activation, the stable rank of the network is bounded by a universal constant. Empirical supports are also provided. This paper provides interesting and accurate characterizations of the implicit bias of gradient flow/descent with nearly-orthogonal data and leaky-ReLU activation. Specifically, although the network can be highly over-parameterized, this paper shows that the classical or stable rank stays small. One weakness is that the near orthogonality condition is a little restrictive, as it implies linear separability. Lemma 3.3 shows that it holds with Gaussian inputs, but it requires d\u2265n2, which does not hold in practice. Can you discuss how this assumption is used in your proof, and why linear separability is not enough? The paper is well-written. This paper gives interesting results on the implicit bias of gradient descent with nearly-orthogonal data and leaky-ReLU activation. I recommend for acceptance. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 52,
        "instruction": "Gray-Box Gaussian Processes for Automated Reinforcement Learning: Despite having achieved spectacular milestones in an array of important real-world applications, most Reinforcement Learning (RL) methods are very brittle concerning their hyperparameters. Notwithstanding the crucial importance of setting the hyperparameters in training state-of-the-art agents, the task of hyperparameter optimization (HPO) in RL is understudied. In this paper, we propose a novel gray-box Bayesian Optimization technique for HPO in RL, that enriches Gaussian Processes with reward curve estimations based on generalized logistic functions. In a very large-scale experimental protocol, comprising 5 popular RL methods (DDPG, A2C, PPO, SAC, TD3), dozens of environments (Atari, Mujoco), and 7 HPO baselines, we demonstrate that our method significantly outperforms current HPO practices in RL.",
        "reference": "The authors propose a new approach to HPO tailored towards RL. Specifically, a HPO approach that is sufficiently sample efficient such that the approach is effective even with limited resources (research lab as opposed to data center) is sought after. Bayesian optimization is used, with a Gaussian process (GP) surrogate. The GP has a novel deep kernel which consists of a parametrized Richard's curve, where the five coefficients of the curve are what the trained deep neural network is used to predict. The purpose of the kernel is to represent, and sample-efficiently infer, reward curves for specific hyperparameter configurations. In an extensive evaluation, on several problem domains comparing with several different kinds of state-of-the-art HBO methods, the proposed approach is demonstrated to be highly effective and clearly outperform competing methods. Strength  The context of the work is well described and the approach is well motivated given related works. The approach is clever and attack the core problem of sample-efficient HPO for RL, namely to predict the continuation of reward curves given the training epochs so far. The connection to and use of multi-fidelity GP well made and highly suitable. The evaluation is strong, with a variety of methods and environments. Clear hypotheses, and interpretation of results in light of these. The ablation study enriches the contributions.  Weaknesses Nothing obvious High clarity: Well written with excellent related work context. Most details are in the paper, but those that did not fit (e.g. BO, deep kernel learning, etc.) are referred to suitable related work. High quality and seemingly high novelity.  The method is well described and source code is available (I did not test it). The paper is well contained, well positioned and makes seemingly large and significant advances on an important problem for RL in general. It was a pleasure to read.  Update Thank you for engaging with the reviewers and clarifying many important things. However, after reading through the other reviews I reduce my score, mainly due to evaluation concerns as pointed out by h99m and eXNC. I still believe that the paper contain contributions of interest to the community, but the clarity and motivation for parts of the evaluation should probably be improved in accordance their comments. eXNC also raise important concerns in the latest comment & update, which affect my judgement. I am consequently still in favor of accept, but only marginally. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. 4: The contributions are significant, and do not exist in prior works. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 53,
        "instruction": "Protein Sequence and Structure Co-Design with Equivariant Translation: Proteins are macromolecules that perform essential functions in all living organisms. Designing novel proteins with specific structures and desired functions has been a long-standing challenge in the field of bioengineering. Existing approaches generate both protein sequence and structure using either autoregressive models or diffusion models, both of which suffer from high inference costs. In this paper, we propose a new approach capable of protein sequence and structure co-design, which iteratively translates both protein sequence and structure into the desired state from random initialization, based on context features given a priori. Our model consists of a trigonometry-aware encoder that reasons geometrical constraints and interactions from context features, and a roto-translation equivariant decoder that translates protein sequence and structure interdependently. Notably, all protein amino acids are updated in one shot in each translation step, which significantly accelerates the inference process. Experimental results across multiple tasks show that our model outperforms previous state-of-the-art baselines by a large margin, and is able to design proteins of high fidelity as regards both sequence and structure, with running time orders of magnitude less than sampling-based methods.",
        "reference": "The authors present a method for protein sequence and structure codesign. Current methods use either autoregressive or diffusion-based models, which the authors claim are computationally expensive and produce suboptimal solutions. Sequence generative models can produce good designs, but do not model protein structures. The authors address the above issue with their method ProtSeed. ProtSeed uses a triangular-aware encoder to learn geometrical constrain from context features and roto-translation equivariant decoder that iteratively improves protein structure followed by an MLP to decode the amino acid identities from the structure. The author performed experiments on the structural antibody database and 2 protein design benchmarks. Strengths:  Good introduction and well-presented related work focusing on key aspects. Good evaluation results compared to previous methods A novel approach for solving the structure and sequence of protein and study for the de novo protein design  Weaknesses:  Lack of specific hyperparameter and training details No mention that code and trained models will be released The application of the antibody CDR design task is clear, but the relevance of the other two tasks is murkier. When would detailed structural specifications be available, but without an existing sequence that forms that fold? The authors note a difference between their evaluation and the published scores for Diffusion, GNN and RABD models (Table 1). However, the concern remains that the Diffusion model shows significantly better RMSD scores compared to the scores in this paper. What is the source of this discrepancy? ProtSeed improves over previous methods in terms of PPL, RMSD, and AAR. However, because the task is to design functional proteins, these metrics may not reflect true model performance, because a model with higher RMSD may, in fact, produce (more) functional proteins while also being worse at predicting structure. It would be helpful to provide some additional, functional-oriented, evaluation. For example, by comparing the protein-protein interaction patterns and charge distributions between the designs and the ground truth. AAR is surprisingly low for all of these methods, including ProtSeed. If only 40% or less of amino acids are correctly recovered, how confident can we be that any of these methods actually produce good designs? I\u2019m not convinced by AF2 structure prediction as an evaluation. It isn\u2019t surprising that AF2 would predict the designed proteins to fold as intended, because all of these models are trained on the same PDB data and, therefore, probably have the similar pathologies.  Other minor comments:  Figure 4. \u2013 To keep a uniform style among all figures, a, b c marks could be moved outside of the figures as in Fig. 1-3 Table 3 \u2013 highlight best results as in table 1 and 2 The manuscript is well written, the method is reasonably novel, and the empirical performance is strong. A good paper about an interesting method. Hyperparameters and training details need to be discussed. The code and models should be released to facilitate reproduction and future work. Some discrepancies with prior work should also be addressed. What would improve my score: include hyperparameter and training details. Resolve discrepancy with prior work. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 54,
        "instruction": "Learning in temporally structured environments: Natural environments have temporal structure at multiple timescales. This property is reflected in biological learning and memory but typically not in machine learning systems. We advance a multiscale learning method in which each weight in a neural network is decomposed as a sum of subweights with different learning and decay rates. Thus knowledge becomes distributed across different timescales, enabling rapid adaptation to task changes while avoiding catastrophic interference. First, we prove previous models that learn at multiple timescales, but with complex coupling between timescales, are equivalent to multiscale learning via a reparameterization that eliminates this coupling. The same analysis yields a new characterization of momentum learning, as a fast weight with a negative learning rate. Second, we derive a model of Bayesian inference over $1/f$ noise, a common temporal pattern in many online learning domains that involves long-range (power law) autocorrelations. The generative side of the model expresses $1/f$ noise as a sum of diffusion processes at different timescales, and the inferential side tracks these latent processes using a Kalman filter. We then derive a variational approximation to the Bayesian model and show how it is an extension of the multiscale learner. The result is an optimizer that can be used as a drop-in replacement in an arbitrary neural network architecture. Third, we evaluate the ability of these methods to handle nonstationarity by testing them in online prediction tasks characterized by $1/f$ noise in the latent parameters. We find that the Bayesian model significantly outperforms online stochastic gradient descent and two batch heuristics that rely preferentially or exclusively on more recent data. Moreover, the variational approximation performs nearly as well as the full Bayesian model, and with memory requirements that are linear in the size of the network.\n",
        "reference": "The paper proposes a gradient-based learning scheme that deals with changing parameters during learning. For this, the authors give a context of their algorithm to the classical momentum learning strategy and a multi-timescale model from neuroscience. The algorithm is derived using a Bayesian inference framework, where the posterior mean recovers the newly proposed learning method. Here, a continuous-time linear state space model is used to model the time-varying parameters. For the case of a supervised learning problem, the authors consider a nonlinear model for the time-varying parameters. Bayesian inference is carried out by the authors by a variational approximation of the extended Kalman filter algorithm. The learning algorithm is tested on two synthetic problems, with time-varying parameters. For both linear regression and classification problems, the proposed algorithm leads to a smaller average loss. Overall, I think the paper presents an interesting new algorithm, for a problem that is probably very relevant in many applications. I am not very used to the style of writing, where the authors are probably coming a bit more from the direction of neuroscience. For me, the paper was pretty hard to follow. However, the results look reasonable even though the derivations are derived in a bit nonstandard way. The presented two empirical case studies are fair and are showing promising results. However, I would have hoped for an application where this strategy is tested for a real-world problem, to show its applicability. A sore point for me is the derivation using the power spectral analysis. I think this paper would have been much easier to understand and interpret would it had been derived using standard Bayesian inference techniques. For example, the definition of the 1/f noise process is very strange to me. To illustrate my confusion: equation (13) is a linear stochastic differential equation, for which the time-point-wise marginal densities are distributed multivariate Gaussian, see, e.g., [1]. Since the sum of the components of a multivariate Gaussian random variable in equation (14) is still Gaussian, see, e.g., https://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/SumOfCorrelatedRVs.pdf , it implies that \u03be(t) is a Gaussian random variable, where the mean and variance parameter can be computed in closed form. Maybe the authors can explain, why the 1/f definition is needed. Strengths:  Interesting Setup Relation to well-known algorithms is given A new learning algorithm, which is easy to implement Convincing empirical results for some synthetic problems under the modeling assumption  Weaknesses:  The derivations are hard to follow The 1/f noise definition is not clear to me No real-world application given in the paper  [1] S\u00e4rkk\u00e4, Simo, and Arno Solin. Applied stochastic differential equations. Vol. 10. Cambridge University Press, 2019. For me, the paper is hard to follow. The results are however interesting and relevant. The inference techniques used are well-known and standard. However, I think the work is probably interesting for the community for trying to solve a learning problem with time-varying parameters. The paper presents an interesting topic. The resulting algorithm is easy to implement and is probably useful for learning problems with time-varying parameters. The derivations are not very clear to me and the paper, but the results look reasonable. An application to a real-world learning problem would have been very helpful to show its applicability. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 55,
        "instruction": "RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates: Proximal splitting algorithms are well suited to solving large-scale nonsmooth optimization problems, in particular those arising in machine learning. We propose a new primal\u2013dual algorithm, in which the dual update is randomized; equivalently, the proximity operator of one of the function in the problem is replaced by a stochastic oracle. For instance, some randomly chosen dual variables, instead of all, are updated at each iteration. Or, the proximity operator of a function is called with some small probability only. A nonsmooth variance-reduction technique is implemented so that the algorithm finds an exact minimizer of the general problem involving smooth and nonsmooth functions, possibly composed with linear operators. We derive linear convergence results in presence of strong convexity; these results are new even in the deterministic case, when our algorithms reverts to the recently proposed Primal\u2013Dual Davis\u2013Yin algorithm. Some randomized algorithms of the literature are also recovered as particular cases (e.g., Point-SAGA). But our randomization technique is general and encompasses many unbiased mechanisms beyond sampling and probabilistic updates, including compression. Since the convergence speed depends on the slowest among the primal and dual contraction mechanisms, the iteration complexity might remain the same when randomness is used. On the other hand, the computation complexity can be significantly reduced. Overall, randomness helps getting faster algorithms. This has long been known for stochastic-gradient-type algorithms, and our work shows that this fully applies in the more general primal\u2013dual setting as well.",
        "reference": "The paper considered convex composition optimization whose objective function is the sum of three with one composed with a linear operator. A randomized algorithm, which generalized previous determined Primal-Dual algorithm, was proposed. Convergence rate in the strongly convex case was presented, while for the only convex case, convergence rate for the primal Bregman divergence was provided. Several special cases of the proposed algorithms were discussed. Strength:  The proposed randomized algorithm can deal with the general three block composite optimization problem; several existing work in the literature can be casted as special case of the method.  For strongly convex cases, linear convergence rate was proved.  Weaknesses:  Practical side, lack of numerical examples to justify the advantage of the proposed scheme. In major parts stochastic methods, the randomization is applied to the gradient parts to reduce complexity. While for the non-smooth part, it is questionable whether randomization can bring as big advantage as the stochastic ``gradient'' methods. For example, when h\u2218K is total variation or wavelet like regularizations, h\u2217 accounts for simple projection, which is not necessarily of very high complexity.  When the problem is just convex, only convergence rate on the Bregman divergence of the primal variable x was provided. However, the no rates for the dual variable. What caused this? Is it because that the dual function under this setting is non-smooth? The paper is well written and clearly explained. The work is original, however the novelty of the algorithm is limited in the sense that randomize existing deterministic methods with convergence guarantee is not a surprising result to date. No numerics were provided to verify the reproducibility. The paper considered three-block composite optimization problem and proposed a randomized Primal-Dual algorithm to solve the problem. Convergence rates are provided, and discussions on some special cases are presented. No numerical experiments are provided to verify the advantages of the algorithms, which is a major drawback. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 56,
        "instruction": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models: Large pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.",
        "reference": "The paper studies the calibration problem of BERT-like models on classification tasks. They show that the problem is created mainly during fine-tuning - the pre-trained model itself is well calibrated on the MLM task it is trained on. This MLM knowledge and calibration is destroyed during fine-tuning (a process called catastrophic forgetting in the literature).  To solve the calibration problem, the authors propose to continue fine-tune the model on the MLM task jointly with the actual classification  task at hand.  Experimentations on 3 datasets and both in-domain and out-of-domain settings show that the proposed method yield lower calibration error compared to other methods (that do not use post-training calibration). In terms of raw quality, the proposed model perform on par with the existing method (and frequently slightly better). Strengths  The task of calibrating the results of a large language model is important and necessary in some practical applications.  The observation that the problem of mis-calibration is created during fine-tuning is interesting and novel.  The experimentation was done on multiple datasets and in two different settings. The authors run fine-tuning 5 time and report average and standard deviation, showing a more complete picture compared to the results of a single run.  Weaknesses  It is unclear why post-processing techniques, such as temperature scaling, are insufficient, that we need a new training paradigm for overcoming this post processing step, especially that temperature scaling is quite lightweight. This makes the overall contribution quite limited.  Using MLM loss in addition to the standard classification loss is known (for a long time) to produce better results. Most work run an additional pre-training step on the target dataset, e.g., https://arxiv.org/pdf/1905.05583.pdf, https://arxiv.org/pdf/2004.11493.pdf, but joint training is also a known technique (which performs slightly worst, so less cited. Still, it was already published on e.g., https://dl.acm.org/doi/pdf/10.1145/3437963.3441777). This makes the comparison to the baseline unfair, since additional (unsupervised) training data was used.  It is unclear what is the performance implications are on the training time. If the training scheme alternates between MLM and classification losses, this would result in 2x longer training time, which is a significant impact. This should be at least clarified in the paper.  It is unclear how Section 5.1 is related to the rest of the paper (except for naming the algorithm), given that after all complex formulas the paper uses the standard MLM loss, and then move to knowledge distillation loss. Clarifying the connection to the rest of the paper (or dropping this part) would make it easier to follow the paper's contribution.  Table 1, showing that after fine-tune the model is unable to do the original task, is not by itself surprising or interesting. The question is how this is impacting calibration on the classification task, which is not directly related to the quality of the model on the MLM task. Quality  The experiments are clear and well done.  Clarity  The paper itself has one (seemly important) section whose connection to the rest of the paper is unclear.  Originality  The idea of training classification together with MLM is not novel.  Studying the impact of this on the calibration error is novel, but the scope/impact of this is quite narrow. The paper is well executed, but the task of reducing calibration error without post processing is quite narrow and has low impact. The approach of jointly training classification and MLM loss is not novel, but the impact on reducing calibration error is somewhat novel. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 57,
        "instruction": "Guarded Policy Optimization with Imperfect Online Demonstrations: The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C.",
        "reference": "The paper studies the Teacher-Student Framework, in the case when the teacher is potentially sub-optimal. Learning here is based on an ensemble off-policy method. The core concept is to develop a teach intervention function that is based on the estimated sub-optimality of student actions with respect to the teacher's value function. The advantage of the proposed method is that it allows for continuous student improvement, to the point that the student takes complete control in the case of sub-optimal teacher. The authors provide theoretical bounds for the proposed method and evaluate it on a driving simulator. The proposed method 1) outperforms baselines across evaluation scenarios and 2) demonstrates the student capability to solve the task, even with a significantly sub-optimal teacher. Strengths:  The proposed algorithm is intuitive and straight-forward It asymptotically outperforms baselines The proposed method is backed by theoretical analysis  Weaknesses  It seems that the algorithm takes some time before it starts improving and is initially substantially outperformed by EGPO. Is this because at the start of the training the switch allows most student actions, while EGPO mostly uses the teacher?  Question: What is the effect of the optimization algorithm here? The proposed method uses Q-ensembles, which combined with more often training have demonstrated significant improvement in sample complexity (RedQ). What update frequency was used in this paper and is this directly comparable to the baseline methods? The paper is well written and clear. The core concept of agent switches here is not new, but the particular implementation allows the student to learn and eventually outperform a sub-optimal teacher, unlike baseline methods. Simple and intuitive method to allow student agent to learn and outperform a potentially sub-optimal teacher. Clear writing with good theoretical backing. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 58,
        "instruction": "Fast Nonlinear Vector Quantile Regression: $$\n\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n$$\nQuantile regression (QR) is a powerful tool for estimating one or more conditional quantiles of a target variable $\\rvar{Y}$ given explanatory features $\\rvec{X}$.\nA limitation of QR is that it is only defined for scalar target variables, due to the formulation of its objective function, and since the notion of quantiles has no standard definition for multivariate distributions.\nRecently, vector quantile regression (VQR) was proposed as an extension of QR for vector-valued target variables, thanks to a meaningful generalization of the notion of quantiles to multivariate distributions via optimal transport.\nDespite its elegance, VQR is arguably not applicable in practice due to several limitations:\n(i) it assumes a linear model for the quantiles of the target $\\rvec{Y}$ given the features $\\rvec{X}$;\n(ii) its exact formulation is intractable even for modestly-sized problems in terms of target dimensions, number of regressed quantile levels, or number of features, and its relaxed dual formulation may violate the monotonicity of the estimated quantiles;\n(iii) no fast or scalable solvers for VQR currently exist.\n\nIn this work we fully address these limitations, namely:\n(i) We extend VQR to the non-linear case, showing substantial improvement over linear VQR;\n(ii) We propose {vector monotone rearrangement}, a method which ensures the quantile functions estimated by VQR are monotone functions;\n(iii) We provide fast, GPU-accelerated solvers for linear and nonlinear VQR which maintain a fixed memory footprint, and demonstrate that they scale to millions of samples and thousands of quantile levels;\n(iv) We release an optimized python package of our solvers as to widespread the use of VQR in real-world applications.",
        "reference": "In this work, motivated by the limitation that there are no fast or scalable solvers for vector quantile regression (VQR), the authors provide a highly-scalable solver for VQR that relies on solving its relaxed dual formulation. In addition, the authors propose vector monotone rearrangement (VMR), which serves as a refinement step and resolves the co-monotonicity violations in estimated conditional vector quantile functions (CVQFs). Moreover, motivated by the limitation of VQR that it assumes a linear model for the quantiles of the target given the features, the authors propose nonlinear vector quantile regression (NL-VQR). Based on extensive experiments on both synthetic and real data, the authors demonstrate the superiority of VMR and NL-VQR in modeling CVQFs accurately. Strength: The studied problem is of sufficient interest to the community, and the authors' work seems to be a significant contribution to the field of VQR.  Weaknesses: Although the experimental results are sufficiently promising, I hope that similar to some closely relevant works such as Carlier et al. (2016) and Chernozhukov et al. (2017), the authors can provide some theoretical justifications for their proposed methods. This paper is generally well-written, and it seems to have sufficiently novel and significant contributions that may advance the field.  The code has been included in the supplementary material (and also in GitHub) for reproducibility. Overall, I think that this is a good work with sufficiently novel methods and interesting numerical results. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 4: The contributions are significant, and do not exist in prior works. NO.Details Of Ethics Concerns: NA. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 59,
        "instruction": "Leveraging Large Language Models for Multiple Choice Question Answering: While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., \u201cA\u201d) associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.",
        "reference": "This paper addresses the difference between multiple-choice prompting and standard prompting (so called cloze prompting), clarifying major reasons why LLM underperforms on Multiple Choice Question Answering (MCQA) problems. First, what LLM tries to predict in terms of \u201cmore likely\u201d does not always mean \u201cmore correctly\u201d. This conflation often happens when the tokens in the answer sequence is less common or less grammatical. Second, LLM must rely on normalization schemes to compare candidate answers with different lengths or different frequencies. But this yields additional dependency on tokenizer. Third, standard prompting compares different options only indirectly via the (normalized) likelihood without direct comparison. Obviously, such standard prompting is expensive comparing to generating one option token. To make LLM solve MCQA problems with order-invariance, the authors propose Multiple Choice Symbol Binding (MCSB) capability that could be model-agnostically testable by recording the answer with the highest probability for each ordering of question (so called PPA). The experimental results show that training on code data (especially by multi-staging) is useful for MCSB. Providing more shots as few-shot examples also help boosting the performance. (Strengths)  Reveals problematic ingredients for likelihood-based answering. Introduce the concept of MCSB and measure it by PPA. Concentrated results that significantly improves QA performance by using multiple-choice prompting.  (Weaknesses)  Individual problematic ingredients are neither being theoretically-proven nor empirically-proven. No novel/brand new ideas. Mostly empirical analysis based on OpenAI playground. Some major arguments are less supported. The submission is an analysis paper rather than finding something new. (Major concerns) How to make sure Codex model clearly outperforms Instruct model? This is a critical question as the authors measures the main experiments (Table 2) that compare Multiple Chocie Prompting (MCP) and Cloze Prompting (CP) only with Codex model.   The capability to perform MCSB could be due to human feedback alignment by Reinforcement Learning rather than other points indicated by the authors.  Are the PPA difference between Codex and Instruct (in Figure 2) statistically significant? While no statistical test has been provided, it seems not easy to decline null hypothesis that says the difference is a random effect.  Only Codex tested on OpenBookQA shows strong performance gain when using MCP, whereas Instruct outperforms Codex on the other two tasks in Table 1. More detailed experiments are necessary to convince how Codex achieve such higher accuracy.   (Minor concerns)  Any reason to choose OpenBookQA which also matters the performance of retriever?  Do you know how Codex model is exactly trained? Codex model that you used could be first based on Instruct, then being further trained on code data. Equally likely, Codex model might perform it's own alignment similar to Instruct but based on the preference of generated codes. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 60,
        "instruction": "Learning with Logical Constraints but without Shortcut Satisfaction: Recent studies have started to explore the integration of logical knowledge into deep learning via encoding logical constraints as an additional loss function. However, existing approaches tend to vacuously satisfy logical constraints through shortcuts, failing to fully exploit the knowledge. In this paper, we present a new framework for learning with logical constraints. Specifically, we address the shortcut satisfaction issue by introducing dual variables for logical connectives, encoding how the constraint is satisfied. We further propose a variational framework where the encoded logical constraint is expressed as a distributional loss that is compatible with the model's original training loss. The theoretical analysis shows that the proposed approach bears some nice properties, and the experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction.",
        "reference": "In this paper, a new framework for learning with logical constraints is proposed. The shortcut satisfaction issue is addressed by introducing dual variables for logical connectives, encoding how the constraint is satisfied. A variational framework is proposed where the logical constraint is expressed as a distributional loss that is compatible with the model\u2019s original training loss. Theoretical analysis shows that the proposed approach bears some nice properties, and experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction. The work successfully addressed the shortcut satisfaction issue and proposed a variational framework where the logical constraint is expressed as a distributional loss that is compatible with the model\u2019s original training loss. However the design lacks theoretical support. The paper is clearly state and the content is well organized. In this paper, a new framework for learning with logical constraints is proposed. The shortcut satisfaction issue is addressed by introducing dual variables for logical connectives, encoding how the constraint is satisfied. A variational framework is proposed where the logical constraint is expressed as a distributional loss that is compatible with the model\u2019s original training loss. Theoretical analysis shows that the proposed approach bears some nice properties, and experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction. The paper is clearly state and the content is well organized. The work successfully addressed the shortcut satisfaction issue and proposed a variational framework where the logical constraint is expressed as a distributional loss that is compatible with the model\u2019s original training loss. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 61,
        "instruction": "Certified Training: Small Boxes are All You Need: To obtain, deterministic guarantees of adversarial robustness, specialized training methods are used. We propose, SABR, a novel such certified training method, based on the key insight that propagating interval bounds for a small but carefully selected subset of the adversarial input region is sufficient to approximate the worst-case loss over the whole region while significantly reducing approximation errors. We show in an extensive empirical evaluation that SABR outperforms existing certified defenses in terms of both standard and certifiable accuracies across perturbation magnitudes and datasets, pointing to a new class of certified training methods promising to alleviate the robustness-accuracy trade-off.",
        "reference": "The authors propose a variant of interval bound propagation (IBP) for certified training using a small region around a preliminary center, e.g. from a PGD attack. Following a theoretical analysis of the employed box propagation, paying special attention to the role of ReLU activations, the authors present a set of experiments showing consistent improvements in terms of both the standard and robust accuracy, along with ablation studies concerning a number of related considerations. Strengths Introducing an auxiliary IBP problem admitting better approximations. Theoretical analysis of hyperbox growth, with a detailed discussion of the role of ReLU activations following Shi et al. (2021). Consistently improved accuracies corroborating the theory, in addition to a valuable ablation study.   Weaknesses The presentation deviates from professional technical writing in a number of critical parts. This leads to unnecessary confusion, as well as a couple inaccurate or unsubstantiated claims: Improving on all SOTA methods Promising to overcome the robustness-accuracy trade-off Clarity and quality are very good.  Abstract  I strongly recommend that the authors refrain from prematurely framing their contribution as novel, as early as the fourth word in the abstract. Such statements are extremely likely to end up harming the contribution. As other reviewers point out, the picture is not as clear cut with respect to randomized smoothing methods. I would also recommend that the problem and method are described first, before how it compares to current SOTA.     Section 1  What was meant to be conveyed through the statement \"This yields networks with complex neuron interactions\"? Is that about the ReLU activations? As it stands, it is rather vague and redundant. Please spell out the main \"commonly used settings\" deferring further details to the experiments section.   Section 3  First sentence: we address \"this\" challenge. What challenge? It's not clear what the words \"capture\" and \"actual\" serve, and they appear more than once in the same paragraph. I believe \"often still captures the actual worst-case loss\" was meant as \"is a plausible approximation of the worst-case loss\". Expressing intuitions is okay, but there's no need to make inaccurate or unsupported claims in the process. I strongly recommend that the authors rewrite the sentences following \"Intuitively, often only small subsets of the input are misclassified and only a single point will realize the worst-case loss.\" Those statements almost surely don't hold in general, and need not be settled to communicate intuition in the first place.   It would help greatly to include rudimentary experiments to provide better intuitions and empirical evidence to support those motivating observations. Second paragraph: we illustrate \"this intuition\". I recommend to start a new paragraph at: \"we tackle this problem\", after clarifying in the first paragraph what the challenge/problem is. This statement \"leading to significantly reduced approximation errors and thus more precise, although not necessarily sound over-approximation of the loss\" is rather too confounding. Please define the propagation of the small region as a proxy or an auxiliary problem to the original problem of propagating the full region. Please include an equation similar to Eq.3 w.r.t. Bp\u03c4p(x\u2032) or some other placeholder symbol. Then, please explain why this new approximation problem admits a more precise solution, and why it can be understood as a regularization in-between the two extremes. Then, please explain how it relates to the original approximation problem, clarifying what was meant by \"not necessarily sound over-approximation\".   The paragraph titled \"Selecting the Propagation Region\" is good enough. The leading paragraphs need not say as much, IMHO, and perhaps can be employed to better anticipate the theoretical analysis culminating in Theorem 4.1, and possibly other potential analyses going beyond BOX.    The rest of the paper looks good. Only that the repeated claim of overcoming the robustness-accuracy trade-off is too strong, and would at least requires demonstration on a range of learning problems, which the current work does not substantiate. I'm assigning a score of 8 since 7 is no longer available. The revised manuscript addressed my reservations. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. 4: The contributions are significant, and do not exist in prior works. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 62,
        "instruction": "Regression with Label Differential Privacy: We study the task of training regression models with the guarantee of _label_ differential privacy (DP). Based on a global prior distribution of label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a \"randomized response on bins\", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.",
        "reference": "Paper proposes regression with label differential privacy and introduces randomized response on bins as a new mechanism to induce differential privacy on labels. Proposed method is interesting and of practical use where one might want label differential privacy. Empirical evaluation shows that the proposed method performs better than other common DP methods such as Laplace, Exponential, and staircase.  Paper is clearly written and is easy to follow. Assuming the proofs go through (I did not check them in detail), I think the paper will make positive contribution to the DP community. My only question is that there should be some discussion on why it is not compared against other label-DP methods such as [1]. [1] Ghazi, B., Golowich, N., Kumar, R., Manurangsi, P., & Zhang, C. (2021). Deep learning with label differential privacy. Advances in Neural Information Processing Systems, 34, 27131-27145. paper is clearly written and is easy to follow paper is easy to follow and has the potential for a net positive impact. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 63,
        "instruction": "Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement: Object rearrangement is a challenge for embodied agents because solving these tasks requires generalizing across a combinatorially large set of configurations of entities and their locations. Worse, the representations of these entities are unknown and must be inferred from sensory percepts. We present a hierarchical abstraction approach to uncover these underlying entities and achieve combinatorial generalization from unstructured visual inputs. By constructing a factorized transition graph over clusters of entity representations inferred from pixels, we show how to learn a correspondence between intervening on states of entities in the agent's model and acting on objects in the environment. We use this correspondence to develop a method for control that generalizes to different numbers and configurations of objects, which outperforms current offline deep RL methods when evaluated on simulated rearrangement tasks.",
        "reference": "This paper is about the task of object rearrangement. The paper proposes a neurosymbolic approach for solving this problem. The method is described as a hierarchical abstraction approach-- object representations are learned, and a factorized transition graph is constructed over these representations.  The method, \"Neural Constraint Satisfaction\" disentangles representation learning from planning and control, therefore leading to a modularized approach.  Experiments are performed on three block-style datasets. Strengths  Object rearrangement using image inputs is a very interesting problem and I can see a lot of domains where it could be useful: robotics, planning, visual reasoning, navigation / RL to name a few. The paper is well written and does a good job of establishing the preliminaries and the method (algo 1, 2, and the figures are especially useful in this regard) experiments appear to be extensive on three datasets/environments (although, note that I am not an expert in RL/planning), with recent baselines The neuro-symbolic system proposed has the potential to be applicable in various domains, by simply swapping the visual module.  Weaknesses  One weakness (which is fixable) is that IMO it would be better to introduce the problem statement very early on so that readers get a taste of what's to come later.  The introduction is dedicated to mostly explaining the importance and difficulty of object rearrangement, without specifying what input-output spaces are being considered -- this comes very late. It would be interesting to see the method being applied to real world datasets -- i.e. images with real objects (see my suggestion 2 for prospective datasets). Perhaps in such settings, more sophisticated feature learning / entity abstraction methods would need to be used. ablation study/analyses to study each module could enhance the evidence for the efficacy of the method  Suggestions  Increase font size in all figures. Tables are fine. References: there are two related papers that fall under the problem formulation given in Sec 2 Page 2:   https://openaccess.thecvf.com/content_CVPRW_2019/papers/Vision_Meets_Cognition_Camera_Ready/Gokhale_Cooking_With_Blocks__A_Recipe_for_Visual_Reasoning_on_CVPRW_2019_paper.pdf This paper is about an object rearrangement task in the \"blocksworld\" setting but with real images. This one seems to be very similar to \"block-stack\" from this paper. https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560324.pdf procedural planning in videos -- this paper involves complex actions beyond simple rearrangement, but in real-world videos like cooking etc. Clarity: well-written paper. Figures can be improved. Quality: good. analysis from appendix can be moved to the main paper. Novelty: Visual planning (called object rearrangement in this paper) is actually a very hard task.  I expect the neurosymbolic method introduced in this paper to have a decent impact on the vision+reasoning domain. Reproducibility: The algorithms are implementation description are useful, but I didn't see any uploaded code. An effective neurosymbolic method for object re-arrangement, effective across many domains.  Real-world applicability (i.e. with complex images) has not been explored -- all the experimental settings are in synthetic environments, which in my opinion, limits the impact of this paper 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 64,
        "instruction": "Transfer NAS with Meta-learned Bayesian Surrogates: While neural architecture search (NAS) is an intensely-researched area, approaches typically still suffer from either (i) high computational costs or (ii) lack of robustness across datasets and experiments. Furthermore, most methods start searching for an optimal architecture from scratch, ignoring prior knowledge. This is in contrast to the manual design process by researchers and engineers that leverage previous deep learning experiences by, e.g., transferring architectures from previously solved, related problems.\nWe propose to adopt this human design strategy and introduce a novel surrogate for NAS, that is meta-learned across prior architecture evaluations across different datasets. We utilizes Bayesian Optimization (BO) with deep-kernel Gaussian Processes,  graph neural networks for the architecture embeddings and a transformer-based set encoder of datasets. As a result, our method consistently achieves state-of-the-art results on six computer vision datasets, while being as fast as one-shot NAS methods.",
        "reference": "This work proposes a transferrable surrogate for NAS based on Bayesian Optimization with deep-kernel Gaussian Processes. The proposed predictor can be adapt to unseen datasets rapidly by significantly reducing the search cost of NAS. On the NAS-Bench-201 and multiple unseen datasets, this work outperformed recent NAS methods including MetaD2A for the performance of the obtained architecture and search efficiency. Strengths  I think this work well addressed the main limitation of MetaD2A which is the meta-learning-based transferrable NAS method. As this paper said, the meta-learning-based predictor proposed by MetaD2A can be exploited to unseen datasets after once meta-training, which significantly reduces the search time for unseen datasets. However, MetaD2A only exploits the meta-learned predictor, can not adapt to the unseen dataset even if the unseen task provides few-shot samples. This work combines the BO method to tackle the problem of transferrable predictors by allowing them to reflect feedback from unseen tasks.   Weaknesses  I think while the contribution of this work in the structure of predictor is limited, this work assigned the part of the paper for them too much. For example, using both transformer-based set encoder and graph encoder together is almost same with MetaD2A, this paper described it  as the figure and performed ablation study about that. I think it would be better to emphasis the difference between MetaD2A and this work or BO + predictor as a Figure. I think clarity of this paper is needed to be improved as there are mixing between their contributions and previous works' contributions. Actually, applying BO to NAS is not new, yet, to my knowledge, this work is the first to use BO for meta-learning-based predictor for NAS and addresses the important problem in meta-learning-based NAS.  On the NAS-Bench-201 benchmark and multiple unseen datasets, their experiments are solid to support this work. Thus, i think the overall quality is good, yet, one caveat is that NAS-Bench-201 is rather small and narrow benchmark. The good points are that this work successfully addressed the important problem of the meta-learning-based prediction model in NAS with solid experiments. The bad points are that BO is not new in NAS domain, the composition of the paper is needed to be improved, and the benchmark that they used is small. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 4: The contributions are significant, and do not exist in prior works. 2: The contributions are only marginally significant or novel. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 65,
        "instruction": "Selective Frequency Network for Image Restoration: Image restoration aims to reconstruct the latent sharp image from its corrupted counterpart. Besides dealing with this long-standing task in the spatial domain, a few approaches seek solutions in the frequency domain in consideration of the large discrepancy between spectra of sharp/degraded image pairs. However, these works commonly utilize transformation tools, e.g., wavelet transform, to split features into several frequency parts, which is not flexible enough to select the most informative frequency component to recover. In this paper, we exploit a multi-branch and content-aware module to decompose features into separate frequency subbands dynamically and locally, and then accentuate the useful ones via channel-wise attention weights. In addition, to handle large-scale degradation blurs, we propose an extremely simple decoupling and modulation module to enlarge the receptive field via global and window-based average pooling. Integrating two developed modules into a U-Net backbone, the proposed Selective Frequency Network (SFNet) performs favorably against state-of-the-art algorithms on five image restoration tasks, including single-image defocus deblurring, image dehazing, image motion deblurring, image desnowing, and image deraining.",
        "reference": "In this paper, the author proposes a Selective Frequency Network (SFNet) for image restoration. In SFNet, the author designs a multi-branch and content-aware module to decompose the feature into separate frequency sub-bands, and then uses the channel-wise attention mechanism to emphasize the useful information. In addition, to cope with the large-scale degradation kernel, the author proposes an decoupling and modulation module to enlarge the receptive field based on global and window-based average pooling. The paper is well presented and claims better performance on different tasks, while there are some problems in theoretical proof and experimental part. There are some problems in the proof of this paper. In Eq. 8 and Eq. 9, the author describe that the first row(diag(1,0,0,\u2026,0)) in the Fourier domain is low frequency and the others are high frequency. According to this description, for a feature of size CxHxW, low frequency is the mean value in the spatial dimension and the size of low frequency is Cx1x1, which can be seen in Section 3.2 and Appendix C. This description is contrary to the commonly used low frequency and high frequency. In practice, the size of low frequency and high frequency are CxHxW and low frequency occupies a large part of information and high frequency is only in the edge area. In the two-dimensional Fourier domain (the data has been processed by fftshift), we generally believe that the low frequency is the region within the radius R, and other regions are the high frequency [1-3]. Therefore, the Eq. 8 and Eq. 9 may not be consistent with the actual situation. In other words, the author only provided theoretical proof of MDSF for extreme low frequency, but did not provide theoretical proof of MDSF for other low frequency. More importantly, the author did not demonstrate that the low frequency in the MDSF meets Eq. 8. The low frequency in the MDSF is obtained through learning, not the mean value of the feature. In addition, the size of low frequency is CxHxW, which is completely inconsistent with the low frequency scale defined by the author. Therefore, this proof is problematic and the author confused the definition of low frequency. The author did not write clearly the experimental settings, such as the number of branch in MDSF, the number of group in MDSF. The experiment is not rigorous. In the ablation experiment, the number of epoch is inconsistent in the ablation experiments of different components. Authors should unify the number of epochs. There are some typos in Section 4.8 and Eq. 15.  [1] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Focal Frequency Loss for Image Reconstruction and Synthesis. In ICCV, 2021. [2] Mu Cai, Hong Zhang, Huijuan Huang, Qichuan Geng, Yixuan Li, and Gao Huang. Frequency Domain Image Translation: More Photo-realistic, Better Identity-preserving. In ICCV, 2021. [3] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A Fourier-based Framework for Domain Generalization. In CVPR, 2021. The paper is well presented and claims better performance on different tasks, while there are some problems in theoretical proof and experimental part. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 66,
        "instruction": "Scaling Up Probabilistic Circuits by Latent Variable Distillation: Probabilistic Circuits (PCs) are a unified framework for tractable probabilistic models that support efficient computation of various probabilistic queries (e.g., marginal probabilities). One key challenge is to scale PCs to model large and high-dimensional real-world datasets: we observe that as the number of parameters in PCs increases, their performance immediately plateaus. This phenomenon suggests that the existing optimizers fail to exploit the full expressive power of large PCs. We propose to overcome such bottleneck by latent variable distillation: we leverage the less tractable but more expressive deep generative models to provide extra supervision over the latent variables of PCs. Specifically, we extract information from Transformer-based generative models to assign values to latent variables of PCs, providing guidance to PC optimizers. Experiments on both image and language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent variable distillation substantially boosts the performance of large PCs compared to their counterparts without latent variable distillation. In particular, on the image modeling benchmarks, PCs achieve competitive performance against some of the widely-used deep generative models, including variational autoencoders and flow-based models, opening up new avenues for tractable generative modeling. Our code can be found at https://github.com/UCLA-StarAI/LVD.",
        "reference": "This paper is about improving the expressivity of large scale probabilistic circuits (PCs). Finding a good starting point for EM based learning of these large latent variable models is problematic and the authors propose one such solution to this problem.  The main idea is to obtain semantic-aware assignments (called supervision) to the latent variables from less tractable deep generative models and then perform maximum likelihood learning over the data combined with these newly assigned latent variables. The variables that receive these assignments are said to be materialized and the assignments themselves are generated by a deep generative model by clustering over the latent embeddings of the observed (sub)space(s). When all the latent variables have been assigned values, the optimization (MLE) can be performed in closed form. The result of MLE serves as a starting point for optimizing the data likelihood in the following steps. The authors propose a couple of techniques to efficiently compute the MLE parameters which include exploiting conditional independency achieved by materialized latent variables and fine tuning the latent distributions only while keeping the parameters learned over the observed space fixed. On CIFAR and ImageNet datasets, the proposed method has shown superior performance compared to other SoTA TPMs learners and were comparable to less tractable but expressive flow-based models and VAEs. Strengths:  a novel method to improve the performance of large scale probabilistic circuits. motivating empirical evaluations on three image datasets to show the performance improvements over very large circuits.  clear writeup with good examples.  Weaknesses: I didn't find major weaknesses in the technical aspects of the paper. Please see some questions and comments below. The paper is well written with illustrative examples. I found the idea to be novel and the authors have detailed their experimental setups for reproducibility. The authors have addressed an important practical issue with large scale probabilistic circuits. The expressivity of these models tend to plateau once a certain capacity is reached typically in the order of millions of parameters. With such large scale circuits of deeply nested latent variables, the optimization landscape becomes very complex and finding a local minima becomes hard. The main idea in the paper is to make latent variables observed by assigning them values and performing an optimization step that works on less number of latent variables. This will give the EM step a good starting point. I found the idea to obtain latent variable assignments using a deep generative model to be interesting. The method seems to be effective according to the empirical results presented in the paper.  Questions: a) In the introduction it is stated that the expressive power of PCs should monotonically increase with respect to the number of parameters. I am curious if these models don't suffer from overfitting issues. Maybe the authors could comment on this aspect.b) Did all the models have the same structure? c) Could the method be useful for smaller scale PCs? It seems that the clustering in the latent embedding is the key reason behind the performance boost of LVDs. d) The significant speed up in training (for all the datasets) should probably be presented in the paper since it is mentioned in the introduction.  e) Have you done any analysis on the number of LVs that were materialized and the performance of the PCs? 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 4: The contributions are significant, and do not exist in prior works. 4: The contributions are significant, and do not exist in prior works. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 67,
        "instruction": "Improving Differentiable Neural Architecture Search by Encouraging Transferability: Differentiable neural architecture search  methods are increasingly  popular due to their computational efficiency. However, these methods have unsatisfactory generalizability and stability. Their searched    architectures  are often  degenerate with a dominant number of skip connections and perform unsatisfactorily on test data. Existing methods for solving this problem   have  a variety of limitations, such as cannot prevent the happening of architecture degeneration, being excessively restrictive in setting the number of skip connections,  etc. To address these limitations, we propose a new approach for improving the generalizability and stability of differentiable NAS, by  developing a transferability-encouraging tri-level optimization  framework  which improves the  architecture  of a main model  by encouraging  good transferability to an auxiliary model. Our framework  involves three stages performed end-to-end: 1) train network weights of  a main model; 2) transfer knowledge from the  main  model  to an auxiliary model; 3) optimize the architecture of the main model by maximizing its  transferability to the  auxiliary model. We propose a new knowledge transfer approach based on matching quadruple relative similarities.  Experiments on several datasets demonstrate the effectiveness of our method.",
        "reference": "The authors tackle the current limitation that most existing NAS methods suffer from, which are unsatisfactory generalizability and stability, such as generating a dominant number of skip connections or poor test performance. To address this, the authors propose a transferability-encouraging tri-level optimization framework for improving the main model and auxiliary models. They demonstrate the effectiveness of their methods. Strength  The paper is easy to read. They tackle the practical limitations of neural architecture search. Experiments are extensive.  Weaknesses  My main concern for this paper is the quality of the validation sets, which play a key role in measuring \u201ctransferability\u201d from the main model to the auxiliary models. The poor representativeness of validation sets (class-imbalanced, noise, etc) for the main model and auxiliary model may affect the optimization process (step 3) and this should be clearly analyzed. What if we cannot get the class-balanced and non-noisy validation set in a practical scenario? The proposed method with a poor validation set still outperforms the baseline models without any degeneration? If not, are there any possible methods to handle this issue? For one of the baseline models, OFA-large (Table 4) seems to show the best performance, but I was not able to find any explanation for it. As far as I know, OFA can sample subnets from the supernet while keeping (or transferring) the supernet\u2019s pretrained knowledge, which is similar to the proposed method. It would be good to compare the proposed method with OFA more closely.  (minor) It would be helpful if the overview illustration for the proposed method is provided. There exist few points to be clarified for improving the quality and originality, as mentioned above. I enjoyed reading the paper, but several improvements seem to be required. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 68,
        "instruction": "MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions: Due to their superior results, Transformer-based models such as BERT have become de facto standards in many Natural Language Processing (NLP) applications. However, the intensive use of complex non-linear functions within the Transformer architecture impairs its computing efficiency and complicates corresponding accelerator designs, because non-linear functions are generally computation-intensive and require special hardware support. In light of this, we propose MA-BERT, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss. Specifically, we propose four correlated techniques that include approximating softmax with a two-layer neural network, replacing GELU with ReLU, fusing normalization layers with adjacent linear layers, and leveraging knowledge transfer from baseline models. Through these techniques, we are able to eliminate the major non-linear functions in Transformer-based models and obtain MA-BERT with only matrix arithmetic and trivial ReLU operations without compromising on accuracy. With mainly regular matrix arithmetic operations, MA-BERT enables hardware-friendly processing on various computing engines, including CPUs and GPUs. Our experimental results show that MA-BERT achieves up to 27% and 41% reduction in inference time on CPU and GPU, respectively, with comparable accuracy on many downstream tasks compared to the baseline BERT models. ",
        "reference": "This paper aims at reducing the complexity of BERT models to accelerate their inference by putting together a series of previously-introduced techniques. To this end, the authors propose to approximate softmax with a two-layer neural network, replace GELU with ReLU, fuse normalization layers with adjacent linear layers, and use knowledge distillation. It has been shown that the acceleration of 1.27x can be obtained on CPUs using the aforementioned four techniques while achieving a comparable accuracy performance w.r.t. the baseline model. Strengths:  -- I believe this work is well-motivated. The paper targets a very important issue (i.e., reducing latency of inference) associated with large models such as BERT.  -- The detection of the source of the latency overhead during inference is insightful as shown in Fig. 1.  -- The amount of speedup during the inference on CPUs is interesting and shows the effectiveness of the proposed techniques. -- The paper in general is well-written and easy to understand. Weaknesses: -- In general, the contribution of this paper is rather limited since each of those techniques have been previously-proposed. -- It would have been great if the breakdown of cycles in Fig. 1 was shown for the sequence length of 128 to be compatible with experimental results in Table 2. -- My major concern about this paper is the lack of comparison with prior works. First, there is no results for inference on GPUs. Second, there is no direct comparison with prior works such as Linformer? Where does the proposed method stand w.r.t. prior works? Finally, what speedup can be obtained when using MA-DistilBERT or MA-DistilRoBERTa? -- The GELU and softmax functions can be approximated using polynomials similar to I-BERT with no impact on accuracy. This work puts together a series of previously-proposed works to reduce the complexity of BERT models. The paper is well-written, clear and easy to understand. I believe the results of this paper can be reproduced. In general, I believe the contribution of this paper is rather limited since the main method is a combination of previous works. On the other hand, the proposed method is simple and effective (when running on CPUs). My main concern is the lack of comparison with other works. It is not clear where this works stands w.r.t. prior works that also tries to reduce the complexity of BERT models such as Linformer and I-BERT. There is also no evaluation on GPUs while it has been mentioned as one of the motivation of this work in the introduction. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 69,
        "instruction": "Efficient Certified Training and Robustness Verification of Neural ODEs: Neural Ordinary Differential Equations (NODEs) are a novel neural architecture, built around initial value problems with learned dynamics which are solved during inference. Thought to be inherently more robust against adversarial perturbations, they were recently shown to be vulnerable to strong adversarial attacks, highlighting the need for formal guarantees.  However, despite significant progress in robustness verification for standard feed-forward architectures, the verification of high dimensional NODEs remains an open problem. In this work we address this challenge and propose GAINS, an analysis framework for NODEs combining three key ideas: (i) a novel class of ODE solvers, based on variable but discrete time steps, (ii) an efficient graph representation of solver trajectories, and (iii) a novel abstraction algorithm operating on this graph representation. Together, these advances enable the efficient analysis and certified training of high-dimensional NODEs, by reducing the runtime from an intractable $\\mathcal{O}(\\exp(d)+\\exp(T))$ to $\\mathcal{O}(d+T^2\\log^2T)$ in the dimensionality $d$ and integration time $T$.  In an extensive evaluation on computer vision (MNIST and Fashion-MNIST) and time-series forecasting (Physio-Net) problems, we demonstrate the effectiveness of both our certified training and verification methods.",
        "reference": "This paper proposes a method called GAINS to address the robustness certification of neural ODEs via combining ODE solvers with variable steps and an efficient graph representation of solver trajectories. The authors provide some arguments showing the proposed approach significantly reduce the run time.  Numerical study is also performed on MNIST, FMNIST, and Physio-Net to demonstrate the certified robustness of the proposed method. Strength:  The combination of ODE solvers with variable steps and the graph representation of solver trajectories seem to be interesting.   The improvement of running time is significant.   Weaknesses:  MNIST is a very simple task. Is it possible to run the proposed method for CIFAR10?  The theoretical novelty of this paper is unclear. The paper is well written and includes some solid results on certified robustness of the neural ODEs. Codes have been provided to reproduce the results. I think the idea of combining ODE solvers with variable steps and the graph representation of solver trajectories is interesting. I am a little bit concerned with the scalability of the proposed approach, since MNIST is really considered as a very simple task. For now I give a \"6.\" Depending on how the authors address my concerns (on scalability and theoretical novelty), I may either increase or decrease my score. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 70,
        "instruction": "Arbitrary Virtual Try-on Network: Characteristics Representation and Trade-off between Body and Clothing: Deep learning based virtual try-on system has achieved some encouraging progress recently, but there still remain several big challenges that need to be solved, such as trying on arbitrary clothes of all types, trying on the clothes from one category to another and generating image-realistic results with few artifacts. To handle this issue, we propose the Arbitrary Virtual Try-On Network (AVTON) that is utilized for all-type clothes, which can synthesize realistic try-on images by preserving and trading off characteristics of the target clothes and the reference person. Our approach includes three modules: 1) Limbs Prediction Module, which is utilized for predicting the human body parts by preserving the characteristics of the reference person. This is especially good for handling cross-category try-on task (e.g., long sleeves \\(\\leftrightarrow\\) short sleeves or long pants \\(\\leftrightarrow\\) skirts, etc.), where the exposed arms or legs with the skin colors and details can be reasonably predicted; 2) Improved Geometric Matching Module, which is designed to warp clothes according to the geometry of the target person. We improve the TPS-based warping method with a compactly supported radial function (Wendland's \\(\\Psi\\)-function); 3) Trade-Off Fusion Module, which is to trade off the characteristics of the warped clothes and the reference person. This module is to make the generated try-on images look more natural and realistic based on a fine-tuning symmetry of the network structure. Extensive simulations are conducted and our approach can achieve better performance compared with the state-of-the-art virtual try-on methods.",
        "reference": "This paper proposes a novel virtual try-on network with a newly collected dataset for handling the all-type clothes try-on task and the cross-category try-on task. It consists of three sub-modules: 1) Limbs Prediction Module, which is designed for predicting the human body parts by preserving the characteristics of the reference person; 2) Improved Geometric Matching Module, where the radial basis function of TPS is replaced by the Wendland\u2019s \u03c8-function to warp clothes more reasonably; 3) Trade-Off Fusion Module, which aims to trade off the characteristics of the warped clothes and the reference person. Extensive experiments are conducted to demonstrate the superior performance of the proposed method. Paper Strengths:  The functions and principles of the three sub-modules are clearly described and illustrated. This paper makes an improvement in the mathematical formulation of TPS, and there are extensive analysis contents to verify its effectiveness. This paper provides a new dataset, which contains a richer variety of clothes. Paper Weakness: Compared with the top clothes, what is the main difficulty of the bottom clothes and whole clothes? In addition to the newly collected dataset, whether the algorithm part is specially designed to handle different clothes categories? The dataset collected by this paper is not introduced in detail, such as the number, resolution, and proportion of clothes categories. From 2021 to 2022, there are a lot of works on image-based virtual try-on, however, this method is not compared with them, the latest work compared in this paper is ACGPN in 2020. Most extensive methods use FID to measure the similarity of data distributions between the generated results and the referenced images, which does not appear in this paper. Several grammar and writing errors can be found in the paper. Although there are some writing errors, this paper is well-organized and the overall logic is clear. It changes the radial basis function of TPS and takes it as the main contribution, and extensive analysis contents have proved its effectiveness. However, the architecture basically follows the previous methods, which does not reflect a more significant innovation. Virtual try-on has great commercial value and application prospects. In this paper, three sub-modules are designed to handle the all-type clothes try-on task and the cross-category try-on task. It is relatively novel to make an improvement in the mathematical formulation of TPS. However, the architecture basically follows the previous methods, and it is not clear which part is explicitly designed for all-type clothes. Besides, it lacks comparisons with the latest methods in recent two years. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 71,
        "instruction": "UL2: Unifying Language Learning Paradigms: Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. We release Flax-based T5X model checkpoints for the 20B model publicly.\n",
        "reference": "This paper proposes a multi-task pretraining objective that outperforms and unifies prior objectives, it attempts to decouple the modeling architecture and the pretraining objective. The proposed objective shows strong performance gain over SOTA. Strength This paper is well written, the experiments are extensive, and detailed ablation study is presented. Weakness While the experiments are extensive, there are questions about the presentation and the conclusion.  relative performance comparison is not a good metric when the baseline is bad. It leads to 1k+ gain in table 4, skewing the averaging comparison. \u201cThe best decoder baseline model here is the Prefix-LM decoder model, which is about 10% worse than the T5 baseline.\u201d While this statement is true on average, the performance difference is not true for all tasks, notably LM. It contradicts the universal claim (\u201c It is clear from these results that encoder-decoder models should be preferred over decoder-only models if and only if there is no concern about storage\u201d) this paper is making. \u201cRegardingPrefix-LM pre-training,  it is interesting that Prefix-LM actually outperforms the T5 span corrupt setup by+16.7%.\u201d Is this finding contrary to Raffel et al., 2019?  Why are some numbers missing in table 6? The paper is well written, the experiments is extensive, and it improves over existing multi-task pretraining objectives. Code and model are publicly available. While the experiment is extensive, some of the strong claims are not supported by the experiment. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 72,
        "instruction": "CASR: Generating Complex Sequences with Autoregressive Self-Boost Refinement: There are sequence generation tasks where the best order to generate the target sequence is not left-to-right. For example, an answer to the Sudoku game, a structured code like s-expression, and even a logical natural language answer where the analysis may be generated after the decision. We define the target sequences of those tasks as complex sequences. Obviously, a complex sequence should be constructed with multiple logical steps, and has dependencies among each part of itself (e.g. decisions depend on analyses). It's a great challenge for the classic left-to-right autoregressive generation system to generate complex sequences. Current approaches improve one-pass left-to-right generation on NLG tasks by generating different heuristic intermediate sequences in multiple stages. However, for complex sequences, the heuristic rules to break down them may hurt performance, and increase additional exposure bias. To tackle these challenges, we propose a PLM-friendly autoregressive self-boost refinement framework, CASR. When training, CASR inputs the predictions generated by the model itself at the previous refinement step (instead of those produced by heuristic rules). To find an optimal design, we also discuss model architecture, parameter efficiency and initialization strategy. By evaluating CASR on Sudoku, WebQSP, MTOP and KVRET through controlled experiments and empirical studies, we find that CASR produces high-quality outputs. CASR also improves Accuracy on Sudoku (70.93% --> 97.28%) and achieves state-of-the-art performance on KVRET with Micro F1 score (67.88% --> 70.00%).",
        "reference": "This paper proposes a novel approach [CASR] for sequence generation where multiple language models are connected in an iterative manner and predictions of a previous timestep are taken as an input to the model at future timestep [different from vanilla RNN]. The rationale is that sequence generation problems only specify \u201cdifficult\u201d FINAL sequences which need to be solved, and the intermediate order to solve \u201csimpler sequences\u201d is defined by heuristics [Tan et Al]. Strengths  The authors demonstrate that a network can itself learn the simpler sequences it needs to solve, and that there is a natural order of progression from solving \u201csimpler\u201d sequences to complex ones. Furthermore, they empirically reveal that this progression is different from the simple left-to-right solving order that sequence generation usually takes.   The manuscript is well crafted, with convincing experiments and detailed ablations. Authors claim SOTA [70%] on KVRET F1 metric.   Weaknesses  The basic training process [Algo 2] has a limitation that the language model at each timestep needs to be trained fully [line 3] before moving on to the next timestep. This prevents the network to be E2E trainable, leaving CASR prone to parameter-effficient tuning. [3.3].  Authors define the notion of density measuring the attention magnitude at each sequence token [5.2]. Increasing density at each recurrent step, seems to show that the prediction made by the model becomes more informative. I am curious as to whether the density becomes maximum when the network converges to optimal solution. Please clarify this.  On sudoku, the restart strategy lags behind Continue strategy by a huge difference ~20% (table 4). But we don\u2019t observe this trend in other three datasets, well recognized in the community (Table 3).  Could the authors please mention their intuitions on these observations? Authors have released the code for this work anonymously. This work makes a SINGLE minor change to the mathematical formulation of Tan et Al [Figure 1], but still improves empirical results on only 1 out of 3 datasets studied. [Sudoku is a toy dataset from Kaggle which authors don\u2019t show other methods against]. Due to lack of novelty in technical contribution, but still beating SOTA on 1 dataset, this work has incremental contributions. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 73,
        "instruction": "Suppression helps: Lateral Inhibition-inspired Convolutional Neural Network for Image Classification: Convolutional neural networks (CNNs) have become powerful and popular tools since deep learning emerged for image classification in the computer vision field. For better recognition, the dimensions of depth and width have been explored, leading to convolutional neural networks with more layers and more channels. In addition to these factors, neurobiology also suggests the widely existing lateral inhibition (e.g., Mach band effect), which increases the contrast of nearby neuron excitation in the lateral direction, to help recognition. However, such an important mechanism has not been well explored in modern convolutional neural networks. In this paper, we explicitly explore the filter dimension in the lateral direction and propose our lateral inhibition-inspired (LI) design. Our naive design incorporates the low-pass filter, while eliminating the central weight to mimic the inhibition strength decay. The inhibition value is computed from the filtering result of the input, with a simple learnable weight parameter per channel for multiplication to decide the strength. Then the inhibition value is subtracted from the input as suppression, which could increase the contrast to help recognition. We also suggest an alternative using depthwise convolution, as a general form. Our design could work on both the plain convolution and the convolutional block with residual connection, while being compatible with existing modules. Without any channel attention along the channel dimension, the preliminary results demonstrate an absolute improvement of 3.68\\% and 0.69\\% over AlexNet and ResNet-18, respectively, in the ImageNet data set, with little increase in parameters, indicating the merits of our design to help feature learning for image classification.",
        "reference": "The authors propose to add a biologically inspired lateral inhibition mechanism into deep convolutional networks for image recognition. When incorporated into AlexNets and ResNets, LI seems to improve performance on ImageNet classification without increasing trainable parameters. The authors examine the LI filter weights and find a biologically-resemblant center-surround pattern of inhibition. Strengths:  There are considerable gains on AlexNet and ResNet while using LI, however, I would like to note that these are preliminary results as also highlighted by the authors.  Weaknesses:  The proposed work lacks novelty, several methods in the past have tried to apply a very similar lateral or divisive inhibition mechanism to deep convolutional networks and have reported gains in image classification performance (particularly when added to AlexNet). See [1] [2] and [3] for example. The proposed work is almost exactly similar to [1]. The evaluation is very preliminary and lacks comparison to suitable baselines (other kinds of normalization such as BatchNorm, LayerNorm, etc.) or other normalization techniques such as [1, 2] which are very relevant.  References:  Hasani, H., Soleymani, M., & Aghajan, H. (2019). Surround modulation: A bio-inspired connectivity structure for convolutional neural networks. Advances in neural information processing systems, 32. Miller, M., Chung, S., & Miller, K. D. (2021, September). Divisive Feature Normalization Improves Image Recognition Performance in AlexNet. In International Conference on Learning Representations. Pan, X., Giraldo, L. G. S., Kartal, E., & Schwartz, O. (2021). Brain-inspired weighted normalization for CNN image classification. bioRxiv. The proposed work is not novel and evaluations are preliminary. The authors don't discuss sharing code or trained models, which is troublesome to reproduce the presented results. The proposed work is preliminary and lacks the novelty and quality of work expected at ICLR. I do not recommend accepting this paper at this stage. I suggest the authors to please consider a suitable workshop for this work and a significant extension of this work could be suitable for the ICLR audience with wider evaluation using suitable baselines and tasks. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 1: The contributions are neither significant nor novel. 1: The contributions are neither significant nor novel. NO. 3: reject, not good enough 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 74,
        "instruction": "A Study of Biologically Plausible Neural Network: the Role and Interactions of Brain-Inspired Mechanisms in Continual Learning: Humans excel at continually acquiring, consolidating, and retaining information from an ever-changing environment, whereas artificial neural networks (ANNs) exhibit catastrophic forgetting. There are considerable differences in the complexity of synapses, the processing of information, and the learning mechanisms in biological neural networks and their artificial counterpart, which may explain the mismatch in performance. We consider a biologically plausible framework that constitutes separate populations of exclusively excitatory and inhibitory neurons which adhere to Dale's principle and the excitatory pyramidal neurons are augmented with dendritic-like structures for context-dependent processing of stimuli. We then conduct a comprehensive study on the role and interactions of different mechanisms inspired by the brain including sparse non-overlapping representations, Hebbian learning, synaptic consolidation, and replay of past activations that accompanied the learning event. Our study suggests that employing multiple complementary mechanisms in a biologically plausible architecture, similar to the brain, can be effective in enabling continual learning in ANNs.",
        "reference": "This paper evaluates previous work on biologically plausible DNNs in the setting of continual learning (CL). Namely, they evaluate ideas around Dale\u2019s principle, Active Dendrites, heterogenous dropout, Hebbian learning, synaptic consolidation and experience replay. They present experimental evidence that these ideas can individually improve the performance on CL, or be combined to achieve greater improvement. This paper provided a useful list of biologically-inspired modifications to DNNs. I find the main idea interesting (investigating such modifications\u2019 usefulness to CL).   The paper is a compilation of already published work. Some of these insights (s.a. dropout being useful for CL) can be found in literature. The experiments are exclusively done on MNIST-based datasets. While they provide evidence to support the claims of this paper, it would be useful to understand whether this translates into other image domains. I found the discussion around the properties of biological neural networks a bit confusing. Concretely, I don\u2019t think that the terminology was clearly described. Novelty: I do not think that the ideas expressed in this paper are novel. A \u201crelated work\u201d section would be a great way to set the paper apart from similar insights expressed in CL literature, and point out how the ideas of this paper are different. While the paper presents an interesting discussion, it reuses ideas from other work and presents limited experimental evidence. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 75,
        "instruction": "pFedKT: Personalized Federated Learning via Knowledge Transfer: Federated learning (FL) has been widely studied as a new paradigm to achieve multi-party collaborative modelling on decentralized data with privacy protection. Unfortunately, traditional FL suffers from Non-IID data distribution, where clients' private models after FL are even inferior to models trained standalone. Existing approaches to tackle this challenge fall into two directions: a) pursuing a better global model through mitigating biases of private models, and b) improving personalized private models by personalized federated learning (PFL). Still, both of them have limited accuracy improvements in private models. To this end, \\textit{we design pFedKT, a novel personalized federated learning framework with knowledge transfer, towards boosting the performances of personalized private models on Non-IID data}. It involves two types of knowledge transfer: a) transferring \\textit{historical private knowledge} to new private models by local hypernetworks; b) transferring \\textit{the global model's knowledge} to private models through contrastive learning. After absorbing the historical private knowledge and the latest global knowledge, the personalization and generalization of private models are both enhanced. Besides, we derive pFedKT's generalization and prove its convergence theoretically. Extensive experiments verify that pFedKT presents $0.31\\%-3.46\\%$ accuracy improvements of private models than the state-of-the-art baseline.",
        "reference": "This paper aims to improve the performance of personalized federated learning, and, for which, the authors propose two knowledge transfer schemes. In particular, the historical knowledge learned in the local clients is transferred from the hypernetwork, which stores the knowledge of previous local models, to the current local model. Also, the global knowledge, obtained by the aggregation of local models, is transferred to the local clients based on the contrastive learning loss, where the similarity between the updated local model and the aggregated global model is maximized while the similarity between the updated local model and the previous local model is minimized. The authors validate the performance of their model, named as pFedKT, on two image (i.e., CIFAR-10/100) and one language (i.e., Stack Overflow) datasets, on which the pFedKT outperforms relevant personalized federated learning baselines. Strengths  The proposed historical knowledge transfer scheme with the hypernetwork brings the performance improvement for the personalized federated learning tasks. The authors make effort to theoretically analyze the generalization and convergence bounds of the proposed pFedKT, while they are mostly inspired by the previous work [1] though. The authors perform extensive analyses on the proposed pFedKT, by varying the hyperparameters, and by ablating the knowledge transfer mechanisms.  Weaknesses  The proposed global knowledge transfer scheme is not convincing enough in terms of both the motivation- and experiment-sides.   At first, the authors argue that, by maximizing the similarity between the locally updated model and the globally aggregated model while minimizing the similarity between the locally updated model and the previously updated local model, the proposed pFedKT improves the generalization performance. However, this design choice is not convincing, since the authors already transfer the historical knowledge in the previously updated local model with the hypernetworks, meanwhile, the historical knowledge is negatively considered (i.e., historical information is avoided) in the contrastive learning loss. Therefore, two objectives are conflicts in the federated learning.    In the experiment-side, the proposed global knowledge transfer scheme also does not bring the meaningful performance improvement, i.e., not much helpful for the personalized federated learning. For example, in Figure 7, the proposed global knowledge transfer scheme based on the contrastive loss does not bring the performance improvement. Similarly, in Table 11, the results w/ and w/o contrastive losses are very similar.    Furthermore, in the experimental-side, it is unclear whether the proposed global knowledge transfer scheme can provide better generalization ability empirically. While the authors provide the theoretical result for the generalization bound, I suggest authors to include additional empirical results, if possible, which makes it more convincing.   In Section 3.4, the authors only compare the computational and storage efficiencies of the proposed pFedKT against the complex pFedHN model. It is meaningful to discuss the efficiencies of the most basic FedAvg model, as well as the other contrastive- and hypernetwork-based federated learning models, such as MOON and Fed-ROD.  Also, in Section 3.4, the authors argue that the proposed pFedKT has obvious strengths against the pFedHN model, since the pFedHN baseline has the larger hypernetwork in the server-side, while the proposed pFedKT has smaller hypernetworks in the client-side. However, this is not convincing, since if we have 1,000 clients, we have 1,000 individual hypernetworks distributed to 1,000 clients, and, in the global view, the size of 1,000 individual hypernetworks would be larger than the size of one hypernetwork in the server. The analysis results in Section 5.3 may be problematic. The authors argue the proposed pFedKT can converge during 100 rounds, therefore, conduct analyses with 100 rounds; however, pFedKT does not converge until 100 rounds, as shown in Figure 4. Since knowledge distillation-based federated learning methods share similar sprits to the proposed knowledge transfer-based federated learning model: the knowledge distillation allows the local/global models to transfer their knowledge effectively, I suggest authors to compare such knowledge distillation-based federated learning baselines: FedPHP, FML and KT-pFL, discussed in the related work section.   [1] Personalized federated learning using hypernetworks, ICML 2021. Clarity  The main idea of the proposed global knowledge transfer scheme with the contrastive loss is not clear enough (See Weaknesses above for details). In regards to the efficiency, the advantage of the proposed models compared to others is not sufficiently clear (See Weaknesses above for details). In Figure 1 (b), it is unclear how to measure the global and local model accuracies. Specifically, in main experiments, the authors report the test accuracy based on the local private data with the local model; then, how to measure the global model accuracy, and how to compare this global model accuracy to the local model accuracies?  Quality  The experimental quality of the analyses in Section 5.3 might be low, since the authors conduct the analyses with the unconverged model.  In Section 2.1, \"clients train the received GM on local datasets from scratch\" should be tone-downed, since the client trains the local model from the globally aggregated model, which indeed contains the information for the local model; not training from the scratch.  Novelty  The novelty is mild, since, for the knowledge transfer, the concepts of hypetnetworks and contrastive learning are already proposed in the previous work, such as pFedHN, Fed-ROD, and MOON; however, the differences are faithfully and sufficiently discussed in the related work section as well as other sections.  Reproducibility  The authors do not provide the source code that lowers the reproducibility of this paper; however, the authors plan to release the source code after the acceptance. Therefore, the reproducibility will be probably high. The main idea of the proposed global knowledge transfer scheme based on the contrastive loss is not convincing (See Weaknesses above), and there are some improvement points, such as efficiency analyses in Section 3.4 which are not sufficient, and model analyses which are perhaps conducted without the model convergence. Therefore, I cannot recommend the acceptance. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 76,
        "instruction": "FARE: Provably Fair Representation Learning: Fair representation learning (FRL) is a popular class of methods aiming to produce fair classifiers via data preprocessing. However, recent work has shown that prior methods achieve worse accuracy-fairness tradeoffs than originally suggested by their results. This dictates the need for FRL methods that provide provable upper bounds on unfairness of any downstream classifier, a challenge yet unsolved. In this work we address this challenge and propose Fairness with Restricted Encoders (FARE), the first FRL method with provable fairness guarantees. Our key insight is that restricting the representation space of the encoder enables us to derive suitable fairness guarantees, while allowing empirical accuracy-fairness tradeoffs comparable to prior work. FARE instantiates this idea with a tree-based encoder, a choice motivated by inherent advantages of decision trees when applied in our setting. Crucially, we develop and apply a practical statistical procedure that computes a high-confidence upper bound on the unfairness of any downstream classifier. In our experimental evaluation on several datasets and settings we demonstrate that FARE produces tight upper bounds, often comparable with empirical results of prior methods, which establishes the practical value of our approach. \n",
        "reference": "In this paper authors exploit the use of a restricted encoder to derive a provably fair (group fairness) representation which has the ability to upper bound the unfairness of any down stream classifier. They demonstrate this ability through the use of an optimal adversary, i.e., a classifier which tries to predict the sensitive variable given the representation. Such a bound is computable as the restriction in the encoder leads to a finite set of representations. Strengths Clear derivations. Sufficient experimental validation. Very clear demonstration of the core concepts even for a casual reader. Weaknesses The classifiers used for empirical validation are fairly simple, which might be suitable for the relatively small datasets being used. However using a diverse set of classifiers for empirical validation might be preferable. Clarity and quality The paper clearly introduces the problem, provides extensive literature survey related to different aspects of the paper and clearly separates out these references in section 2. It provides a demonstrative example in Figure 1 which helps the reader quickly understand the main idea. The empirical and theoretical validation for the proposed technique are very neatly communicated to the reader. The authors motivate the need for provable guarantees and at the same time demonstrate that these do not come with a significant cost in terms of accuracy. Novelty The critical contribution of this paper is in the provability of the fairness guarantees for any down-stream classifier operating on the proposed representations. This paper contributes to the very critical area of fair representation learning. The extensive literature survey and the clear description of the contributions of proposed technique in contrast with existing literature can make it a very good introduction paper for readers delving into this topic. The experimental validation could be strengthened by focussing on tasks (at least simulated tasks) where the data sizes are larger. This can allow for the development of more powerful downstream classifiers which might help provide a stronger estimate of accuracy of an unfair classifier and thus the loss in accuracy due to the fair representation. It would be also helpful if the authors discuss the impact of balance in the training data on the granularity of the restricted representations. It is not uncommon in many domains to be presented with datasets which are highly imbalanced w.r.t. sensitive variables. In such cases Fairness-aware categorical splits could possibly lead to uninformative representations for classification purposes. Adding a discussion on balance of training data would be helpful to readers dealing with such datasets. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 77,
        "instruction": "ONLINE RESTLESS BANDITS WITH UNOBSERVED STATES: We study the online restless bandit problem, where each arm evolves according to a Markov chain independently, and the reward of pulling an arm depends on both the current state of the corresponding Markov chain and the action. The agent (decision maker) does not know the transition kernels and reward functions, and cannot observe the states of arms all the time.  The goal is to sequentially choose which arms to pull so as to maximize the expected cumulative rewards collected.  In this paper, we propose TSEETC, a learning algorithm based on Thompson Sampling with Episodic Explore-Then-Commit. The algorithm proceeds in episodes of increasing length and each episode is divided into exploration and exploitation phases. In the exploration phase in each episode, action-reward samples are collected in a round-robin way and then used to update the posterior as a mixture of Dirichlet distributions.  At the beginning of the exploitation phase, TSEETC generates a sample from the posterior distribution as true parameters. It then follows the optimal policy for the sampled model for the rest of the episode. We establish the Bayesian regret bound $\\tilde {\\mathcal{O}}(\\sqrt{T})$ for TSEETC, where $T$ is the time horizon. This is the first bound that is close to the lower bound of restless bandits, especially in an unobserved state setting. We show through simulations that TSEETC outperforms existing algorithms in regret.",
        "reference": "This paper focuses on solving online restless bandits with unknown parameter and unobservable states by the proposed algorithm TSEETC. A Bayesian regret bound with O(T) dependency is established, which matches the lower bound dependency on T and improves the existing O(T2/3) bound derived for a related problem setting by Zhou et al.  Also, simulations are showing that TSEETC outperforms existing algorithms in regret as proof-of-concept. Strengths  This paper is a solid work providing theoretical understanding for a well defined problem, which has characteristics from both bandit and POMDP and is novel with no existing work addressing the exact same setting.  The O(T) dependency, matching the lower bound dependency on T, is a significant improvement compared to the existing bounds of T2/3. However, I\u2019m not fully convinced by this improved dependency and have concern on the regret\u2019s dependency on S and N, the number of states and arms respectively. See more detailed discuss in weaknesses and concerns.  Weaknesses  Notations are somewhat sloppy: To name a few which cause me the most trouble while reading: Even in the main theorem Theorem 1: the notation A comes out of nowhere, I assume it should be the number of arms N. In the main algorithm Algorithm 2:   (i) Line 4, gtk(P) and gtk(R) could be misleading. If following Lemma 1, gtk(P) should refer to the posterior of P conditoned on the history up to tk, however it could also mean gtk\u22121+\u03c41(P), which is what I assume the auther is actually referring to. This two interpretation have drastic difference since it depends on whether the data from the exploitation phase is used to update the posterior or not.    (ii) Line 12, it's no clear what are the obtained r\u00af\u03c41 and b\u00af\u03c41, though for this case I can guess them from the context. Some others in the main text like M\u2217, Mk on page 9. Also I came across complete or repeated sentences in the appendix.    Though the paper is written in a well-organized way most of the time, notations coming out of the blue and not rigorous statements in the main algorithm make it confusing for ones who are trying to parse the algorithm and theorem to get some intuitions behind the math. Sloppy notations truly harm the clarity as well as the formality of this paper.   Exponential dependency on S: It feels the exponential dependency SN appering in constant C1 is far from tight, given the markov chain associated with each arm is independent. To compare with, the regret by Zhou et al scales linearly with M, which is the number of hidden states in the common MC shared by all arms. In the restless bandit setting, the complexity should be of M independent MCs with S hidden states rather than one MC with SN hidden states.  Other Concerns  Why T? More Intuition is needed.  I\u2019m not fully convinced by why TSEETC is able to improve regret from T2/3 by zhou, whose algorithm mostly resembles TSEETC, except for using the UCB estimator constructed with the spectral method for HMM. Based on comparing both algorithms and going through the proof sketch, what directly improves the bound is that TSEETC has a longer exploitation phase in each episode and thus there are only T episodes less than T2/3 by Zhou et al. Given both algorithms do not use the on-policy data in exploitation phase (by the way I assume it happens for TSEETC because the notation is not clear), it implies posterior sampling concentrates to the ground truth model parameter better than UCB in terms of sample efficiency. It seems kind of counterintuitive based on the understanding of TS v.s. UCB from classic bandit literature, or the bottleneck of Zhou et al is to the spectral estimator around which the UCB is constructed?  Bayesian regret. This concern relates to the previous one. A common understanding from the classic bandit literature is that UCB and TS based algorithms usually have regrets of the same order, and TS based algorithms have strictly worse regret bounds from a frequentist view. I\u2019d like to know if it\u2019s possible to have a UCB-based algorithm achieving \\sqrt{T} regret.  Computational cost of posteriors. To compute the exact posterior, one has to exhaust all possible state transitions of length \u03c41, which means a total number of passes exponential to \u03c41,  for T episodes. Though \u03c41 would be of a constant order in theory, does this impose a higher computational cost when realizing TSEETC than SEEU in practice? Clarity in writing could be improved. Quality and ovelty have been evaluated in Strength/Weakness/Concern in detail. Overall, this paper has sufficient novelty for the probelm it studies and the results it claims to get, which I may need more evidence/intuitions to be convinced. If the notation could be revised carefully throughout the paper, then the quality of presentation is good. I didn\u2019t check the reproducibility of simulations but I\u2019d like to believe the results are reproducible. Based on my current appreciation of the reget bound which I'm not fully convinced by and the current techinical presetation where misleading/confusing notations appear here and there, I give my recommendation as a borderline/ marginally weak rejection. I'd be more than happy to raise my score if mainly the Weakness 2 and Concern 1 can be addressed and cleared out, with notations being improved in the revision. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 4: The contributions are significant, and do not exist in prior works. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 78,
        "instruction": "Learning to aggregate: A parameterized aggregator to debias aggregation for cross-device federated learning: Federated learning (FL) emerged as a novel machine learning setting that enables collaboratively training deep models on decentralized private data. Due to the heterogeneity (non-iidness) of the decentralized data, FL methods (e.g. FedAvg) suffers from unstable and slow convergence. Recent works explain the non-iid problem in FL as the client drift, and deal with it by enforcing regularization at local updates. However, these works neglect the heterogeneity among different communication rounds: the data of sampled candidates at different communication rounds are also of non-iid distribution, and we term it as period drift, which as well as client drift can lead to aggregation bias that degrade convergence. To deal with it, we propose a novel aggregation strategy, named FedPA, that uses a Parameterized Aggregator, as an alternative of averaging. We frame FedPA within a meta-learning setting, and formulates the aggregator as a meta-learner, to learn to aggregate the model parameters of clients. FedPA can directly learn the aggregation bias and well calibrate and control the direction of aggregated parameters to a better direction towards the optimum. Experiments show that FedPA can achieve competitive performances compared with conventional baselines.",
        "reference": "The paper presents a learnable aggregation scheme in the context of federated learning. The paper achieves this using meta-learning to generalize the parameters of the aggregator with a proxy dataset. The paper identifies 'period drift' in the current federated learning setup and presents the meta-learning-based aggregator as a way to overcome this issue. The paper follows up with experimental results showing increased accuracy for different methods and heterogeneity rates across two datasets. Strengths  The paper identifies a possible source of client drift The paper proposes a novel aggregation scheme.  Weaknesses  The paper does not do enough to discriminate between regular client drift and the so called period drift either theoretically or through experiments. The aggregation strategy uses a proxy dataset which limits use cases. Also, it is very similar to other knowledge distillation-based techniques like FedET[1] and DS-FL[2]. A comparison of performance with these methods should be shown to justify its usefulness. There is no ablation study showing the effect of the data distribution in the proxy data on model performance. The experimental settings are not strong. The datasets and models are too simple. I suggest including results on CIFAR-100 and Stack Overflow datasets.  [1] Cho, Y. J., Manoel, A., Joshi, G., Sim, R., & Dimitriadis, D. (2022). Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning. arXiv preprint arXiv:2204.12703. [2] Itahara, S., Nishio, T., Koda, Y., Morikura, M., & Yamamoto, K. (2020). Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data. arXiv preprint arXiv:2008.06180. Clarity: The paper is not very well written and has some grammatical mistakes. Quality: The paper quality needs to be improved. The axes font in the figures is too small to read and overall the writing needs to be updated. Novelty: The paper has limited novelty. Reproducibility: No code was given. The paper proposes a meta-learning-based aggregation scheme. However, it does not show enough theoretical or experimental justification to highlight the effectiveness of the algorithm. Additionally, the paper lacks enough ablation studies on the different aspects of the algorithm like the data distribution of proxy data, the influence of the size of the aggregator model, etc.  Furthermore, the paper's concept of 'period drift' is not well defined despite being a key motivation of the algorithm. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 1: The contributions are neither significant nor novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 79,
        "instruction": "Deep Reinforcement Learning based Insight Selection Policy: We live in the era of ubiquitous sensing and computing. More and more data is being collected and processed from devices, sensors and systems. This opens up opportunities to discover patterns from these data that could help in gaining better understanding into the source that produces them. This is useful in a wide range of domains, especially in the area of personal health, in which such knowledge could help in allowing users to comprehend their behaviour and indirectly improve their lifestyle. Insight generators are systems that identify such patterns and verbalise them in a readable text format, referred to as insights. The selection of insights is done using a scoring algorithm which aims at optimizing this process based on multiple objectives, e.g., factual correctness, usefulness and interestingness of insights. In this paper, we propose a novel Reinforcement Learning (RL) framework for insight selection where the scoring model is trained by user feedback on interestingness and their lifestyle quality estimates. With the use of highly reusable and simple principles of automatic user simulation based on real data, we demonstrate in this preliminary study that the RL solution may improve the selection of insights towards multiple pre-defined objectives.",
        "reference": "This work provides a reinforcement learning solution for the insight selection problem and use two experiments to verify the feasibility of the proposed framework. The main claimed contribution is that the framework can provide insights that are both relevant to user preferences and improve users' healthcare. Preliminary experimental result on the American Time Use Survey 2003-2020 shows that the proposed RL solution outperforms insights from multiple pre-defined objectives. Strengths:  It is an interesting idea to generate and select insights using the reinforcement learning diagram.  The paper is well organized, with a good hierarchical structure and clear chapter headings.  Weaknesses:  This paper concentrates on two kinds of insights, insights that are appreciated by the user and insights that are beneficial to their life quality; what is the relationship between them? The article does not clearly explain. Since the American Time Use Survey (ATUS) 2003-2020 dataset exists, can the supervised learning method and the reinforcement learning framework proposed in this paper be compared with the experimental results? If it is difficult to compare with a supervised learning framework, at least compare with a framework that is also modeled by MDP, such as works in the survey by Afsar et al. (2021). In this paper, real life is modeled in the offline dataset ATUS by state machines. If a state outside of the dataset emerges in a real-world application, how does the insight selection network make decisions? Some writing errors, such as \" Those subjects have been selected to tell to the policy network about what measurement is the insight, if it compares it to the benchmark value and to which day of the week it refers to.\", and \"On figure 6 is presented the behavior of the policy network after 12.000.000 steps of training.\" and so on. The paper is well-writhing and motivated. The main contribution is the use of RL algorithm in the insight selection problem. The article provides a reinforcement learning solution to the insight selection problem, and it would be nice to have a more detailed experimental comparisons, analyses, and discussion to highlight the technical contribution of this work besides only using the RL algorithm in a new task. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 80,
        "instruction": "Data Leakage in Tabular Federated Learning: While federated learning (FL) promises to preserve privacy in distributed training of deep learning models, recent work in the image and NLP domains showed that training updates leak private data of participating clients. At the same time, most high-stakes applications of FL (e.g., legal and financial) use tabular data. Compared to the NLP and image domains, reconstruction of tabular data poses several unique challenges: (i) categorical features introduce a significantly more difficult mixed discrete-continuous optimization problem, (ii) the mix of categorical and continuous features causes high variance in the final reconstructions, and (iii) structured data makes it difficult for the adversary to judge reconstruction quality. In this work, we tackle these challenges and propose the first comprehensive reconstruction attack on tabular data, called TabLeak. TabLeak is based on three key ingredients: (i) a softmax structural prior, implicitly converting the mixed discrete-continuous optimization problem into an easier fully continuous one, (ii) a way to reduce the variance of our reconstructions through a pooled ensembling scheme exploiting the structure of tabular data, and (iii) an entropy measure which  can successfully assess reconstruction quality. Our experimental evaluation demonstrates the effectiveness of TabLeak, reaching a state-of-the-art on four popular tabular datasets. For instance, on the Adult dataset, we improve attack accuracy by 10% compared to the baseline on the practically relevant batch size of 32 and further obtain non-trivial reconstructions for batch sizes as large as 128. Our findings are important as they show that performing FL on tabular data, which often poses high privacy risks, is highly vulnerable.",
        "reference": "This paper considers the data leakage attack in federated learning and focuses on the tabular data. A new method called TabLeak is proposed, which consists of three ingradients: (Section 3.1) softmax structural prios; (Section 3.2) pooled ensembling; and (Section 3.3) entropy-based uncertainty estimation. The combined attack is given in Section 3.4. Numerical experiments is presented in Section 4 and conclusions are drawn in Section 5. Strength: This paper considers a very interesting topic.  Weakness:  First, the solutions to tackle the challenge brought by tabular data are quite standard.  Second, this paper is not well written. Certain sentences are very obscure and it is hard to understand what the authors are trying to say.  Also, the grammars are not correct in certain sentences. One example is the sentence following Equation (2). Moreover, in this paragraph, the tense should be consistent. The present tense is used in the beginning of the paragraph, however is switched to past tense for unknown reasons. Although this might be correct in grammar, it is very annoying to the readers. This paper is not well written and it is hard to understand its content. Also, the methods in this paper are quite standard and do not show much novelty. Please see my comments above. Although this paper may contain some intellectual merit, the poor presentation makes it hard for readers to appreciate it. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 81,
        "instruction": "Long-horizon video prediction using a dynamic latent hierarchy: The task of video prediction and generation is known to be notoriously difficult, with the research in this area largely limited to short-term predictions. Though plagued with noise and stochasticity, videos consist of features that are organised in a spatiotemporal hierarchy, different features possessing different temporal dynamics. In this paper, we introduce Dynamic Latent Hierarchy (DLH) -- a deep hierarchical latent model that represents videos as a hierarchy of latent states that evolve over separate and fluid timescales. Each latent state is a mixture distribution with two components, representing the immediate past and the predicted future, causing the model to learn transitions only between sufficiently dissimilar states, while clustering temporally persistent states closer together. Using this unique property, DLH naturally discovers the spatiotemporal structure of a dataset and learns disentangled representations across its hierarchy. We hypothesise that this simplifies the task of modeling temporal dynamics of a video, improves the learning of long-term dependencies, and reduces error accumulation. As evidence, we demonstrate that DLH outperforms state-of-the-art benchmarks in video prediction, is able to better represent stochasticity, as well as to dynamically adjust its hierarchical and temporal structure. Our paper shows, among other things, how progress in representation learning can translate into progress in prediction tasks.",
        "reference": "The paper presents a method for hierarchical representation learning of spatiotemporal features in long-term video prediction. The proposed method is called: Dynamic Latent Hierarchy (DLH). The method distinguishes between features that are changing and those that are not changing in the video sequence. DLH is able to handle multiple objects moving at different speeds and differentiate moving objects from a static environment. The advantages of DLH include: long-term video prediction, improved modeling of stochasticity, and dynamic, efficient latent structure. DLH outperforms baseline approaches on Moving MNIST, KTH Action, and DML Mazes datasets. Strengths:  I really enjoyed reading this paper. It is well written and the ideas are easy to follow.  The problem is well-motivated and the literature review does a good job at contextualizing the paper in prior work. The methods section was presented with intuition for the design choices and notation. This intuition was then supported empirically. An attempt was made to interpret the hierarchical levels semantically in the experiments. Given the considered datasets, the experiments section is constructed thoughtfully and the results are convincing. The figures are informative and effectively illustrate the benefits of the proposed approach. Particularly, the intuition provided by the results in Fig. 5 was helpful in understanding how the method works.  Weaknesses:  Overall, the datasets considered are fairly uncluttered and simplistic. The video examples do not highlight the capabilities of DLH to handle many objects at many different speeds (e.g., in crowded urban scenes). The data also does not showcase what happens when the background is not stationary and there are moving objects. I would recommend considering a more complicated, dynamic dataset, for example, from the autonomous driving setting (i.e., Waymo Open Dataset [1], NuScenes [2], or KITTI [3]). The simplicity of the toy Moving Ball dataset is also underscored by the results of Table 2, where the full capacity of the hierarchical model is not necessary to model the data. Although it is great to see that the model can dynamically adapt to use less of the latent space when the underlying data distribution is simpler, it would be compelling to see how the full latent space would be used in a more complex setting. Although there is a discussion on the importance of stochasticity, this capability is only explored in a toy-dataset setting with random color changes. No discussion of multimodality in the distribution is included. Could DHL handle multimodal outputs (e.g., multiple equally valid possibilities for the future)? This would again be relevant to more complex video datasets (e.g., in the urban setting), where given observations of a person walking straight, they could choose to continue walking straight or turn in the future. It would be helpful in Fig. 4 or in an appendix to show the outputs of the baseline approaches for comparison. In Fig. 5, it is not entirely clear that levels 1 and 2 for the KTH Action dataset are disentangled. It would be interesting to benchmark against a deterministic video prediction method in Table 1 to see if the considered datasets are sufficiently stochastic to warrant modeling of stochasticity. Is there a way to report a measure of statistic significance of the proposed method's metric performance over the baselines in Table 1? In Fig. 6, it is not very clear to me what is wrong with some of the highlighted frames output by the CW-VAE. Is the issue that the ball reduces in size for those frames? Why were the other baseline results not shown? Further explanation for the values in Table 3 would be helpful.  [1] Sun, Pei, et al. \"Scalability in perception for autonomous driving: Waymo open dataset.\" CVPR, 2020. [2] Caesar, Holger, et al. \"nuScenes: A multimodal dataset for autonomous driving.\" CVPR, 2020. [3] Geiger, Andreas, et al. \"Vision meets robotics: The KITTI dataset.\" IJRR, 2013. Some typos and minor points of confusion are listed below:  I am not sure I fully followed the diagrams in Fig. 2. Are there temporal indices missing from the states? Are there hierarchical levels and \u03c8 parameters missing in the 'Estimating' paragraphs in Sec. 2.2 and similarly dropped indexes in p(e\u2223s) in the paragraph before Eq. 3? In Eq. 3, \u039b is not defined. Missing period at the end of Eq. 3. In Sec. 2.3, I am having some trouble understanding the notation. Should q(en+1\u2223en=0)=0 be q(en+1=1\u2223en=0)=0? It would be helpful to derive Eq. 8 from Eq. 7 in the appendix for completeness. Fig. 3 was referenced much earlier than it appears. Am I correct in understanding that at the first hierarchical level e1=1 always? In Fig. 4, it should be made clear whether the 30 past context frames are included in the visualization or only the 100 predicted frames are shown. In Sec. 3, temporal abstraction paragraph, the sentence \"Temporal abstraction models ...\" has a grammatical typo, is a bit long, and is missing a period at the end. Missing periods at the end of table captions. Unclear what the * symbol signifies. In Sec. 4.3, I did not fully understand what it means that \"DLH learns transition between progressively slower features in the higher levels of its hierarchy\". Does this mean that minor variations in the scene are faster features than location changes of the view? The references should be proofread (e.g., to ensure the year is not entered twice in a citation, the conference venue is listed instead of ArXiv when available, the confererence name formatting is consistent, etc.). The overall presentation and quality of the paper was quite high. Although the method builds on components from existing approaches, the dynamic hierarchical architecture for video prediction appears to be novel. Aside from the points of clarification listed above, I found the paper to be clear. There does not appear to be code provided in the supplemental material, which hurts reproducibility. Overall, this is a good, clearly written paper that proposes a reasonable approach for video prediction that outperforms the considered baselines across several datasets. I am currently inclined to accept it. I encourage the authors address the first two weaknesses listed above, in particular, regarding the complexity of the datasets used for evaluation. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 82,
        "instruction": "SwinZS3: Zero-Shot Semantic Segmentation with a Swin Transformer: Zero-shot semantic segmentation (ZS3) aims at learning to classify the never-seen classes with zero training samples. Convolutional neural networks (CNNs) have recently achieved great success in this task. However, their limited attention ability constraints existing network architectures to reason based on word embeddings. In this light of the recent successes achieved by Swin Transformers, we propose SwinZS3, a new framework exploiting the visual embeddings and semantic embeddings on joint embedding space. The SwinZS3 combines a transformer image encoder with a language encoder. The image encoder is trained by pixel-text score maps using the dense language-guided semantic prototypes which are computed by the language encoder. This allows the SwinZS3 could recognize the unseen classes at test time without retraining. We experiment with our method on the  ZS3 standard benchmarks (PASCAL VOC and PASCAL Context) and the results demonstrate the effectiveness of our method by showing the state-of-art performance.",
        "reference": "The paper proposes a transformer based approach for zero shot semantic segmentation. It makes the use of different loss functions like cross entropy loss for seen classes, regression loss between language and visual features to account for unseen classes, a pixel text score map to reduce the seen bias problem and a semantic consistency loss to transfer the relationship of word2vec features to the semantic prototypes of the embedding space. State of the art results are shown on Pascal VOC and Context datasets. The method obtains reasonable improvements over good baselines and state of the art results.  The use of pixel text score map is interesting.  No ablation experiments for Lsc and Lr, which are described as sub-sections of the technical contribution in the approach.  The paper is not written well and has several errors. Introduction:  WSSS are often based on easily obtaining annotations, such as scribbles => sentence seems incorrect We argue that a shared shortcoming of previous ZS3 models falls in the reduced receptive field of CNNs and less uses attention mechanisms for extracting the global relations of visual features conditioned with language semantic information. => 'less uses' does not seem right Current networkBaek et al. (2021) adopt traditional => space between network and Baek. Sentence also is not correct Although generative methods achieve impressive performance in zero-shot semantic segmentation tasks.The methods are limited by a multi-stage training strategy, Remove although or replace '.' with ',' The lsc is proposed by Baek et al. (2021), which define the relation between prototypes as follows: => language does not seem correct The overall loss is finally formulated as L = Lce + Lr + \u03bb1Lsc + \u03bb2Laux, what is Laux in equation 2, I only see Lps in equation 2 The paper has many errors and has a few missing ablation experiments. The method does not seem very novel as it mixes a combination of loss functions (mostly known) with transformers to improve results marginally over baselines. Overall the contributions are borderline and near the acceptance threshold if some concerns are alleviated.  Updated rating after reading other reviews. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 83,
        "instruction": "Proper Scoring Rules for Survival Analysis: Survival analysis is the problem of estimating probability distributions for future events, which can be seen as a problem in uncertainty quantification. Although there are fundamental theories on strictly proper scoring rules for uncertainty quantification, little is known about those for survival analysis. In this paper, we investigate extensions of four major strictly proper scoring rules for survival analysis. Through the extensions, we discuss and clarify the assumptions arising from the discretization of the estimation of probability distributions. We also discuss the relationship between the existing algorithms and extended scoring rules, and we propose new algorithms based on our extensions of the scoring rules for survival analysis.\n",
        "reference": "This work is after finding proper scoring rules in survival analysis. They have a parameter vector w whose true specification underpins the proofs their provide for the discussed scoring rules being proper. They approximate the parameter vector w using an EM algorithm which can in turn be plugged into their scoring rule. It is not obvious from the paper how explanatory covariates can be integrated. The parameter vector which is central in the correctness of the deduced proofs is distribution dependent. For each covariate combination, the values for w changes. Therefore, I cannot think of applying what this paper proposes in a survival analysis application in which the sample is not thought to be drawn from a homogeneous population. The experiments are not shedding any light on what I mentioned above. Table 1 has several metrics for which several loss functions have been tried but the author is left guessing what to take out of it. Since this paper is concerned with survival analysis, it would have been better to show that the metrics that this paper proposes to be proper, when minimized, prove to be better than those normally minimized for which such guarantees do not exist. The paper is quite clear. However, given the parameter set w is so central in this paper I would have liked it explained better. The first encounter with the parameter w is after equation 2 and I find the description quite terse. I have doubts about the applicability of this work. It is not obvious to me how this work can be applied to a real-world scenario where explanatory covariates exist and the population from which the data is sampled is not homogeneous. The experiments did not seem to counter my current understanding. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 84,
        "instruction": "Social Network Structure Shapes Innovation: Experience-sharing in RL with SAPIENS: \nThe human cultural repertoire relies on innovation: our ability to continuously explore how existing elements can be combined to create new ones. Innovation is not solitary, it relies on collective accumulation and merging of previous solutions. Machine learning approaches commonly assume that fully connected multi-agent networks are best suited for innovation. However, human laboratory and field studies have shown that hierarchical innovation is more robustly achieved by dynamic social network structures. In dynamic settings, humans oscillate between innovating individually or in small clusters, and then sharing outcomes with others. To our knowledge, the role of multi-agent topology on innovation has not been systematically studied in machine learning. It remains unclear a) which social network topologies are optimal for which innovation tasks, and b) which properties of experience sharing improve multi-level innovation. Here we use a multi-level hierarchical problem setting (WordCraft), with three different innovation tasks. We systematically design networks of DQNs sharing experiences from their replay buffers in varying topologies (fully connected, small world, dynamic, ring). Comparing the level of innovation achieved by different experience-sharing topologies across different tasks shows that, first, consistent with human findings, experience sharing within a dynamic topology achieves the highest level of innovation across tasks. Second, experience sharing is not as helpful when there is a single clear path to innovation. Third, two metrics we propose, conformity and diversity of shared experience, can explain the success of different topologies on different tasks. These contributions can advance our understanding of optimal AI-AI, human-human, and human-AI collaborative networks, inspiring future tools for fostering collective innovation in large organizations.",
        "reference": "The authors present study of the role of multi-agent topology on innovation towards goal of clarifying which social network structures are optimal for which innovation tasks, and which properties of experience sharing improve multi-level innovation. For multi-level hierarchical problem setting (WordCraft), three different innovation tasks were considered. The design networks of DQNs enables sharing experiences from their re- play buffers in varying structures (fully connected, small world, dynamic, ring). The level of innovation achieved by different setting, shows that, first, consistent with human findings, experience sharing within a dynamic structure achieves the highest level of innovation across tasks. Second, experience sharing is not as helpful when there is a single clear path to innovation. For Third, two metrics we propose, conformity and diversity of shared experience, can explain the success of different social network structures on different tasks. SAPIENS experiments show that dynamic topologies of experience sharing are best suited to solve complex innovation tasks both multi-agent network topology and task structure affect the performance of SAPIENS. Based on our experimental results, we can provide general recommendations on which topology to use for which task class.   The single-path task is an instance of a class of tasks with no strong local optima (similarly to long-horizon tasks. results show no benefit of experience sharing  The paper lays out how the various forms of the network interconnect settings considered performed in tasks that are individual (global and local optima same) or group, such as merging-path task. These exhibits strong local optima that requires explotation a certain point in order to discover the global optimum   The results also show that topologies with low initial connectivity (such as no-sharing, small world and dynamic) performs best here by improving the exploration of different innovation paths. The dynamic topology shows up as the highest performance, allowing different groups to reach the merging innovation level in non-optimal paths before sharing their experience during visits to other groups to find the optimal one. Finally, the best-of-ten task is an instance of a class of tasks with a large search space, many local optima and a few global ones. The results show that the dynamic topology performs best, allowing different groups to first explore different paths, then spread the optimal solution to other groups once discovered. Below are some of the improvements I will like to suggest  why 20 trials (referred in section 3) was deemed sufficient in dynamic network is perf. best because you already ran over the combinations many times and chose best interconnect (app a.3 has some details but unclear which one was used) how many steps were in each trial not indicated If first is the case then how does the conclusion follows (fig 4 sec 3.1): merging paths task the performance of the dynamic structure is significantly better than all other baselines except for no-sharing (p-value 0.22) and small-world (p-value 0.07)\u2026This indicates that, while learners that do not share experiences manage to solve the task with relative success, learners that share experiences under social networks combining large clustering and small shortest path perform best. Also, why the group diversity changes from Fig 5 to Fig 6 for singlepath task Minor: Figure 2 top row the merged path see first element as 5 rewards written in text but shows up as 8 in figure The work seems original and quality/novelty is adequate SAPIENS experiments show that dynamic topologies of experience sharing are best suited to solve complex innovation tasks both multi-agent network topology and task structure affect the performance of SAPIENS. Based on our experimental results, we can provide general recommendations on which topology to use for which task class.   The single-path task is an instance of a class of tasks with no strong local optima (similarly to long-horizon tasks. results show no benefit of experience sharing  The paper lays out how the various forms of the network interconnect settings considered performed in tasks that are individual (global and local optima same) or group, such as merging-path task. These exhibits strong local optima that requires explotation a certain point in order to discover the global optimum   The results also show that topologies with low initial connectivity (such as no-sharing, small world and dynamic) performs best here by improving the exploration of different innovation paths. The dynamic topology shows up as the highest performance, allowing different groups to reach the merging innovation level in non-optimal paths before sharing their experience during visits to other groups to find the optimal one. Finally, the best-of-ten task is an instance of a class of tasks with a large search space, many local optima and a few global ones. The results show that the dynamic topology performs best, allowing different groups to first explore different paths, then spread the optimal solution to other groups once discovered. Below are some of the improvements I will like to suggest  why 20 trials (referred in section 3) was deemed sufficient in dynamic network is perf. best because you already ran over the combinations many times and chose best interconnect (app a.3 has some details but unclear which one was used) how many steps were in each trial not indicated If first is the case then how does the conclusion follows (fig 4 sec 3.1): merging paths task the performance of the dynamic structure is significantly better than all other baselines except for no-sharing (p-value 0.22) and small-world (p-value 0.07)\u2026This indicates that, while learners that do not share experiences manage to solve the task with relative success, learners that share experiences under social networks combining large clustering and small shortest path perform best. Also, why the group diversity changes from Fig 5 to Fig 6 for singlepath task Minor: Figure 2 top row the merged path see first element as 5 rewards written in text but shows up as 8 in figure 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. 4: The contributions are significant, and do not exist in prior works. NO. 6: marginally above the acceptance threshold 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 85,
        "instruction": "Convergence is Not Enough: Average-Case Performance of No-Regret Learning Dynamics: Learning in games involves two main challenges, even in settings in which agents seek to coordinate: convergence to equilibria and selection of good equilibria. Unfortunately, solving the issue of convergence, which is the focus of state-of-the-art models, conveys little information about the quality of the equilibria that are eventually reached, often none at all. In this paper, we study a class of games in which q-replicator (QRD), a widely-studied class of no-regret learning dynamics that include gradient descent, \u201cstandard\u201d replicator, and log-barrier dynamics as special cases, can be shown to converge pointwise to Nash equilibria. This is the starting point for our main task, which is the mathematically challenging problem of performance. In our main contribution, we quantify both conceptually and experimentally the outcome of optimal learning dynamics via average performance metrics, i.e., metrics that couple the regions of attraction with the quality of each attracting point. We provide an exhaustive comparison between gradient descent and \u201cstandard\u201d replicator in a class of games with severe equilibrium selection problems and empirically extend our results to all dynamics in the QRD class. Our results combine tools from machine learning, game theory, and dynamical systems and provide a framework to initiate the systematic comparison of different optimal learning dynamics in arbitrary games.",
        "reference": "This paper proves pointwise convergence of q-replicator dynamics to NE and corresponding bounds on average price of anarchy, generalizing previous works. Strengths: The results are solid. The motivation and proof ideas are well explained.  Weaknesses: I'm not familiar with this specific topic studied in this paper, however I found it's a follow-up of (and resembles) the EC paper \"Average Case Performance of Replicator Dynamics in Potential Games via Computing Regions of Attraction\" (PP16) which this paper doesn't seem to give enough credit to. The studies of convergence of RD to NE, region of attraction and average price of anarchy already appeared in PP16, yet this paper doesn't have enough discussion on PP16 at all. PP16 were only mentioned and cited a few places in this paper just like other much less relevant papers, which might cause misleading judgement on the novelty of the results. The significance of results seems questionable. Theorem 3.2 is a generalization of PP16 to QRD whose proof idea seems similar. The applications Theorems 4.4/4.6 are limited to the 2X2 case. Some terms are not defined rigorously. For example, xk\u2217 is a point in simplex in Definition 2.1, what do you mean by a best response is contained in a point? This paper is well-written in general. However I believe there should be more discussion on comparison with PP16. I tend to reject due to lack of comparison with PP16 and seemingly incremental results. After discussion with other reviewers, I'm satisfied with added discussion with PP16 and will raise the score accordingly. 4: All of the claims and statements are well-supported and correct. 2: The contributions are only marginally significant or novel. Not applicable NO. 5: marginally below the acceptance threshold 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked. Re:"
    },
    {
        "id": 86,
        "instruction": "Gene finding revisited: improved robustness through structured decoding from learning embeddings: Gene finding is the task of identifying the locations of coding sequences within the vast amount of genetic code contained in the genome. With an ever increasing quantity of raw genome sequences, gene finding is an important avenue towards understanding the genetic information of (novel) organisms, as well as learning shared patterns across evolutionarily diverse species. The current state of the art are graphical models usually trained per organism and requiring manually curated data sets. However, these models lack the flexibility to incorporate deep learning representation learning techniques that have in recent years been transformative in the analysis of protein sequences, and which could potentially help gene finders exploit the growing number of sequenced genomes to expand performance across multiple organisms. Here, we propose a novel approach, combining learned embeddings of raw genetic sequences with exact\ndecoding using a latent conditional random field. We show that the model achieves performance matching the current state of the art, while increasing training robustness, and removing the need for manually fitted length distributions. As language models for DNA improve, this paves the way for more performant cross-organism gene-finders.  ",
        "reference": "The authors tackle the problem of annotating genes in newly-sequenced genomes. They develop a model called GeneDecoder which uses a combination of a CRF, LSTM and dilated convolutional layers. The authors show that the model relearns several properties of genes, including directionality and length distribution. The model achieves similar predictive performance to existing method Augustus. Overall, the problem of annotating genes is important, the method is reasonable and the manuscript is understandable. The methodological novelty is low, as the method is a combination of standard CRF, LSTM and dilated CNN models. The experimental setup is flawed and the results are poor.  I couldn't tell what the model used by GeneDecoder is. It involves a neural network, whose architecture is described in A.4. However it also uses a Latent CRF. I think the NN outputs a representation which forms the input to the CRF? This isn't actually stated. Benchmarking gene prediction tools is challenging because many of the annotations within gene databases (e.g. GENCODE) are derived from computational predictors. Thus it can be hard to tell whether a predictor is good at discovering real biology or simply recapitulating the errors made by previous predictors. There exists a benchmark for this task, G3PO (Scalzitti et al 2020), cited by the authors.  Unfortunately, the authors did not use this benchmark and instead used an ad-hoc strategy with many issues (see below). The authors compared to pretrained versions of existing models, so differences in performance could result from differing training sets. When the authors compared against Augustus using a training set similar to Augustus's, results were similar or worse. The authors trained and tested on genes of the same species. This not a proper simulation of the target application, in which a new species is sequenced and its genome must be annotated from scratch.  In splitting genes into train and test sets, the authors ensure that all isoforms of the same gene are placed in the same set. However, since many genes overlap, the same sequences likely appear in both train and test sets. This pitfall is described in detail in the following paper. A better strategy would be to split train and test by chromosome (or by species; see previous note).  https://pubmed.ncbi.nlm.nih.gov/34837041/ See above. See above. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 1: The contributions are neither significant nor novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 1: strong reject 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 87,
        "instruction": "Detecting Out-of-Distribution Data with Semi-supervised Graph \u201cFeature\" Networks: Anomalous and out-of-distribution (OOD) data present a significant challenge to the robustness of decisions taken by deep neural networks, with myriad real-world consequences. State-of-the-art OOD detection techniques use embeddings learned by large pre-trained transformers. We demonstrate that graph structures and topological properties can be leveraged to detect both far-OOD and near-OOD data reliably, simply by characterising each data point (image) as a network of related features (visual concepts). Furthermore, we facilitate human-in-the-loop machine learning by expressing this data to comprise high-level domain-specific concepts. We obtained \\textit{97.95\\% AUROC} on far-OOD and \\textit{98.79\\% AUROC} on near-OOD detection tasks based on the LSUN dataset (comparable to the performance of state-of-the-art techniques).",
        "reference": "In this paper authors propose a mechanism for deriving low-dimensional representations suitable for effective use of established non-parametric and parametric out-of-distribution data detection methods. Specifically they utilize graphs which represent relationships among the objects detected in an image. They claim that this low-dimensional representation mechanism mimics human cognitive processes and is better suited to detect novelty. Strengths The paper clearly describes the intuitions and designs a feature extraction technique based on these.  It performs extensive comparisons using a variety of projection techniques to derive embeddings from the graph representation. Weaknesses The paper uses weakly defined terms like \"Common sense\". Defining these concepts in a more rigorous way would help the reader. The authors summarize their work in the introduction. However it is not straightforward to relate this summary to the actual implementation. It would be helpful if they better establish this relationship with the rest of the paper  e.g. \"... Commonsense service that learns from experience\"   By common-sense service are they alluding to their object detection models and relationship generator ? Is this \"experience\" their training data or they saying that they are creating a continuous learning system, which they aren't in this particular paper. \"computational models that mimic child cognition\" A strong reference which establishes the relationship with child cognition is necessary here. The idea is intuitive and the authors claims are plausible, however the standard of this conference requires more rigorous demonstration of such claims. The paper utilizes the performance reported in a reference to claim it is close to SoTA techniques. Typically such claims are validated by reproduction of the SoTA techniques for controlled comparisons. One of the critical assumptions here is the accuracy of the object detection model. Though the authors have clarified that embedding extraction methods are susceptible to generate incorrect representations for out-of-domain data, they haven't provided an analysis of how the object detectors accuracy impacts the current model. This is a critical detail as OOD data affects all models. There are quite a few grammatical mistakes in the paper which make it difficult to understand. The paper presents an interesting idea, however more clear representation of the critical details of the paper would be very helpful to the reader e.g. A more detailed description of the feature extraction procedure, including an example which guides the reader through image->object detection->graph creation steps using actual data would be very helpful. Figure. 1 in its current form looks representative but it is not clear if it qualifies as an actual example. Novelty of the paper lies in deriving visual semantic structures from images. It employs existing feature extraction techniques to convert these structures into representations suitable for training OOD detection models. The paper utilizes an open dataset for experimental results, but the algorithm has not been described in detail sufficient for reproduction of the results. The paper translates an intuitive representation mechanism into a feature extraction technique. It demonstrates that this technique is highly competitive. However there are a few changes which could further strengthen the paper and make it more accessible to the reader. These include   more detailed description of the feature extraction procedure with a real example stronger experimental validation by comparing to other low dimension projection techniques which can be utilized with OOD detection algorithms more rigorous/well referenced and less hand-wavy descriptions of the motivation/intuition 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 88,
        "instruction": "Let Offline RL Flow: Training Conservative Agents in the Latent Space of Normalizing Flow: Offline reinforcement learning aims to train a policy on a pre-recorded and fixed dataset without any additional environment interactions. There are two major challenges in this setting: (1) extrapolation error caused by approximating the value of state-action pairs not well-covered by the training data and (2) distributional shift between behavior and inference policies. One way to tackle these problems is to induce conservatism - i.e., keeping the learned policies closer to the behavioral ones. To achieve this, we build upon recent works on learning policies in latent action spaces and use a special form of normalizing flow for constructing a generative model, which we use as a conservative action encoder. This normalizing flow action encoder is pre-trained in a supervised manner on the offline dataset, and then an additional policy model - controller in the latent space - is trained via reinforcement learning. This approach avoids querying actions outside of the training dataset and therefore does not require additional regularization for out-of-dataset actions. We evaluate our method on various locomotion and navigation tasks, demonstrating that our approach outperforms recently proposed algorithms with generative action models on a large portion of datasets.",
        "reference": "Offline RL is challenging when the learned RL policy drifts too far from the support of the dataset. Thus, many offline RL methods use some form of constrained or conservative policy update to ensure that the RL policy remains close to the behavior policy of the dataset. One method for doing this is to train a generative model (e.g. a VAE) on the data, and learn an RL policy that picks embedding vectors which are decoded into actions by the VAE decoder. This paper proposes a slight tweak on that prior approach: rather than learning a VAE on the dataset, what if we learned a model using normalizing flows? Using this Conservative Normalizing Flows approach, the authors transform the prior into a Uniform distribution rather than a Normal distribution, which ensures that the RL policy can choose samples anywhere within the support of the Uniform distribution without generated OOD samples, unlike with the VAE. The results show some improvement over existing methods on a sample of tasks from the D4RL offline RL benchmark. A strength of the paper is that it precisely targets a clear hypothesis: whether NF can improve the ability for an offline RL agent to learn to control a pretrained model vs. a VAE. The motivation for the paper is clear, as is the distinction from prior work. A weakness of the paper is its significance, or impact. It makes a relatively minor tweak to the approach proposed by Zhou et al. 2020, and while the results demonstrate gains on 67% of environments sampled for the paper (8/12), it also leads to dramatically worse performance in some environments. Thus, the novelty and effectiveness of the approach is limited.  One possible avenue for future work that the authors could consider in order to increase the significance of their results, is to think about applying this approach to learning to control large language models with RL. Currently, it's difficult to train LLMs with RL due to the complexity of LLM infrastructure and training with RL on such a large scale (i.e. fine-tuning all the parameters could be prohibitively expensive). One approach would be to impose a bottleneck embedding layer within an LLM (see e.g. https://arxiv.org/abs/2209.06792), and then train an RL agent to pick embedding vectors to optimize some language objective. The issue is that this can be quite difficult to get working. The linked vec2text paper suggests that using a VAE doesn't work well in this context. So perhaps normalizing flows could be useful there as well. Clarity: For the most part, the paper is written clearly. In particular, the abstract and intro motivate the problem well, and clearly outline the paper's contributions.   Figure 1 nicely explains both the approach and its difference from prior work.  The toy example in Figure 3 helps build intuition for the problem.  However, the paper becomes less clear in the methods section. More details for Section 3.3 would be helpful. For example, providing an explanation of algorithm 2 in the text would be more clear. There is far too much detail of low-level hyperparameter settings in Section 4.1 that could be moved to the Appendix, in order to spend more time in Section 3 explaining the methodological choices and the actual algorithm. Other issues with the methods clarity:  The ordering of presentation of the equations seems unclear. A critic is needed to estimate Eq 7 but hasn't been introduced yet Why use two critic networks as in Fujimoto? This should be motivated better.  There appears to be a typo in step 7 of algorithm 2, where it takes A(A(s,a)). What is the advantage of the advantage?  Minor clarity issues:  \"Supervisely\" pre-trained models in Figure 1 Why use the normalizing flow conditioning scheme from PARROT? Why do you hypothesize it will be useful here / how is it relevant?  Quality:  This paper criticizes prior work for having to manually bound the RL agent's action space with the VAE approach, but also takes the approach of manually bounding the agent's actions to be within the (-1,+1) interval of support of the uniform distribution. So it would be better to tone down or rephrase those claims. For example, the related work talks about how CNF \"makes agents conservative by design, rather than squeezing policy outputs\". But in effect you are squeezing policy outputs also.  It appears the paper tests on a subset of the D4RL environments. Why were these chosen vs. others? This should be justified better.  As mentioned above, according to the results CNF helps in 67% of environments, but it hurts badly in others. Are these results of significant interest to the community?  The ablation studies such as those of Figure 6 are a great thing to include and help illustrate why the approach is useful. However, the top paragraph of p. 8 states that the ablation using a Normal latent space was carried out with the best hyperparameters found for the Uniform latent space. So how do we know that the difference in performance can't be attributed to doing better hyperparameter tuning for the proposed approach?  Originality: The idea of pre-training a conditional latent model on data and then using an RL agent to pick embedding vectors was proposed by Zhou et al. (2020). The difference with this work is that while Zhou used a VAE, this work proposed to use normalizing flows. This is a relatively minor change from Zhou et al.  The paper is missing a relevant citation to https://arxiv.org/abs/2010.05848, which is a conservative Offline RL method that minimizes divergence from the behavior policy using KL-control. In summary, the paper clearly and precisely investigates a specific hypothesis related to using normalizing flows rather than a VAE for a particular type of Offline RL approach. It is benchmarked against relevant Offline RL baselines and shows an improvement in 67% of environments, and a strong detriment in others. Relevant ablations are conducted to give intuition as to why the proposed approach is important. The novelty above prior work is somewhat limited. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 89,
        "instruction": "Machine Learning from Explanations: Machine learning needs a huge amount of (labeled) data, as otherwise it might not learn the right model for different sub-populations, or even worse, they might pick up spurious correlations in the training data leading to brittle prediction mechanisms.  Also, for small training datasets, there is a huge variability in the learned models on randomly sampled training datasets, which makes the whole process less reliable.  But, collection of large amount of useful representative data, and training on large datasets, are very costly.  In this paper, we present a technique to train reliable classification models on small datasets, assuming we have access to some simple explanations (e.g., subset of influential input features) on labeled data.  We also propose a novel two stage training pipeline that optimizes the model's output and fine-tunes its attention in an interleaving manner, to help the model to agree with the provided explanation while learning from the data. We show that our training pipeline enables faster convergence to better models, especially when there is a severe class imbalance in the population or spurious features in the training data.",
        "reference": "This paper proposes a method to train more accurate image classification models by leveraging human explanations in the form of input masks to increase test accuracy. While the pitched idea is promising, this work is fundamentally flawed in a number of ways and needs to be completely rethought. Strengths:  The proposed setting of leveraging human explanations in order to increase performance is interesting.  Weaknesses:  The main issue with this work is that the evaluation setup is not realistic at all. For an experimental paper like this, verifying its applicability on real-world datasets is important. Yet, 2 datasets are synthetically generated and only 1 is of real birds. This birds dataset, too, is very simple, in that the feature is very easily identifiable (the beak), and it is not clear if this method scales to more realistic distributions where the features are not as simple. Another huge issue is that experiments are only conducted at the extremely small-sample regime, up to 500 samples on the synthetic datasets of shapes and up to 60 examples on the bird dataset. No one is deploying machine learning trained on 60 samples. If the method was to train on all labeled data, and only incorporate some additional explanations, then that would be much more reasonable. But that is not what is happening here.  Advice:  The idea of leveraging a few human annotations to increase performance is interesting, but the rest of the paper needs to be completely reworked. Here's what a great version of this paper would look like: Consider a suite of real-world datasets, such as those in the WILDS benchmark. Do not include any synthetic data experiments (they add no value) and report performance on the specific metric for each dataset. Another benefit of this is that experiments are run on non-binary tasks as well. Train on all available labeled data. The WILDS dataset contains training data splits. You should compare two main methods primarily: 1) the baseline of training on the labeled data, and 2) the new method of training on the labeled data, plus incorporating input mask explanation annotations for a few (say, 60) examples. Use modern backbone baselines (say, Resnet50 or DenseNet121) for the feature extraction layer - 3 conv layers is definitely too small for anything non-synthetic. I have to say that even given this version of the idea, I am skeptical this would work (lots of such robustness/domain invariance interventions have been proposed and have failed). But this is just my opinion, my advice, and the rest of this review is independent of this viewpoint. The paper makes a number of unsubstantiated claims with exaggerated language throughout. To give an example:  \"we argue that it is necessary to incorporate explanations in learning algorithms, if we aim at using machine learning in real-world scenarios\" \"we need datasets of astronomical sizes\"  These claims are littered all over and need cites or need to be removed. Code seems to be provided (but no README on how to interact with it), so there is partial reproducibility. The paper was often unclear, specifically:  specify that the explanations e(x) are binary masks much before the bottom of page 3. in the network (Figure 1), which layers are frozen and which layers are trained? equation 3 - are you computing the KL divergence between two probability distributions over features induced by the feature maps? In it's current form, equation 3 is dividing 2 tensors and take its log, which mathematically does not make any sense. The experimental results have several major flaws (as denoted above), and for this reason I believe there are significant structural changes that need to be made in order to make this a valuable contribution. 1: The main claims of the paper are incorrect or not at all supported by theory or empirical results. 1: The contributions are neither significant nor novel. 1: The contributions are neither significant nor novel. NO. 1: strong reject 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 90,
        "instruction": "Functional Risk Minimization: In this work, we break the classic assumption of data coming from a single function $f_{\\theta^*}(x)$ followed by some noise in output space $p(y|f_{\\theta^*}(x))$. Instead, we model each data point $(x_i,y_i)$ as coming from its own function $f_{\\theta_i}$. We show that this model subsumes Empirical Risk Minimization for many common loss functions, and provides an avenue for more realistic noise processes. We derive Functional Risk Minimization~(FRM), a general framework for scalable training objectives which results in better performance in small experiments in regression and reinforcement learning. We also show that FRM can be seen as finding the simplest model that memorizes the training data, providing an avenue towards understanding generalization in the over-parameterized regime.",
        "reference": "This paper presents a novel framework for supervised learning problems. First, it introduces functional generative models which represent P(x,y) in terms of a latent variable \u03b8 that is sampled independently of x and then determines y as a function f\u03b8(x). This is contrasted to a standard additive noise model, where y=f\u03b8(x)+\u03f5. Using this noise model, the paper proposes functional risk minimization (FRM) as an objective to find the maximum likelihood parameter \u03b8\u2217 and proposes an approximate algorithm that relies on approximating an integral over all parameters by a local Laplace approximation. Strengths  This paper presents a very interesting and (to my knowledge) novel take on representing noise in a learning framework. This could be highly impactful as it may allow for more easily dealing with the variation in real datasets than standard MLE in additive noise models.   The empirical results on linear regression with non-uniform noise seem to be strong, demonstrating that as the noise distribution changes, FRM begins to outperform ERM.   Weaknesses  It is not clear how closely the implementable algorithm relates to the theoretical presentation and derivations. Equations 7 and 8 seem to be the core algorithmic contribution of the paper, but they are never formally derived and it is highly unclear under what conditions they will actually relate to the population objective.   The implementable algorithm as proposed does not seem to be scalable. In particular, it requires both a significant approximation to integrate over parameters and then uses Hessian information that could be difficult to get for large models. This is born out by the fact that all the experiments, while interesting, are quite small in scale.   There is no formal argument made as to why or when FRM will outperform ERM. In fact, there are no arguments at all showing that (a) the FRM minimizer will achieve low loss, (b) the finite-sample version of FRM approximates the population variant, or (c) the finite-sample FRM minimizer will achieve low loss.      The presentation is confusing.  a. The use of \"ERM\" is non-standard and confusing. ERM, as used in foundational work like [1], simply refers to minimizing an empirical loss as a proxy for an expected loss. There is no mention of any particular assumption about additive noise in the ERM principle. Moreover, the entire reason to use the ERM term is to contrast it with learning objectives like structural risk minimization. This paper instead uses ERM to refer to the noise model induced by ERM with particular loss functions by viewing them as MLE. It would perhaps be more clear to frame the novelty of the approach as replacing MLE under an additive noise model with a novel MLE under the functional latent variable noise model.    b. Similarly, FRM is often used to refer to the noise model, which is in fact the FGM under the nomenclature introduced in the paper. The paper would be much improved by being more clear about the difference between the FGM modeling assumption and the FRM learning objective.  c. The FRM learning objective is never clearly and explicitly layed out. When the authors say FRM, what implementable algorithm are they explicitly referring to? This needs to be more clear.   [1] Vapnik, V. (1991). Principles of risk minimization for learning theory. Advances in neural information processing systems, 4. Minor: There are a few typos throughout the paper. For example, in the abstract \"regraded\" should be \"regarded\". In the \"FGMs encode...\" paragraph on page 4, many spaces are missing after periods. In the Table 1 caption \"Furthermoe\" should be \"furthermore\". Clarity is poor, as explained above. The novelty and potential significance are high. The code is provided in the supplement for reproducibility purposes, but I have not checked it. I think this paper clearly has some interesting and promising ideas. However, as currently presented I cannot support acceptance because of the significant issues connecting the actual algorithm to the motivation, lack of basic theory, and general lack of clarity. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 4: The contributions are significant, and do not exist in prior works. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 91,
        "instruction": "Latent Linear ODEs with Neural Kalman Filtering for Irregular Time Series Forecasting: Over the past four years, models based on Neural Ordinary Differential Equations have become state of the art in the forecasting of irregularly sampled time series. Describing the data-generating process as a dynamical system in continuous time allows predictions at arbitrary time points. However, the numerical integration of Neural ODEs typically comes with a high computational burden or may even fail completely. We propose a novel Neural ODE model that embeds the observations into a latent space with dynamics governed by a linear ODE. Consequently, we do not require any specialized numerical integrator but only an implementation of the matrix exponential readily available in many numerical linear algebra libraries. We also introduce a novel state update component inspired by the classical Kalman filter, which, to our knowledge, makes our model the first Neural ODE variant to explicitly satisfy a specific self-consistency property. It allows forecasting irregularly sampled time series with missing values and comes with some numerical stability guarantees. We evaluate the performance on medical and climate benchmark datasets, where the model outperforms the state of the art by margins up to 30%.",
        "reference": "The authors propose a novel Neural ODE model that embeds the observations into a latent space with dynamics governed by a linear ODE.  They carefully show that the model satisfies self-consistency, which allows forecasting irregularly sampled time series and have some numerical stability guarantees. They evaluate the performance on medical and climate synthetic datasets, where the model outperforms the similar state of the art models. The theoretical guarantees of the model are nicely derived and clearly presented, and Table 2 is a nice summary of the theoretical advantages of the proposed model.  The empirical evaluation is compelling, but is on synthetic benchmark datasets only. Including results on a real dataset would make the paper stronger. Even without ground truth, including some visualization of the learnt dynamics vs true dynamics and other models for real datasets would be a great addition to the paper (for example, modeling ocean dynamics) Figure 1 should also be updated. I think it is of poor quality and doesn't give much insight into the model. Finally, the OBSERVATIONS section (6.2.) is interesting as it shows the Eigen-values of the kernel matrix, and the authors speculate that the learnt dynamics are periodic. There is space to include some true and inferred dynamics to visualize the periodic signal (or maybe include it in the appendix) The paper is very clear and of good quality. The paper is very clear and shows the the method outperforms previous state of the art models. However, I think the paper would gain in quality with better visualization of the learnt dynamics, in synthetic and real datasets. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 92,
        "instruction": "Gradient-Based Transfer Learning: We formulate transfer learning as a meta-learning problem by extending upon the current meta-learning paradigm in that support and query data are drawn from different, but related distributions of tasks. Inspired by the success of Gradient-Based Meta-Learning (GBML), we propose to expand it to the transfer learning setting by constructing a general encoder-decoder architecture that learns a map between functionals of different domains. This is achieved by leveraging on the idea that the task-adapted parameters of a meta-learner can serve as an informative representation of the task itself. We demonstrate the proposed method on regression, prediction of dynamical systems and meta-imitation learning problems.",
        "reference": "This paper proposes to solve support-query distributional shift problem which has not been addressed by the previous meta-learning literatures. Instead of assuming that the same function f is used to sample both support and query set, they assume that different functions f and g are generating each support and query set. And then they propose to learn to map from f to g with simple transformations. The proposed method outperforms the simple baselines over various problems, especially well when the white noises are added. Strength  The paper is well written The problem formulation is novel and important The method is simple and intuitive The proposed method outperforms the simple baselines over various problems The proposed method provides a new way to generalize to a distributional shift  Weaknesses  As already pointed out in the main paper, the dimensionality of both the parameters of the function f (source) and g (target) are high-dimensional, which may prevent the proposed method from being applied to larger scale problems. As far as I understand, the experiments are all small scale due to this reason. I wonder whether there exists no such literatures solving exactly the same problem. It's quite surprising. Maybe there should be some that I'm not aware of. I will defer this point to the discussion phase with other reviewers. It's unclear why the proposed method should outperform with strong noise level. Could you provide some intuition? Although it is nice that the method provides a way to generalize to a distributional shift, it is only for a single specific distributional shift at a time. For instance, in pendulum experiments it is only generalizable to double pendulum, while we would wish to generalize to any number of pendulums. Clarity, quality, and reproducibility are good enough. However, I'm not really sure if this the novelty of this paper is significant. There may be some other literatures solving the same problem, so I will defer it to the discussion phase. In summary, the paper addresses a very important problem of support-query distributional shift that has not been fully addressed by the previous meta-learning literatures. The method is clear, simple, and effective. I thus recommend acceptance, but for the novelty part I will have a discussion with other reviewers. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 93,
        "instruction": "Coreset for Rational Functions: We consider the problem of fitting a rational function $f:\\mathbb{R}\\to\\mathbb{R}$ to a time-series $g:\\{1,\\cdots,n\\}\\to\\mathbb{R}$. This is by minimizing the sum of distances (loss function) $\\ell(f):=\\sum_{i=1}^n |f(i)-g(i)|$, possibly with additional constraints and regularization terms that may depend on $f$. Our main motivation is to approximate such a time-series by a recursive sequence model $F_n=\\sum_{i=1}^k \\theta_i F_{n-i}$, e.g. a Fibonacci sequence, where $\\theta\\in \\mathbb{R}^k$ are the model parameters, and $k\\geq1$ is constant.\nFor $\\varepsilon\\in(0,1)$, an $\\varepsilon$-coreset for this problem is a small data structure that approximates $\\ell(g)$ up to $1\\pm\\varepsilon$ multiplicative factor, for every rational function $g$ of constant degree.\nWe prove that every signal has an $\\varepsilon$-coreset of size $O(n^{0.001}/\\varepsilon^2)$, and provide a construction algorithm that computes it in $O(n^{1.001})$ time.\nOpen source code is provided, as well as extensive experimental results, on both real and synthetic datasets, which compare our method to existing solvers from Scipy.",
        "reference": "This paper studies the problem of building a coreset for fitting rational functions to time series data. In particular, suppose y1,...,yn\u2208R is a time series. Then, in rational function fitting, we want to find a rational function r of degree k such that r(i)\u2248yi. We formally measure error with the \u21131 metric, the sum of absolute errors. This paper isn't as much about solving this problem, as it is about compressing the problem, so we want to find a small data structure such that all rational functions r have their loss with respect to y preserved within a multiplicative (1\u00b1\u03b5) factor. So, in order to approximately solve the full problem on all n time points, it suffices to minimize over rational functions on this smaller coreset. For constant failure probability, the paper builds such a coreset in O~(2O(k2)nO(klog\u2061log\u2061n)) time and it uses O~(1\u03b52poly(k)logO(log\u2061(k))\u2061(n)) space. For constant degree k, this is O(n1+o(1)) time and O(polylog(n)\u03b52) space. The coreset is constructed in two parts:  First, they use a bicriteria-approximation algorithm to partition the time domain into O(log\u2061n) subintervals, and approximately fit a rational function ri to each subinterval. This is an approximation because each ri is an approximate minimizer, and is bicriteria because it returns O(log\u2061(n)) rational functions instead of just one. Then, since this bicriteria approximation is not accurate everyone, a randomized \"sensitivity sampling\" stage explicitly stores the time points where both (i) the bicriteria approximation is inaccurate and (ii) where a rational function could be able interpolate that point.  The loss of an arbitrary rational r function against the full dataset is then approximated by sum of the loss of r on the sampled points and the sum of the distance between r and ri on the subintervals created by the bicriteria approximation. The authors say that most of the technical novelty and effort goes into building the bicriteria approximation. [edit: fixed the block quote in Part 1 to correctly separate my writing from the paper's] This paper gave me some whiplash. It's got some sections written really clearly, and some which are brutally opaque. I wrote a long summary for this paper because it took me a long time to just understand the algorithm and construction of the paper. In the places it's opaque, I'm not comfortable saying that I believe it's correct. Because of concern about correctness, I'm not comfortable accepting this paper. Part 1: Introduction and Motivation The paper starts with an interesting and compelling story about why MSE is a bad loss metric for fitting time series data: often time series models like the \"auto-regressive\" model have variance that grow exponentially, so minimizing the MSE is just fitting the last couple data points. So, we should consider fitting in some other metric that. The authors then define a generating function and seem to fail to give any fundamentally new metric, and end up using \u21131 error instead of MSE (i.e. \u21132) error. I'd like to know what the authors were going for, because minimizing the \u21131 loss would also just fit the last few data points from an exponentially growing time series. Regardless, fitting a time series in \u21131 norm is a perfectly well motivated problem, so it's not a huge problem, but it's certainly confusing. I really wished they stuck the landing and suggested a new way to talk about error in time series data though. This question of error metric is a great example of how Section 1 of the paper is a mix of extremely clear and very confusing writing. Beyond this one example, there's also a few times where the authors just introduce a term from the literature without any explanation. As someone not familiar with this particular term from the prior work, I found the following line pretty funny:  This is due to the three main coreset properties: merge and reduce.  I have no idea why \"merge and reduce\" is three properties, and this is not explained in the introduction. The authors similarly do this with \"(\u03b1,\u03b2)-approximation\", which ends up being super important in this paper, and it was only by digging in the prior work that I figured out that \"(\u03b1,\u03b2)-approximation\" means bicriteria approximation. \"Merge and reduce\", and bicriteria approximation are both heavily used in this paper, and not clearly saying what these intuitively are in the introduction is a huge misstep that really hurts the legibility of the paper. Part 2: Describing how the Algorithms Work Page 5 of this paper is a summary of how they construct the bicriteria approximation and the subsampled dataset that comprise their coreset. This page took me an immense amount of effort to understand what the algorithms intuitively are doing, and I don't have an intuition for why these algorithm construct a near-optimal coreset. The clarity issues from the introduction could be fixed with a minor revision, but the issue of clarity on this page is more brutal. This page is full of high-level technical descriptions of what the various parts of various algorithms do. I'm not going to belabor a huge list of points that confused me, but I will give a demonstrative example. I'll include broader frustrations about this writing in my list of typos & suggested edits. For my example, consider the middle line from the first paragraph in the section \"Algorithm 3: the merge-reduce step\", which reads:  This is by computing the following for every possible subset C\u2286B of size \u03b2\u22126k+3  I can understand that this algorithm is given a set B, and it can search over subsets of size \u03b2\u22126k+3. I have no idea why \u22126k+3 is meaningful, what's it's used for in the analysis, if the numbers 6 and 3 are important, or anything else about it. This whole page is full of instructions whose value is completely lost on me. Part 2 again: The (hidden?) Strengths The frustrating layer under this somewhat brutal writeup, was that as I progressed through page 5 of the paper, I could see there's a careful and clever design under all of this. The details seem precise. When I wanted to understand a claim in a bit more detail, there were easy-to-find formalisms in the appendix. I am emotionally convinced there's a cool result with a neat and careful construction under this all. I just haven't been given a correctness argument that I could review in the body of the paper. I think the authors could reduce the formal specification of the algorithms on page 5. Then, remove the formal algorithm from the body of the paper, and replace it with an informal pseudocode that skips over implementation details and covers more high-level ideas. Then, with the recovered space, justify more clearly why the construction is accurate with high probability. Part 3: Experiments The experiments are decently cool. It's a hard problem with an exponential dependence on the degree of the rational function, so the experiments are just stated for degree 2 rational functions. But, for this setting, the experiments are convincingly interesting, and show that the coresets can be pretty efficient for real data (figure 3 on page 9). I've already written a bunch about a lack of clarity leads to a certain lack of quality in this paper. The paper definitely seems novel and significant enough for publication though. The results as written are compelling. I devote the rest of this space to a list of typos. Typos  [Page 1, background paragraph] \"the references therein\" instead of \"reference therein\" [Page 2, section 1.2 first paragraph] Say \"Informally, given an input signal of d-dimensional points\" instead. Also, why not just say 2-dimensional instead of d-dimensional? [Page 2, section 1.2 last paragraph] If there's no coreset which is a weighted subset, then it makes sense to use a bicriteria approximation in addition. But, why restrict yourself to consequtive integers in the first dimension then? There's nothing wrong with assuming that to be the input data given to you, but the flow of this paragraph's logic / justification doesn't quiet stand up. [Figure 4] Not a big deal, but this might read a bit more clearly if we also had an error plot, showing if any approximations are especially accurate or innacurate, and where those accuracies/innaccuracies are. [Throughout the paper] Feldman & Langberg 2011a and 2011b are the same paper. This is cited a lot, so it's helpful to realize they're the same paper. [Page 3, section 1.4 last paragraph] Say \"believe\" instead of \"expect\", or some other weaker language. This sentence comes off kinda weird with \"expect\" there. [Page 3, section 1.4 last paragraph] I have no idea what the \"have few dependent recursive functions ...\" phrase means. No clue what message you're trying to send with this sentence. [Page 3, section 2.1 first line] I'd probably say \"set of k-dimensional\" instead of \"union of k-dimensional\". Not sure what's getting union'ed here. [Page 4, section 2.1 just before definition 2] \"Projection of f onto P\" makes a lot more sense than the \"Projection of P onto f\". f exists where P doesn't, so we project f onto P by evaluating f on that interval. [Page 4, definition 2] Why define a rational function this way? Why not just leave it be a ratio of two arbitrary polynomials. Justify this in text. [Page 4, paragraph after definition 2] Remove the comma after \"Definition 2\" [Page 4, paragraph after definition 2] \"approximation as Feldmen & Langberg (2011) defined\" [Page 4, paragraph after definition 2] Expliciltly site section 4.2 of Feldmen & Langberg (2011) and explicitly use the language \"bicriteria approximation\" [Page 4, definition 3] Consider using the language \"consecutive intervals\" instead of \"consecutive sets\" [Page 4, definition 3] Missing subscript on P1 [Page 5, first line] Use the standardized notation no(1)/\u03b52 instead of this n\u03d5 for any phi notation. It's standard in TCS [Page 5, first line] If it's with probability 1\u2212\u03b4, then \u03b4 should appear in the space complexity [Page 5, first line] Remove \"; see Definition 4\", that definition was 2 lines ago. [Page 5, first paragraph] It's algorithm 2, not algorithm 1 [Page 5, first paragraph] I'd say something like \"probability of placing a point p\u2208P into the samplet set C\" [Page 5, first paragraph] Write out the formal probability proportionality, so it's more clear. It's quick to state something like Pr[picking\u00a0pi]\u221dD(qi,pi). [Page 5, second paragraph] To clarify the break in the sentence, add \"it has\" after \"\u03b2 child nodes, \" and before \"O(\u03b2i) leaves. [Page 5, third paragraph] Don't use C here. C already means the subsampled points returned in the coreset. [Page 5, third paragraph] Don't change the meaning of i from the second paragraph to the third. Keep i as the level, and let j denote an element Bj\u2208B. [Page 5, third paragraph] The (0,ri) notation is really unclear. As written, I think means that Bi has ri subintervals which exactly interpolate the given dataset y, which I don't think is the case. [Page 5, fifth paragraph] Lemma 8 isn't anything that the reader knows. It's burried in the appendix. Say somethign like \"this can be solved using <brute force / greedy search / whatever>, with details shown in Lemma 8\". [Page 5, fifth paragraph] As far as I understand, the input signals C are arbitrary subsets of B1,...,B\u03b2 of size \u03b2\u22126k+3. Not consecutive. How does this mesh together with point (i) from the third paragraph on this page? [Page 5, fifth paragraph] Add \"approximation\" after \"bicriteria\" [Page 5, last paragraph] Add \"time\" after \"and thus the running\" [Page 5, last paragraph] Again, please abandon this \"for all \u03b5>0 notation\". There's (in my opinion) a better notation to say what you're trying to say. [Page 6, \"optimal soloution\" paragraph] The phrase \"that minimizes 2\" -- I've got no idea what 2 is here. There's no equation 2 in the body of the paper. [Page 6, \"efficient (1,\u03b2)\" paragraph] \"relatively fast\" doesn't give me a good intuition. Give me a big-oh notation. [Page 8, list of implimentations] Is one of these analogous to Lemma 8's algorithm? If not, would it be possible to add that to the experiments. If so, could you lable which one that is? [Page 8, section 3.1 results] You don't explain what the error bars are here [Page 8, \"dataset\" paragraph in section 3.2] Missing parentheses on the citations after \"Beijing Air Quality Dataset\" and \"UCI Machine Learning Repository\" [Page 8, section 3.2 results] If you use 25th and 75th quantiles, you should probably use median for the central line instead of an average. Probably won't change the data much, but it's a bit more emotionally consistent. [Page 9, section 3.3 results] Figure 4 shows a brutally hard adversarial edge case for polynomial, but no such example for rationals. It's not fair to point at Runge's phenominon and just say that polynomials are worse at fitting time series than rationals. There's arguments that can be made there, but this is unreasonable. The paper seems probably cool, and I want to like it, but the authors didn't give me good reason to think their claims are correct. As such, I reject the paper. A rewrite of the paper could definitely be published though (supposing the results are correct). 2: Several of the paper\u2019s claims are incorrect or not well-supported. 4: The contributions are significant, and do not exist in prior works. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 3: reject, not good enough 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 94,
        "instruction": "Transformer needs NMDA receptor nonlinearity for long-term memory: The NMDA receptor (NMDAR) in the hippocampus is essential for learning and memory. We find an interesting resemblance between deep models' nonlinear activation function and the NMDAR's nonlinear dynamics. In light of a recent study that compared the transformer architecture to the formation of hippocampal memory, this paper presents new findings that NMDAR-like nonlinearity may be essential for consolidating short-term working memory into long-term reference memory. We design a navigation task assessing these two memory functions and show that manipulating the activation function (i.e., mimicking the Mg$^{2+}$-gating of NMDAR) disrupts long-term memory formation. Our experimental data suggest that the concept of place cells and reference memory may reside in the feed-forward network layer of transformers and that nonlinearity plays a key role in these processes. Our findings propose that the transformer architecture and hippocampal spatial representation resemble by sharing the overlapping concept of NMDAR-like nonlinearity.",
        "reference": "This paper applies the transformer model to spatial navigation problem in a grid world with labeled grid positions. The task is to predict the label of the next position that is either visited or unvisited. The paper connects this task to working memory and reference memory, as well as place cells. The finding is that NMDAR-like nonlinearity in the feedforward block of the transformer model is important for reference memory and neurons behave like place cells. Strengths:  (1) The analogy between the transformer model for prediction and the hippocampus in the brain is interesting, although this analogy has been explored in a recent paper.  (2) The focus on the feedforward block of the transformer model seems novel, and the connection between the GeLU non-linearity and the NMDA receptor (NMDAR) is novel.  (3) The idea and method in this paper is simple and interesting.  Weaknesses:  (1) The paper is entirely empirical. There is no theoretical or mathematical analysis. The empirical similarities between transformer and hippocampus are noteworthy, but some theoretical understanding can greatly improve the paper.  (2) The focus on transformer architecture is understandable given its empirical successes and its popularity, but it may be worthwhile to explore simpler models that may have similar behaviors. The key idea, method and findings of this paper are original.  The quality of empirical study is good.  But the clarity of the presentation can be improved. The main text should provide more details about the transformer architecture used in this paper. Some discussions on its biological plausibility may also help. The paper compares transformer and hippocampus in navigation tasks, and the findings are interesting. However, the paper is too empirical with no theoretical investigation. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 95,
        "instruction": "Simple Spectral Graph Convolution from an Optimization Perspective: Recent studies on SGC, PageRank and S\\textsuperscript{2}GC have demonstrated that several graph diffusion techniques are straightforward, quick, and effective for tasks in the graph domain like node classification. Even though these techniques do not even need labels, they can nevertheless produce more discriminating features than raw attributes for downstream tasks with different classifiers. These methods are data-independent and thus primarily rely on some empirical parameters on polynomial bases (e.g., Monomial and Chebyshev), which ignore the homophily of graphs and the attribute distribution. They are more insensitive to heterophilous graphs due to the low-pass filtering. Although there are many approaches focusing on GNNs based on heterophilous graphs, these approaches are dependent on label information to learn model parameters. In this paper, we study the question: are labels a necessity for GNNs with heterophilous graphs? Based on this question, we propose a framework of self-representation on graphs related to the Least Squares problem. Specifically, we use Generalized Minimum RESidual (GMRES) method, which finds the least squares solution over Krylov subspaces. In theoretical analysis, without label information, we enjoy better features with graph convolution. \nThe proposed method, like previous data-independent methods, is not a deep model and is, therefore, quick, scalable, and simple. We  also show performance guarantees for models on real and synthetic data. On a benchmark of real-world datasets, empirically, our method is competitive with existing deep models for node classification.",
        "reference": "The paper proposes a novel approach for shallow graph representation through combining the idea behind label propagation with Krylov subspace methods. Label propagation is applied to the node features, and then the closed form solution is substituted into a least squares fitting term, which is then reparametrized using the Krylov subspace of order-r. Interpretation is provided as a polynomial approximation problem, and extension using Chebyshev polynomials is proposed. Theoretical analysis is provided in the context of the contextual stochastic block model in support of the approach. Experiments are carried out on synthetic data, and real-world node classification datasets. Strengths:  The formulation in terms of the Krylov subspace is interesting and novel. Improvement on some heterophilous datasets (CHAMELEON and SQUIRREL) Theoretical analysis supports hints at some benefits of the approach on heterophilous datasets  Weaknesses:  The exposition is very convoluted. The motivation behind the core idea seems lost. Minor problems in the algorithm formulation. On most other datasets the approach underperforms.  The exposition is very convoluted. The related work section mentions several approaches, but the discussion provided for each of them is confusing. Although the authors have tried to connect it to the current work, these points are hard to follow. Similarly, the \"Relation to GPR-GNN\" section is also hard to follow, where there is new notation used but not defined. The motivation behind the core idea seems lost. It seems to me that the main idea behind the approach is to substitute the closed form solution of LP into the least squares \"fitting term\". Why this is a good idea is not discussed at the moment. The formulation of the algorithm here seems unmotivated, and there are also minor errors, which further blurs the intuition. I was also confused about the role of the \"Polynomial Approximation with Constraints\" section. The contribution of the first paragraph is tautological. Parametrizing the least squares solution in terms of the Krylov subspace is obviously equivalent to minimizing the residual with respect to all polynomials of order-r over A. The second paragraph, on the other hand, suggests to use Chebyshev polynomials instead, but then at the end the authors say Chebyshev polynomials underperform in GNNs. No results are reported for this setting either. This makes me wonder why the authors chose to include this paragraph. Minor problems in the algorithm formulation. I have found several inconsistencies in the mathematical derivation of LP and the Krylov subspace formulation. First, equation 3) seems incorrect since LP does not use the adjacency matrix A in the update rule, but its normalized counterpart D\u22121/2AD\u22121/2. This follows from the Dirichlet energy term containing the division by Dii\u22121/2 in equation 1). Further, in equation 4) the second equality is not true. In general, minimizing the quantity on the rightmost hand side with respect to w is not equivalent to minimizing the one on the middle with respect to \u03b1. In particular, the minimum will be lower. Afterwards, the authors state that wi=(1\u2212\u03b1)\u03b1i. This contradicts the previous line, where optimization over w\u2208Rn was denoted to be unconstrained. Overall, there are some inconsistencies here, which make me unsure about the actual algorithm the authors use. On most other datasets the approach underperforms On the homophilous datasets, it seems like the approach is outperformed by both shallow and deep baselines. On the heterophilous datasets, it outperforms all baselines on 2 datasets, while for the other 3, it performs on par with raw logistic regression  (i.e. without connectivity information) and outperformed by deep baselines. Overall, these results are not convincing enough for me. It is not investigated where the improvements come from on the 2/5 datasets. Training time comparison against shallow/deep baselines is not reported. Clarity: Writing is mostly clear, but the exposition is hard to follow. Inconsistencies in the algorithm formulation. Novelty: The idea itself seems novel to me. Reproducibility: I did not find any provided supplementary material or code. The paper combines label propagation with optimization over Krylov subspaces. The core idea seems interesting, but the point seems lost in the convoluted exposition, technical inconsistencies in the derivation. The presentation could be improved to help better convey the idea. Experimental results only show strong performance on 2 out of 5 heterophilous datasets, while on the other 3 it performs the same as raw logistic regression, hence the feature propagation effectively having no effect. On homophilous datasets mostly weaker performance compared to baselines. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 96,
        "instruction": "Rethinking the Value of Prompt Learning for Vision-Language Models: Large-scale visual-language pre-training like CLIP has demonstrated great success in open-set visual concept learning that enables zero-shot transfer to downstream tasks through prompting. To automate prompt engineering, prompt learning is proposed to automatically learn the optimal task-relevant prompts. In this paper, we make some surprising observations that contradict common beliefs about prompts. We observe that even random prompts can achieve pretty good performance for zero-shot recognition. We also find that prompt learning gives comparable or worse performance than directly fine-tuning of the linear classifier. Moreover, prompt learning is no more than parameter-efficient learning, and is a trade-off between optimality and generalization. Our results highlight the need for the rethinking of existing prompt learning, more careful baseline evaluations in future research on prompt learning methods in vision-language models. ",
        "reference": "In this work, prompt learning is reexamined, and several unexpected findings that defy accepted notions of the prompt are presented. First , random prompts without learning or fine-grained design may likewise function effectively in zero-shot recognition. Second, direct linear classifier fine-tuning performs more effectively than prompt learning. Furthermore, prompt learning is essentially a subset of parameter-efficient learning and represents a trade-off between generalization and optimality.  Findings across 11 datasets show that the approach presented in this research can significantly influnce the use of trained vision-language models in subsequent challenges. Strength  The experiments are sufficient and exhaustive. The results provide the valuable hints that what's the better way to deploy the pretrained vision-language model. The paper inspire people to think about more effective prompt design. Weakness,  The results are sufficent and the conclusion are well established but the reasons behind the result need to be more explored. For example, why classifier fintuning is much better than learning prompt?  The novelty is limited but as a rethinking paper, it is fine.  Can other models except clip still support findings? Overall, this paper provides some valuable hints about prompts but the novelty is limited. Therefore, I give my initial rating as borderline 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 97,
        "instruction": "Disentangled Feature Swapping Augmentation for Weakly Supervised Semantic Segmentation: Weakly supervised semantic segmentation utilizes a localization map obtained from a classifier to generate a pseudo-mask. However, classifiers utilize background cues to predict class labels because of a biased dataset consisting of images, in which specific objects frequently co-occur with certain backgrounds. Consequently, the classifier confuses the background with the target objects, resulting in inaccurate localization maps. To this end, we propose DisEntangled FeaTure swapping augmentation(DEFT) to prevent the classifier from being biased by a misleading correlation. Our method first disentangles the foreground and background features. Then, we randomly swap the disentangled features within mini-batches via a two-way process. These features contain various contexts that do not appear in the biased dataset, but the class relevant representation is preserved. In addition, we introduce training schemes to obtain further performance gains. Experimental results showed that when our augmentation was used in various weakly supervised semantic segmentation methods trained on the Pascal VOC 2012 dataset, the performance of the localization maps and pseudo-mask as well as the segmentation results improved. ",
        "reference": "This paper studies the phenomenal that WSSS performance will degrade with dataset biases whee a specific target object frequently appears in the background. To resolve it, the paper proposed an augmentation method that disentangles the target object and background-related features, such that the localization map will more focus on the real foreground. Experimental results show that the proposed module can largely improve the mIoUs on top of various existing approaches. Strength:  The author provide a detailed explanation to the unresolved issue for data augmentation of current WSSS approaches (based on CAM). The proposed idea is simple and reasonable. Improvements are obvious and consistent. The two-way swapping idea is also interesting and reasonably improved the performance.  Weaknesses:  The overall idea is similar to Lee et al. (2021d) despite with different tasks. I am not 100% convinced by the paper being motivated as \"biased data leading to degraded WSSS performance\". My understanding is that the CAM will attend to the background since classification also utilize the background context, so this paper proposed an approach to avoid such \"leaking\" and to have more accurate foreground seeds. The goal here is very different as Lee et al. (2021d) that de-bias to increase the diversity and improve the generalization. I am not sure if describing it as de-bias is a good choice here. There are many unclear presentation that need to be improved. Clarity & Quality:  There are many unclear presentations:  \"bias-aligned\" is not defined. I have to resort to Lee et al. 2021d for a clearer definition. I am not sure what Figure 1 (b) trying to show even with explanation in the text. How\u2019s the AFA/ABA value calculated in detail and why it shows two regions are correlated? Where is f_{fg} and f_{bg} in Figure 2?  Novelty: many ideas are similar to Lee et al. (2021d), i.e., feature swapping for augmentation. The paper proposed an augmentation method that disentangles the target object and background-related features, such that the localization map will more focus on the real foreground. The work demonstrates an interesting idea and consistent improvements on existing WSSS approaches. Still, many questions/concerns exists. I am currently in the borderline position, and would like to render a final decision upon the author's response.  Post rebuttal/Discussion: The authors' rebuttal/revision has partially address several of my points, including the AFA/ABA descriptions, the formulations of equations. However, there are serval concerns remaining: 1. For the idea, CAM is based for classification which explores both fg and bg. Thus, I am not sure whether the CAM itself can be well-trained if they are randomized with this augmentation.  2. As pointed by other reviewers, the presentation is not perfect and still has potential to be significantly improved. 3. Performance concern, as pointed by sGvK. Thus, I'd lower my rating to 5. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 98,
        "instruction": "Distributed Least Square Ranking with Random Features: In this paper, we study the statistical properties of pairwise ranking using distributed learning and random features (called DRank-RF) and establish its convergence analysis in probability. Theoretical analysis shows that DRank-RF remarkably reduces the computational requirements while preserving a satisfactory convergence rate. An extensive experiment verifies the effectiveness of DRank-RF. Furthermore, to improve the learning performance of DRank-RF, we propose an effective communication strategy for it and demonstrate the power of communications via theoretical assessments and numerical experiments.",
        "reference": "The authors study the statistical properties of pairwise ranking using distributed learning and random features ( DRank-RF) and establish its convergence analysis in probability. Numerical results confirm the practical aspects of the theory. The paper is rigorous, but several results are based on the prior work of DX Zhou and collaborators. The paper is clearly written, novel, and reproducible. The authors develop the DRank-RF approach and study its theoretical properties. Empirical results confirm its practical utility. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 99,
        "instruction": "Doing Fast Adaptation Fast: Conditionally Independent Deep Ensembles for Distribution Shifts: Classifiers in a diverse ensemble capture distinct predictive signals, which is valuable for datasets containing multiple strongly predictive signals. Performing fast adaptation at test time allows us to generalize to distributions where certain signals are no longer predictive, or to avoid relying on sensitive or protected attributes. However, ensemble learning is often expensive, even more so when we need to enforce diversity constraints between the high-dimensional representations of the classifiers. Instead, we propose an efficient and fast method for learning ensemble diversity. We minimize conditional mutual information of the output distributions between classifiers, a quantity which can be cheaply and exactly computed from empirical data. The resulting ensemble contains individually strong predictors that are only dependent because they predict the label. We demonstrate the efficacy of our method on shortcut learning tasks. Performing fast adaptation on our ensemble selects shortcut-invariant models that generalize well to test distributions where the shortcuts are uncorrelated with the label.\n",
        "reference": "This paper quantifies a notion of diversity for deep ensembles that facilitates efficient estimation. The authors show that it is sufficient to enforce conditional independence on the output distributions of the classifiers. This leads to their main contribution concerning the regularizing metric: conditional mutual information (CMI), efficiently computed in classification problems. The authors name this approach Conditionally Independent Deep Ensembles (CoDE).  The authors evaluate CoDE on benchmark datasets for shortcut learning. Strength:  The main strength of this paper lies in the clear exposition and development of CoDE. The definitions of invariance and diversity specific to CoDE are novel. The CoDE optimization problem is new, and its solution is efficient.   The numerical results are promising.  Weakness:  Novelty of the CoDE objective is unclear in light of the earlier work on conditional mutual information. The paper is clearly written and novel. The authors' main contribution is the development of CoDE approach and the objective for efficient learning in CoDE. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
    },
    {
        "id": 100,
        "instruction": "Diversity Boosted Learning for Domain Generalization with a Large Number of Domains: Machine learning algorithms minimizing the average training loss typically suffer from poor generalization performance. It inspires various works for domain generalization (DG), among which a series of methods work by $O(n^2)$ pairwise domain operations with n domains, where each one is often costly. Moreover, while a common objective in the DG literature is to learn invariant representations against spurious correlations induced by domains, we point out the insufficiency of it and highlight the importance of alleviating spurious correlations caused by objects. Based on the observation that diversity helps mitigate spurious correlations, we propose a Diversity boosted twO-level saMplIng framework (DOMI) to efficiently sample the most informative ones among a large number of domains and data points. We show that DOMI helps train robust models against spurious correlations from both domain-side and object-side, substantially enhancing the performance of five backbone DG algorithms on Rotated MNIST and Rotated Fashion MNIST.",
        "reference": "This paper:  solves the domain generalization problem presents several observations that diversity helps mitigate serious correlations proposes a sampling method that helps train robust models conducts experiment on Rotated MNIST and Rotated Fashion MNIST to show the effectiveness of the proposed algorithm The paper is easy to follow and the proposed algorithm is easy to understand. The idea is not totally new but seems promising. Since the work is likely considered as adopting and improving existing method, I believe the experimental results should be strong. However, I am not fully convinced by the current experiments. Here are my complaints:  The paper can be more professionally written. For example, the definition of set C is very vague. What is precisely a \"good\" correlation? The experiments are very limited. Why not try some large scale experiments mentioned in DomainBed? It is important to show the proposed algorithm on larger and tougher dataset. I am also curious to see how it works on ColoredMNIST. Please see above. The paper can be more professionally and well written. The idea is new and promising but needs more support. Source code is provided. I think the paper is not ready for publication at this point. If the authors can show more evidence and support, I am happy to raise my scores. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 101,
        "instruction": "Towards Performance-maximizing Network Pruning via Global Channel Attention: Network pruning has attracted increasing attention recently for its capability of transferring large-scale neural networks (e.g., CNNs) into resource-constrained devices. Such a transfer is typically achieved by removing redundant network parameters while retaining its generalization performance in a static or dynamic pruning manner. Concretely, static pruning usually maintains a larger and fit-to-all (samples) compressed network by removing the same channels for all samples, while dynamic pruning can adaptively remove (more) different channels for different samples and obtain state-of-the-art performance along with a higher compression ratio. However, since the system has to preserve the complete network information for sample-specific pruning, dynamic pruning methods are usually not memory-efficient. In this paper, our interest is to explore a static alternative, dubbed GlobalPru, to conventional static pruning methods that can take into account both compression ratio and model performance maximization. Specifically, a novel channel attention-based learn-to-rank algorithm is proposed to learn the global channel attention of the network for various samples, wherein, each sample-specific channel saliency is forced to reach an agreement on the global ranking. Hence, all samples can empirically share the same pruning priority of channels to achieve channel pruning with minimal performance loss. Extensive experiments demonstrate that the proposed GlobalPru can achieve better performance than state-of-the-art static and dynamic pruning methods by significant margins.",
        "reference": "The proposed work initially obtains a majority vote-based prior on the global rank of channel saliencies before forcing each sample-level channel saliency to match the global prior. In this way, the proposed work aims to use the platform of static pruning yet match the high pruning levels similar to dynamic pruning while maintaining a common channel saliency across all samples. Strengths  The context and explanation provided for static and dynamic pruning are well done.  Weaknesses  Quantitatively, on a channel to channel comparison, could the authors provide more insight in to the difference in performance and saliency between dynamic pruning approaches and GlobalPru? This could expose both the positive and negative aspects of both approaches. Channel level attention spans multiple ideologies and cannot be solely categorized as methods that are \"local\", especially over datasets, since certain methods learn inter-channel relationships over the dataset as opposed to sample-specific properties. Could the authors justify their statement in Pg. 3, Section 2.2, Lines 8-10? I encourage the authors to take a closer look at Figure 1 and revise it slightly so that it can be a common reference to the underlying process, especially across later sections. As constituted currently, there are certain missing elements and the flow of processes in the diagram is confusing. Equation 1 emphasizes objective functions which learn the mask to be applied on the weight matrices. Could the authors clarify if subsequent comparisons in the experiments section maintain this characteristic? The nomenclature of \"prior\" and \"global attention during training\" need to be clearly defined before being put to use. As constituted currently, they are clarified just before Section 3.3. I encourage the authors to revise the explanation in Section 3 to ensure preliminary terms are well defined before they are put to use. Could the authors clarify in some detail the reasoning behind the choice of expressions and formulation for the \u03d5(), and all relevant information beyond Equation 5? On first glance, there seems to be some inconsistency in notation in Equations 6 and 7. After establishing \u03b1,\u03b2 as balancing coefficients in the main loss function, the experimental setup highlights their values to be 0.0001. Could the authors justify the choice of small values, including a comparison of the impact of varying them across a range of values? Results from Table 1 consistently compare against ThiNet. However, there exist a number of more advanced methods, even static pruning, which improve upon ThiNet. Could the authors provide comparisons against current works that improve upon the performance of ThiNet? Could the authors clarify the baseline performance across the result tables provided and whether they match the relative drop in accuracy values across baselines? Figure 2, the X-axis is incorrectly labelled \"Interation\". Please revise the label. Clarity Conceptually, the ideas are clear. However, the explanation of \u03d5() and related formulae is unclear. Quality and Originality A one-to-one relative comparison of dynamic vs. the proposed approach is not available. This direct comparison could serve to highlight a number of aspects of the proposed work, in terms of bringing concepts from dynamic pruning and meshing them into static approaches. In addition, the tables of results can be further updated to reflect state-of-the-art methods in the pruning domain.  While the idea of a common channel attention rank is interesting, the above issues detract from the current work. Justifying and addressing the points addressed in the weaknesses mentioned above could serve to highlight interesting comparisons between the proposed and existing methods. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 102,
        "instruction": "Adaptive Block-wise Learning for Knowledge Distillation: Knowledge distillation allows the student network to improve its performance under the supervision of transferred knowledge. Existing knowledge distillation methods are implemented under the  implicit hypothesis that knowledge from teacher and student contributes to each layer of the student network to the same extent. In this work, we argue that there should be different contributions of knowledge from the teacher and the student during training for each layer. Experimental results evidence this argument. To the end, we propose a novel Adaptive Block-wise Learning~(ABL) for Knowledge Distillation to automatically balance teacher-guided knowledge between self-knowledge in each block. Specifically, to solve the problem that the error backpropagation algorithm cannot assign weights to each block of the student network independently, we leverage the local error signals to approximate the global error signals on student objectives. Moreover, we utilize a set of meta variables to control the contribution of the student knowledge and teacher knowledge to each block during the training process. Finally, the extensive experiments prove the effectiveness of our method. Meanwhile, ABL provides an insightful view that in the shallow blocks, the weight of teacher guidance is greater, while in the deep blocks, student knowledge has more influence.",
        "reference": "The manuscript observes the problem of fixed contributions of ground truth knowledge and teacher knowledge at different blocks of the student networks during knowledge distillation training. The author proposes a bi-level optimization scheme to balance the knowledge on the lower level and update the network based on optimized balance at the higher level. Strength:  Paper is well written Experiments on common benchmark datasets for KD A number of KD schemes are tested with the proposed scheme in the experimental results  Weaknesses:  The problem might not be well justified. The main argument of the paper is to use the performance of the proposed scheme to claim the solving of the fixed contributions of the two types of knowledge in different blocks. It would be more interesting to see what part of the network goes wrong with the fixed contributions scheme.   Whether the improvement of the network comes from the auxiliary network? Please consider adding an experiment by removing the bi-level optimization but still keeping all the auxiliary networks. Consider tuning the fixed parameters between ground truth knowledge and teacher knowledge in this scheme.   The increase in performance might be the result of solving the gradient vanishing not solving the balance between ground truth information and the teacher at the different blocks. The addition of an auxiliary could also be interpreted as a shortcut to the last layer. Consider checking the gradient vanishing between the standard KD scheme and the proposed KD scheme. Or further, can we just add a shortcut from each layer to the final layer and have different losses corresponding to each shortcut?  For some datasets, the utilization of teacher knowledge only could also achieve comparable or even better results compared to the utilization of both teacher knowledge and ground truth knowledge. The authors should consider adding a KD scheme with only teacher knowledge guidance as one of the baselines. So that the scheme of balancing between teacher knowledge and ground truth knowledge would be more meaningful.  How does the proposed scheme training time increase compared to the standard KD schemes? The main concern about the bi-level optimization is that it always takes too much time, while the achieved improvement in the manuscript seems to be not too significant (only around 0.5%, and poorer in some cases). This might be the strongest challenge to apply the proposed scheme to practical applications.  In table 3, Is that normal where the standard KD performs poorer than supervised training?  The authors should details out how each parameter is fixed in standard KD schemes in all experiments. Clarity: good Quality: good Novelty: good The paper might be helpful to the research community. However, the concern about training time might reduce the chances of its practical applications. While considering about the fundamental contributions, the authors should justify the problem clearly as well as put more analyses on the proposed schemes. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 103,
        "instruction": "Object-Centric Learning with Slot Mixture Models: Object-centric architectures usually apply some differentiable module on the whole feature map to decompose it into sets of entities representations called slots. Some of these methods structurally resemble clustering algorithms, where the center of the cluster in latent space serves as slot representation. Slot Attention is an example of such a method as a learnable analog of the soft k-Means algorithm. In our work, we use the learnable clustering method based on Gaussian Mixture Model, unlike other approaches we represent slots not only as centers of clusters but we also use information about the distance between clusters and assigned vectors, which leads to more expressive slots representations. Our experiments demonstrate that using this approach instead of Slot Attention improves performance in different scenarios achieving state-of-the-art performance in the set property prediction task.",
        "reference": "This paper proposes to combine the slot-based model with the gaussian mixture model (GMM) to improve the object-centric model. It explicitly represents the slot as the clustering center and uses the distance between slots to learn the mixture model. The experiments show certain improvements compared with some previous models. Strength  The SMM model integrates the learnable slots into the GMM model by replacing the clustering assignment and center update with learnable functions, such design learns the mixture model in an end-to-end manner. The paper conducts some experiments to compare with other object-centric models, demonstrating its efficacy in terms of performance.  Weakness  The SMM model learns to update the slots with a density function and distribute the learned density using a mechanism similar to slot competition. Such design is basically the composition of slot attention and GMM with incremental contribution, the novelty is not sound. The experimental results are not sufficient to convince me that SMM is a promising model that has certain significant advantages over the slot-attention model or GMM. It achieves SOTA performances on set prediction, which is not sufficient for evaluating object-centric learning, direct metrics for object-centric learning such as ARI and PSNR should be used to evaluate. Table 2 does not contain any comparison with other methods, same as Figure1 and Figure2. There is no ablation or further analyses for the modules in SMM, makes its performance mistery and hard to explain. Clarity and Quality Middle. The paper is not well-written, with some clear grammar errors, typography issues, and implausible references. For example,  in abstract: entities -> entity, in intro: centir -> centric. The writing is rough, and some sentences are long and not easy to understand. It seems the paper is not finished entirely. Overall, I think the paper is not ready and the contributions are incremental, detailed justification are list in weakness. I suggest the author to submit it to next venue with sufficient experiments and analyses. The SMM model might deserve further investigation for its potentials in object-centric learning and broader domains. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 1: The contributions are neither significant nor novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 104,
        "instruction": "Pocket-specific 3D Molecule Generation by Fragment-based Autoregressive Diffusion Models: Autoregressive model is widely adopted to generate 3D molecules which can fit any protein binding pocket. Current autoregressive model suffers from two major drawbacks. First, it is hard to  capture local geometric patterns as only one atom is generated at each step. Second, most of the autoregressive models generate atoms and chemical bonds in two separate processes, which causes a number of problems such as incorrect counts of rings, a bias distribution of bond lengths, and inaccurate 3D molecular structures. To tackle this problem, we designed a model, named FragDiff, to generate 3D molecules fragment-by-fragment for pockets. In each generation step, FragDiff places a molecular fragment around the pocket by using E(3)-equivariant diffusion generative models to simultaneously predict the atom types, atom coordinates and the chemical bonds of the fragment. Extensive experimental results confirm our assumption that unifying the atoms and bonds generations could significantly improve the quality of the sampled 3D molecules in terms of more accurate distributions of 2D subgraphs and 3D substructures.",
        "reference": "A general framework called FragDiff for pocket-specific 3D molecule generation is introduced. In particular, the generation process is executed in a local-to-global style. Namely, the diffusion model is adopted to generate the local fragment from scratch, while the autoregressive model is used to assemble the fragment into molecules. Strength: (1) It is reasonable to leverage the advantages of two kinds of generative models for molecule generation. (2) The experiment results are impressive.  Weaknesses:  (1) The organization of this paper is not clear. As claimed in the paper, the contribution is the fusion of two kinds of generation models into the same framework. However, the whole technique part, Section 3, is about the construction of diffusion for fragment generation.   (2) How to combine the diffusion model and autoregressive model is not detailed. More content should be added.  (3) Some equations are not correct or confusing.  3.1: Eq.6 is wrong. As the Intermediate variables, F1:T, are not integrated out, F0 should be replaced with F0:T in the left part of Eq.6 as well.   3.2: Eq.7 and Eq.8 is wrong. The summation operator over the set of edges, i.e., E and C, should be normalized.  3.3:  \u03c81, \u03c82 and \u03c83 in Eq.7 and Eq.8 should be different, as they have different types of input.   (4) It seems Eq.9 is the loss for the diffusion model only. Does it mean the autoregressive model is fixed in the whole training process?  (5) Too many grammatical errors, the paper should be double-checked. Different tenses are mix-used in related work. Singular and plural are mix-used in Section 3.3.2. Clarity: low, too many equation errors and grammatical errors.  Quality & Novelty: average. The molecule generation process based on the diffusion model and autoregressive model is reasonable.  Reproducibility: difficult. Too many components. Some important parts are missing. Overall, the idea of this paper is average, but it is hard to read due to the wrong equations. Moreover, the organization of this paper is poor. Some important contents are not discussed. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 105,
        "instruction": "Towards scalable and non-IID robust Hierarchical Federated Learning via Label-driven Knowledge Aggregator: In real-world applications, Federated Learning (FL) meets two challenges: (1) scalability, especially when applied to massive IoT networks, and (2) how to be robust against an environment with heterogeneous data. Realizing the first problem, we aim to design a novel FL framework named Full-stack FL (F2L). More specifically, F2L utilizes a hierarchical network architecture, making extending the FL network accessible without reconstructing the whole network system. Moreover, leveraging the advantages of hierarchical network design, we propose a new label-driven knowledge distillation (LKD) technique at the global server to address the second problem. As opposed to current knowledge distillation techniques, LKD is capable of training a student model, which consists of good knowledge from all teachers' models. Therefore, our proposed algorithm can effectively extract the knowledge of the regions' data distribution (i.e., the regional aggregated models) to reduce the divergence between clients' models when operating under the FL system with non-independent identically distributed data. Extensive experiment results reveal that: (i) our F2L method can significantly improve the overall FL efficiency in all global distillations, and (ii) F2L rapidly achieves convergence as global distillation stages occur instead of increasing on each communication cycle.",
        "reference": "This paper proposes a hierarchical FL framework with a new label-driven distillation method to handle non-iid FL scenarios S1. The proposal of a hierarchical structure for FL is reasonable. W1. The paper is very hard to follow, as a lot of design considerations are proposed, but which parts are the most novel ones are unclear. W2. The comparison with baselines is unfair. It seems that the authors assume there is some test data in the server so that every region\u2019s model can be validated for each label; then label-driven distillation can be proposed. However, the baselines seem not to have this assumption and can work well without the centralized test data. In other words, the authors\u2019 method uses external knowledge (i.e., server test data), then performance improvement is expected. W3. In practice, it is very hard to collect test data for the server; even if the server can collect some, such data may be very biased (because many clients will not allow the server to collect data; this is actually why FL is needed). Then, the server\u2019s validation accuracy may be doubtful. Hence, the application of the proposed mechanism in practice is doubtful. W4. The experiment settings are manually controlled so whether the regional hierarchy can work in practice is still unknown. From my reading of the paper, I think the mechanism may work well when different regions have non-iid samples (the experimental setting); however, if non-iid happens for the clients in a region, the mechanism may still fail. The authors need to use realistic data/region partitions instead manually controlled ones to validate the usefulness of the proposed mechanism. W5. While the hierarchy structure of FL is sensible, the authors may have real experiments to deploy hundreds of clients in different regions to verify the practicality of the hierarchy structure. Otherwise, how this structure is useful in practice is unclear. clarity: the paper is a bit hard to follow quality: the experiments need to be improved novelty: the hierarchy structure of FL is somehow new if the authors can really build such an FL system including hundreds of clients distributed in different regions (instead of only simulation in one computer) In general, I think that the hierarchy structure has potential in real FL systems. Meanwhile, the challenge of building such a system may be more in other parts, e.g., communication/node joining & dropping, instead of gradient aggregation. For the label-driven distillation, I think the authors' comparison with baselines is a bit unfair, as the authors' mechanism has more information (i.e., test data in the server). 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 106,
        "instruction": "Quantized Disentangled Representations for Object-Centric Visual Tasks: Recently, the pre-quantization of image features into discrete latent variables has helped to achieve remarkable results in image modeling. In this paper, we propose a method to learn discrete latent variables applied to object-centric tasks. In our approach, each object is assigned a slot which is represented as a vector generated by sampling from non-overlapping sets of low-dimensional discrete variables.\nWe empirically demonstrate that embeddings from the learned discrete latent spaces have the disentanglement property. The model is trained with a set prediction and object discovery as downstream tasks. It achieves the state-of-the-art results on the CLEVR dataset among a class of object-centric methods for set prediction task. We also demonstrate manipulation of individual objects in a scene with controllable image generation in the object discovery setting.",
        "reference": "This work proposed to combine the idea of slot-attention and vector quantization to learn discrete object-centric representation of visual scenes. The proposed model utilizes slot-attention to decompose the image into a set of object-centric slots, and then transform each inferred slot into a few concatenated vectors by vector quantization from a learned codebook. Both set prediction and object discovery tasks are evaluated to show the effectiveness of the proposed model, while particular efforts are made to show the ability to discover disentangled subspace in the latent space partitioned by the discrete quantization. Strengths:  The proposed work seems to be a reasonable extension of the slot-attention model, and the empirical results on set prediction show some extent of improvements over the baseline.  Weaknesses:  The writing is generally not as clear as it could be. The most important part of the paper should be Sec. 2.3, however, it's unnecessarily hard to follow, and Figure 1 is not very helpful here. The notations of introduced variables are not introduced in a more natural way, for example, the eji term is not described in a clear way in its first appearance. In Sec 2.5, the authors propose to use beta-VAE style loss to encourage the latent space to be disentangled. However, there's no mention and discussion on the weighting effect, i.e. \u03b2>1, which plays a crucial role. In object discovery tasks, only qualitative results are provided, and no quantitative evaluation is included, which is not convincing enough, especially when only CLEVR dataset is considered. Please see above. This work proposes an approach for learning discrete object-centric representation by combining existing ideas, I believe more thorough evaluation and better clarity are needed in this reviewing cycle. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 107,
        "instruction": "Supervised Random Feature Regression via Projection Pursuit: Random feature methods and neural network models are two popular nonparametric modeling methods, which are regarded as representatives of shallow learning and Neural Network, respectively. In practice random  feature methods are short of the capacity of feature learning, while neural network methods lead to computationally heavy problems. This paper aims at proposing  a flexible but computational efficient method for general nonparametric problems.  Precisely, our proposed method is a feed-forward two-layer nonparametric estimation, and the first layer is used to learn a series of univariate basis functions for each projection variable, and then search for their optimal linear combination for each group of these learnt functions. Based on all the  features derived in the first layer, the second layer attempts at learning a single index function with an unknown activation function. Our nonparametric estimation takes advantage of both random features and neural networks, and can be seen as an intermediate bridge between them.",
        "reference": "The paper proposes an approach for boosting the effectiveness of kernelized Ridge regression by first learning a set of random features through a deep-learning inspired preprocessing step, then applying kernelized Ridge regression to the learned features. Strength: While much work has been done on constructing arbitrarily expressive kernels [1, 2] and scaling up kernel-based regression methods, more work is needed to make learning more efficient in kernel methods. Borrowing from deep learning to learn expressive families of kernels, as attempted by this paper, is laudable.  Weaknesses:   Clarity: The paper is hard to read and contains too many typos. Comparison to Deep Learning: It is unclear why this approach would perform better than vanilla deep learning. A strong intuition and substantially more experiments are needed to make this case. Comparison to expressive kernel: This paper is missing much of the literature on expressive kernel methods, especially Generalized Spectral Kernels [1]. In particular, [2] introduced kernel families (namely GSKs) that are general-purpose in that they contain kernel that can perform as well as any other kernel not in the family, stationary or non-stationary. Additionally, a flurry of methods have been developed to scale up kernel regression. It would have been interesting to discuss what benefits this approach has over GSKs.  Additional Comments: Page 1: No condition is required for Eq (1) to be Kernel Ridge regression. Basis function regression with Ridge penalty is always kernelized Ridge regression. The kernel implied by Eq (1) is random and the behavior as N\u2192\u221e pertains to the convergence of the random kernel to a deterministic kernel.  [1] Samo, Y.L.K. and Roberts, S., 2015. Generalized spectral kernels. arXiv preprint arXiv:1506.02236. [2] Samo, Y.L.K., 2017. Advances in kernel methods: towards general-purpose and scalable models (Doctoral dissertation, University of Oxford). Clarity: The paper could be made easier to read and could benefit from some proofreading to iron out typos (e.g. Page 2: \"This is because that the RF\", \"Comparing to other kernel methods that mapping x to a high dimensional space [...]\" etc.)  Originality: I did not find any idea in this paper particularly original. The paper should be proofread, and more intuition and more experiments should be added to argue the benefits relative to vanilla deep learning or expressive kernel methods. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 108,
        "instruction": "Graph Spline Networks for Efficient Continuous Simulation of Dynamical Systems: While complex simulations of physical systems have been widely studied in engineering and scientific computing, lowering their often prohibitive computational requirements has only recently been tackled by deep learning approaches. In this paper, we present GraphSplineNets, a novel deep learning approach to speed up simulation of physical systems with spatio-temporal continuous outputs by exploiting the synergy between graph neural networks (GNN) and orthogonal spline collocation (OSC). Two differentiable time-oriented OSC and spatial-oriented OSC are applied to bridge the gap between discrete GNN outputs and generate continuous solutions at any location in space and time without explicit prior knowledge of underlying differential equations. Moreover, we introduce an adaptive collocation strategy in space to enable the model to sample from the most important regions. Our model improves on widely used graph neural networks for physics simulation on both efficiency and solution accuracy. We demonstrate SplineGraphNets in predicting complex dynamical systems such as the heat equation, damped wave propagation and the Navier-Stokes equations for incompressible flow, where they improve accuracy of more than 25% while providing at least 60% speedup. ",
        "reference": "The aurhors provide GRAPHSPLINENETS, a novel deep learning approach to speed up simulation of physical systems with spatio-temporal continuous outputs by exploiting the synergy between graph neural networks (GNN) and orthogonal spline collocation (OSC). Two differentiable OSC (time-oriented OSC and spatial-oriented OSC) are applied to bridge the gap between discrete GNN outputs and generate continuous solutions at any location in space and time without explicit prior knowledge of underlying differential equations. Strength The physics application is quite interesting Combining deel learning with physics are interesting Weakness 1 experiment designs:  Heat Equation:  Damped Wave:  Navier-Stokes are simulated? not real world data?  simulated data will be much easier to fit.  2 motivation and clarity not fully board with the motivation of Time\u2013oriented OSC/Space\u2013oriented OSC:  Is Time\u2013oriented OSC/Space\u2013oriented OSC for introducing continuous solution on mesh grid data? I am not fully on board on why using GNN is helpful, because mesh-grid data? 3 evaluation Table 1: existing methods seem not for this tasks specific (i.e. GNN in general), so performance on this tasks may not be good.  compare with methods that  without or without  \u201cspace and time continuous simulations\u201d are more relevant. GEN is one of the example.   Table is shows larger gain on 5s versus 1s, are there some discussion on this? I feel the paper can provide a clearer story and provide better experiment details I have some concerns on \"experiment design\", \"motivations\", \"evaluations\" 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 109,
        "instruction": "Online black-box adaptation to label-shift in the presence of conditional-shift: We consider an out-of-distribution setting where trained predictive models are deployed online in new locations (inducing conditional-shift), such that these locations are also associated with differently skewed target distributions (label-shift). While approaches for online adaptation to label-shift have recently been discussed by Wu et al. (2021), the potential presence of concurrent conditional-shift has not been considered in the literature, although one might anticipate such distributional shifts in realistic deployments. In this paper, we empirically explore the effectiveness of online adaptation methods in such situations on three synthetic and two realistic datasets, comprising both classification and regression problems. We show that it is possible to improve performance in these settings by learning additional hyper-parameters to account for the presence of conditional-shift by using appropriate validation sets. ",
        "reference": "It is well known that the performance of machine learning models is highly dependent on the distribution of the data on which it is evaluated: model performance deteriorates when tested on data generated from a distribution shifted with respect to the training data generating process. Identifying and mitigating the effects of distribution shifts is a major open challenge for machine learning practitioners, as distribution shifts are ubiquitous in an ever-changing world. In the supervised learning context, evaluating test performance and mitigating it usually require labelled testing data, which is often difficult or impossible to obtain.  Little can be done about arbitrary distribution shifts \u2013 generalization from training to test data is only possible if the shift leaves some structure in the data unchanged. Label shift is a basic example of such a distribution shift, where the conditional probability P(X|Y) remains fixed, and only P(Y) changes. Here X are the covariates, Y the label, and P(X,Y) = P(X|Y)P(Y) is their joint distribution. Recent years saw much progress with the analysis of label shift, and methods have been developed to mitigate its impact on black box models \u2013 with deep learning a primary application -- in both offline and online settings. Essentially, these methods rely on re-weighting model predictions using the distribution of predicted (pseudo-)labels, and thus do not require true labels for the test data.  The current paper follows three goals related to label shift adaptation:  The paper\u2019s main effort focuses on examining how previously proposed label shift mitigation methods perform on shifted distributions do not satisfy the label shift condition \u2013 a scenario highly relevant to real-world applications, where often as pure label shifts are rare. In an online learning setting, albeit one in which the distribution does not shift continuously, the paper examines empirically how recently proposed algorithms for online adaptation to label shift perform on a few synthetic and realistic datasets that exemplify different kinds of \u201cnon-label\u201d distribution shift. The empirical investigation also considers a couple of heuristically-motivated extensions to these algorithms, most notably performing model selection on an OOD validation set which is shifter with respect to both the training and test sets. The findings of these investigations are not clear cut, but suggest that in some cases, the proposed algorithms provide an improved adaptation to the distribution shift. The takeaway is that label shift adaptation methods (or some heuristic generalization thereof) might sometimes be useful to mitigate general distribution shifts, even if this practice has no known theoretical justification.  The paper considers two further issues related to label shift adaptation:  Past work on label shift adaptation has mostly focused on classification problems. The paper proposes an algorithm for label shift adaptation in regression settings, and studies it empirically. Past algorithms for online label shift adaptation require the inversion of an empirically measured confusion matrix. The paper suggests a heuristic fix for the case when this matrix is non-invertible and studies it empirically.  The latter two issues are discussed briefly (compared to the main topic of the paper), and here too the investigations do not provide clear cut conclusions on the efficacy of the proposed methods, but in some cases these methods perform better than the baseline. Major Strengths:   The problem investigated is well motivated. Distribution shifts are indeed a big and relevant problem when machine learning models are deployed in the real world. Much of the work to date has focused on idealized types of shifts, like label or covariate shift. It is natural to wonder how much methods developed for idealized shifts might be useful in more realistic settings. Furthermore, if label shift adaptation methods generalize to realistic shift scenarios, they are attractive from a practical standpoint, as they do not require labelled test data.  Empirical results are, for the most part (except for some comments below), clearly presented: I could understand what was done and believe I have enough information to attempt to reproduce the results.  The paper is quite honest about the inconclusive nature of much of the results, and does not try to oversell the proposed methods.  Major Weaknesses:   A systematic or principled approach to the types of distribution shifts considered is missing. Distributions can shift in many ways and for many reasons. Adding conditional shift to label shift is tantamount to considering general distribution shifts. Indeed, the paper considers two examples with no label shift (P(Y) is not changed in the synthetic MNIST and COCO-on-Places datasets), an example with covariate shift (Mixture of Gaussians), and two general distribution shifts (from the WILDS dataset). Framing the issue as \u201clabel shift in the presence of conditional shift\u201d might give a wrong impression that the conditional shift is a perturbation of the label shift condition. I find it clearer to state that general distribution shifts are considered.  Little can be said about distribution shifts in general, without focusing on particular types or characteristics of the shifts, such as label/covariate shift, subpopulation shift [6], or shifts where the data generating process has a fixed known causal structure [3]-[5]. Since experiments in the paper do not belong to a particular type of shift, it is hard to compare results or to generalize from them to general shifts.  The lack of a systematic approach to general label shifts is reflected also in the absence of discussion of relevant work on this issue, including refs [1]\u2014[6].  Given the vast scope of possible distribution shifts, with no systemic understanding of how they relate to or differ from label shift, and with heuristic methods lacking a theoretical foundation \u2013 given these, a major and comprehensive empirical study is necessary in order to ascertain the usefulness of the proposed methods. The paper offers modest experiments, in terms of types and strengths of shift, types of data, and alternative baselines/methods. This severely limits the usefulness of the results, as it is unclear when the suggested methods can be expected to improve upon baselines, and how good such improvement are compared to alternative methods. As it stands, few generalizable insights can be drawn from the empirical scope of the paper. The paper itself is honest about the modest and tentative nature of the findings, when it concludes that the experiments are \u201csuggestive\u201d that the proposed methods show \u201cpromising trends for the most part\u201d in the limited scope in which they were tested.  Concretely, for the experiments performed, here are some suggestions of baselines/methods that might provide a wider context for obtained results:    a. An optimal fixed classifier, as considered by Wu et al. (2021).   b. Results obtained from offline domain adaptation methods (Garg et al., 2020).   c. Results obtained from known domain generalization methods such as those mentioned in the related works section of the paper, or the ones surveyed by Gulrajani & Lopez-Paz (2020). In particular, if I understand correctly, CORAL was used for the two WILDS datasets considered in the paper, but not the others. It might be more informative to test all datasets with and without CORALS (and/or other domain adaptation methods).    d. The paper emphasizes the importance of the use of an OOD validation set. It would thus be useful to test the effect of this OOD validation set on test performance by considering the effect of different validation sets, preferably with different characteristics. For example, for the synthetic colored MNIST dataset, one could use validation sets that are more or less correlated with the test sets.   Goals 2+3 above are not explored in detail in the paper. No references are given to prior work on regression label shift / domain adaptation (e.g., [7]-[8] below), nor to the discussion in Lipton et al. (2018, section 7) about remedies to non-invertible empirical confusion matrices. The corresponding experiments provide only an initial investigation into them. The paper provides some interesting but embryonic discussion/exploration of both. Their inclusion in the current form of the paper   Some key definitions and explanations are lacking in the paper, making it difficult to understand some sections of it.   a. \u201cConditional shift\u201d is not defined. While it is a term used in the literature and whose meaning might be intuitive, many other terms are used in the literature as well. To guarantee that there are no misunderstandings regarding this central concept, its definition should be provided.    b. Method FTH-H-B and FTH-H-B (R) are never clearly defined (what is the \u201cpseudo-count hyper-parameter\u201d mentioned? I did not understand).   c. In equation (3), the definition of the expected error rate \\ell^{\\test{new}} is only given in words, not in a formula.    d. In section 4.1, what are a, b, kappa, and mu?  e. In appendix A, none of the notation is defined, and in fact no information is given about the context and goal of the derivation there.   Further comments  Online vs offline methods. The scope of label shifts considered in this paper is more limited than those considered by Wu et al.: here only constant shifts are considered (test data is drawn from a fixed shifted distribution), whereas Wu et al considered distributions that keep changing throughout training. An important strength of online methods are their ability to deal with continual changes. Considering only constant changes reduces (but does not invalidate) the usefulness of online methods compared to offline ones. The decision to focus on online methods should be motivated in the paper.   OOD validation: the concept of OOD validation is introduced in Heuristic 1 without being properly defined/explained. As this is a central tenet in the proposed methods, the idea and procedure should have a clear and detailed explanation. Furthermore, in Heuristic 1 it is written that OOD validation is a standard practice of model selection, with a reference to Gularjani & Lopez-Paz (2020). As far as I can tell, this reference (which emphasizes the importance of validation set details in the context of domain generalization) does not advocate the use of validation on a separate OOD set. Rather, it attributes this method to Krueger et al. (2020), who indeed mention it in an appendix.  Regarding the method itself OOD validation itself: why should it work? I can understand that it might be useful when the shifts in the validation and test sets are somehow related (like the Skewed-MNIST example where test is a more severe shift of the same type as validation), but why would it help in examples like the mixture of Gaussians, or the WILDS datasets? Looking at the experiment results, it indeed seems to me that OOD validation is helpful only for the skewed-MNIST example. If my reading is correct, this should be stated clearly, and the appropriate qualifications should be made in the conclusions about the merits of OOD validation. Currently, section 5.3 states that \u201cUsing OOD validation sets \u2026 improves results on the whole\u201d \u2013 but for S-COCO-on-Places and iWildCam (Avg) I do not see any improvement more significant than the noise level, and for iWildCam (F1) there is a small deterioration (which is also consistent with noise).  From a practical perspective, performing OOD validation is not always possible as it requires more labelled data \u2013 it would be useful to emphasize this fact. Technically \u2013 what are all the optimization steps performed on this validation set? I.e., which hyper-parameters are calculated on this validation set, other than the confusion matrix?   Non-invertible confusion matrices. The methods proposed in Heuristic 3 surely generate invertible matrices, but why would they be expected to work for label shift and general distribution shift adaptation? They seem to me ad-hoc and unmotivated. What would be their merit compared to using a pseudo inverse, or the soft probability matrix suggested by Lipton et al. (2018)?   Section 4: The role of this Bayesian discussion is not clear to me. What insights are gained from this Bayesian perspective? Are these insights relevant also to cases of pure-label shift, or only general distribution shifts? I found the discussion around equations (11)-(14) confusing on first reading. The notation in equations (11)-(12) is confusing, perhaps Y|\\phi ~ Cat(\\alpha) and \\phi ~ Dir(\\alpha). The notation in equation (13)-(14) \u2013 P_t(\\phi), P^{new}_{t+1} is not defined anywhere.  I found the whole of section 4.1 confusing. How is the discussion related to label shifts in regression problems? What are the takeaways or results of this section? Are the results valid only for the Gaussian example with a conjugate prior, or more generally applicable? What kind of calibration is performed in this section, and why is it useful?  Experiment details. Right before section 5.1:    a. It would be worthwhile to provide the details of \u201cthe surrogate loss implementation of Wu et al.\u201d    b. What are the details of the grid search used for the parameter of OGD? On which validation set is it taking place.   c. Skewed-MNIST should reference the inspiration from color MNIST of Arjovsky et al. (2019). A table with the makeup (number of digits of each color) of each of the train/val/test datasets would be useful. It is stated that \u201cSince the overall class frequencies are balanced \u2026 we drop the P(Y)\u201d. Drop it from where (same comment for skewed COCO on Places)? Appendix C.1 describes how digits were split into two sets \u2013 was there a precise protocol for this? How is the \u201ctend(ency) to be confused\u201d measured context? What was the optimizer used for training  - SGD?    d. WILDS-iWildCam: it is stated that \u201cWe use Heuristic 3 for evaluating methods on this dataset. Heuristic 3 mentions several approaches: adding a tunable scalar to the diagonal? Using the identity matrix? Using a \u201cpseudo-count\u201d?    e. Table 2: How are the error estimates estimated relevant to all tables)? Why are the error estimates here +- 0? Are the quantities really measured to perfect accuracy?   Minor comments  Before equation (4): \u201cwhere e is a one hot vector for the predicted category\u201d \u2013 the description and notation there can be clarified: it was initially unclear to me which predicted category is referred to, and only after reading Wu et. al (2021) did I understand that these are calculated for each step I separately.   After equation (4), it is stated that calculating the gradients is tricky. Why is it so? For self-containedess, the statement should be explained. Similarly, before equation (7) it is stated that FTH is more efficient than OGD \u2013 efficient in which sense? Compute time? Memory? Data complexity?   Right before 5.1.3, it is mentioned that \u201ctest-sets are smaller\u201d. Smaller than what?   Typos:    Heuristic 1, line 2: shiftis -> shift is Two lines below equation (19): minimum -> minima Last line of page 6: there\u2019s a superfluous ).  5.2.2, last line of first paragraph, should read \u201cin neither training nor validation sets for OOD test.\u201d The reference to Sun and Saenko (2016) is missing bibliographic info (journal name). Appendix A: equations (23) and (24) seem to be the same  References  [1] Storkey, When training and test sets are different, in:Quinonero Candela et al., Dataset Shift in Machine Learning, 2009 [2] Moreno-Torresa et al., A unifying view on dataset shift in classification  (2012) [3] Schoelkopf et al., On Causal and Anticausal Learning (2012) [4] Zhang et al., Domain adaptation under target and conditional shift (2012)  [5] Kull and Flach, Patterns of dataset shift (2014) [6] Breeds: Benchmarks for subpopulation shift, Santurkar et al. (2020) [7] Cortes and Mohri, Domain Adaptation in Regression (2011) [8] Cortes and Mohri, Domain adaptation and sample bias correction theory and algorithm for regression (2014) The paper, while being short and concise, is for the most part easily readable. Some sections that I found to be more difficult to understand are listed above.  Experiments are described clearly and seem reproducible. Some minor misunderstandings that I had regarding experimental protocols are listed above.  As far as I can tell, the paper's examination of online labels+conditional shift adaptation of neural networks is novel, as are the experiments performed here.  As detailed above, the quality of the paper can in my opinion be greatly improved if more context was provided about the distribution shifts considered, a more thorough empirical investigation was conducted, and the unclear/undefined terms and sections are clarified. The work presented in this paper is novel, seems technically correct, and addresses a key problem to many real-world scenarios. I believe that the work in its current state with some corrections/improvements could and should merit publication in some venue. However, with the flaws described above, I do not believe this paper is ready for publication. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 110,
        "instruction": "RuDar: Weather Radar Dataset for Precipitation Nowcasting with Geographical and Seasonal Variability: Precipitation nowcasting, a short-term (up to six hours) rain prediction, is arguably one of the most demanding weather forecasting tasks.\nTo achieve accurate predictions, a forecasting model should consider miscellaneous meteorological and geographical data sources.\nCurrently available datasets provide information only about precipitation intensity, vertically integrated liquid (VIL), or maximum reflectivity on the vertical section.\nSuch single-level or aggregated data lacks description of the reflectivity change in vertical dimension, simplifying or distorting the corresponding models.\n\nTo fill this gap, we introduce an additional dimension of the precipitation measurements in the RuDar dataset that incorporates 3D radar echo observations.\nMeasurements are collected from 30 weather radars located mostly in the European part of Russia, covering multiple climate zones.\nRadar product updates every 10 minutes with a 2 km spatial resolution.\nThe measurements include precipitation intensity (mm/h) at an altitude of 600 m, reflectivity (dBZ) and radial velocity (m/s) at 10 altitude levels from 1 km to 10 km with 1 km step.\nWe also add the orography information as it affects the intensity and distribution of precipitation.\nThe dataset includes over 50 000 timestamps over a two-year period from 2019 to 2021, totalling in roughly 100 GB of data.\n\nWe evaluate several baselines, including optical flow and neural network models, for precipitation nowcasting on the proposed data. We also evaluate the uncertainty quantification for the ensemble scenario and show that the corresponding estimates do correlate with the ensemble errors on different sections of data.\nWe believe that RuDar dataset will become a reliable benchmark for precipitation nowcasting models and also will be used in other machine learning tasks, e.g., in data shift studying, anomaly detection, or uncertainty estimation.\nBoth dataset and code for data processing and model preparation are publicly available.",
        "reference": "This paper propsed a different weather prediction model by inducing a new dataset [above russia] which is adopted to evaluate all current models such as ConvLSTM etc.   The results looks promising on this new datasets. Pro:   The dataset is attractive, I wonder how the models performance across different datasets, are their performance rankings consistent or not ?  Cons:  2) The methods used are relatively low-cost, I wonder how heavy cost models, such as transformers like architectures, perform comparing with these baselines.  How is the dataset usability ?  Is the model trained based on such dataset can be comparable with the most advanced weather forcast devices . The paper is clear and easy to understand, novelty is limited but the dataset is attractive.  The paper should be easy to reproduce. Good dataset, while limited domain relativity and novelty. 4: All of the claims and statements are well-supported and correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 111,
        "instruction": "Learning Representations for Reinforcement Learning with Hierarchical Forward Models: Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions, which may miss relevant information if important environmental changes take many steps to manifest. We propose Hierarchical $k$-Step Latent (HKSL), an auxiliary task that learns representations via a hierarchy of forward models that operate at varying magnitudes of step skipping while also learning to communicate between levels in the hierarchy. We evaluate HKSL in a suite of 30 robotic control tasks with and without distractors and a task of our creation. We find that HKSL either converges to higher episodic returns or optimal performance more quickly than several current baselines. Furthermore, we find that HKSL's representations capture task-relevant details accurately across timescales (even in the presence of distractors) and that communication channels between hierarchy levels organize information based on both sides of the communication process, both of which improve sample efficiency.",
        "reference": "Learn a hierarchy of latent models where each level takes in the latent state and a length n concatenation of actions and predicts the state after n steps, instead of simply predicting single step transitions. Information is passed from the layers predicting longer temporal distance to the lower levels. The representations are trained with l2 loss between the representations and the encoded observations. The representation is then used by SAC with h critics, corresponding to different levels of the hierarchy, and a policy representation that is based on the encodings learned by the levels. The reasoning that single step forward models fail to capture relevant information is a bit weak in the context of an MDP, where the markov property should ensure that all information is captured in a single state.  It isn't clear exactly how the encoder is prevented from collapse, since if the encoder is encoded to zero state then the forward models would have zero loss. While the results are promising and the writing is clear, the intuition behind what the different levels of the hierarchy might be encodinig. In particular, it would be nice ot have some visualization demonstrating how the higher levels capture different information from the lower ones, and which parts of the encoding the policy attends to. This might be more informative than indicating if task-relevant components are attended to, since it isn't clear why hskl would be especially effective at capturing task-relevant components. Otherwise, it is possible that a multi-head representation might get similar results, without the heads having temporally different meanings. The experiments have many domains with only two levels, except for the falling pixels. It might be nice to look at a very long horizon tsk with a large number of levels. The paper is relatively clear in terms of what it is doing, though the method is somewhat unintuitive. The writing does not detract from the paper. While methods doing multi-step dynamics modeling have been used in RL, having multiple encoders appears to be a novel contribution. The experiments are somewhat weak, since the domains are such that the number of layers remains low, and it is not clear what the layers are learning that is distinct between each other. I think this paper can be accepted because it appears to provide a novel, simple change to existing model-based state representation RL methods, and it is implemented well enough with sufficient experiments to constitute a contribution. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 112,
        "instruction": "xTrimoABFold: Improving Antibody Structure Prediction without Multiple Sequence Alignments : Antibody, used by the immune system to identify and neutralize foreign objects such as pathogenic bacteria and viruses, plays an important role in immune system. In the field of drug engineering, the essential task is designing a novel antibody to make sure its paratope (substructures in the antibody) binds to the epitope of the specific antigen with high precision. Also, understanding the structure of antibody and its paratope can facilitate a mechanistic understanding of the function. Therefore, antibody structure prediction has always been a highly valuable problem for drug discovery. AlphaFold2, a breakthrough in the field of structural biology, provides a feasible solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy on antibody, especially on the complementarity-determining regions (CDRs) of antibody limit its applications on the industrially high-throughput drug design. In this paper, we present a novel method named xTrimoABFold to predict antibody structure from antibody sequence based on a pretrained antibody language model (ALM) as well as homologous templates, which are searched from protein database (PDB) via fast and cheap algorithms. xTrimoABFold outperforms the MSA-based AlphaFold2 and the protein language model based SOTAs, e.g., OmegaFold, HelixFold-Single and IgFold with a large significant margin (30+% improvement on RMSD) while performs 151x faster than AlphaFold2. To the best of our knowledge, xTrimoABFold is the best antibody structure predictor to date in the world.",
        "reference": "Antibody structure prediction is a highly sought after problem in industrial drug discovery pipelines. As with standard proteins, accurate predictions can lead to a better understanding of an antibody's function. Unfortunately, current methods for antibody structure prediction are not yet able to produce high-resolution predictions, particularly at the variable CDR regions of the antibodies. The paper presents xTrimoABFold, which is able to achieve high accuracy on a curated antibody structure dataset at much lower computational cost. Strengths:  The results in Tables 2 and 3 are astounding, though some of the numbers may require further exploration/explanation.  Weaknesses:  The method itself is not novel at all. The authors essentially reuse the loss function and architecture from AlphaFold2 (without MSAs).  The analysis of the results are quite shallow. For example, a structure prediction paper with no figures with structures is quite strange and unusual. The results are almost too good to be true. In addition, the results for IgFold look confusingly poor. IgFold can be considered the current state-of-the-art.  Please include more background information on antibodies and proteins as a whole. For example, why do we even care about CDRs? There was not a single mention in the paper about what the CDR actually is and what is so critical about the CDR to an antibody's function. Many grammatical errors and typos, to the point that the paper looks like it was rushed and ill-prepared. I understand that the authors may not be fluent in English, but many of these errors could easily have been fixed by copy-and-pasting into standard text editors/word processors with spelling and grammar checking. Clarity: The paper itself is quite poorly written with many grammatical errors and typos, many of which could have been avoided with a simple copy-and-paste into a word-processor. The methods and results are also not well organized or explained. For example: As mentioned before, there is very little background knowledge provided about antibodies.  The section \"Template Searching Algorithms and Tools\" is extremely difficult to read. There is very little background into  In equation (6), what exactly do you mean Tj is an SE(3) transformation? What is the j subscript indicating? In equation (7), what is the purpose of \u03f5?  In equation (8), why divide by Z? What does Z represent. Without the 1/Z factor, it looks like the average of min(dclamp,dij). Many comparisons are made against AlphaFold2. One of the most relevant hyperparameters to mention would then be MSA depth. However, there is no mention of this hyperparameter nor an ablation compared against this.  You make a mention of DeepAb and ABlooper but there is no comparison made with those methods. In Figure 3, median time of what? There is no mention of what is actually being measured. There should be some figure(s) that actually show the ground-truth and predicted 3D structures (for both xTrimoABFold and a comparable baseline).   Quality: I'm quite unsure about the numbers for Table 2, particularly for IgFold. RMSD of 14 is extremely large, and the numbers for IgFold look more like the result of poor implementation. If this result is indeed valid, then the authors should include some exploration/explanation into why this may be so, as IgFold could be considered state-of-the-art. In Table 3, the RMSD values for the CDR3 region are almost too good to be true. In reality, from the perspective of antibody design and engineering, the CDR3 region is the most variable and difficult to model. The RMSD values computed for even the baselines is extremely low, which is quite surprising. This may, however, be due to the nature of the dataset. The authors specifically use single chains for the antibody dataset. This part requires further exploration.  Novelty: The paper has very little novelty. The authors generate a filtered dataset from data that is publicly available (though it is not clear why the authors did not simply choose to use a dataset that others have used in the literature). The authors use the AlphaFold2 architecture (without MSAs) and the same loss function (without MSA loss). The authors add a fine-tuning loss that penalizes RMSD for CDRs.  Reproducibility: The authors do not provide any open source code or the data they trained on. The paper is quite poorly written. Besides the numerous grammatical errors and typos, the content of the paper, as it currently stands, would not qualify for publication. There is very little background knowledge provided, e.g. why do we care about CDRs for antibodies and how does this make the problem very different than standard protein structure prediction. The method itself is unoriginal as it uses the exact same architecture (without MSAs) and loss function (again, without MSA contribution) as AlphaFold2. The authors use FoldSeek for template search. The analysis of the results is also quite shallow. I have never seen a biological structure-prediction paper without a figure that displays 3D structures. The results in Table 2 and 3, while impressive, look almost too impressive. In addition, the results for IgFold look almost unreal as IgFold could be considered the current state-of-the-art for antibody structure prediction. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 1: The contributions are neither significant nor novel. 1: The contributions are neither significant nor novel. NO. 1: strong reject 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 113,
        "instruction": "Thresholded Lexicographic Ordered Multi-Objective Reinforcement Learning: Lexicographic multi-objective problems, which impose  a  lexicographic importance order over the objectives, arise in many real-life scenarios. Existing Reinforcement Learning work  directly addressing lexicographic tasks has been scarce. The few proposed approaches were all noted to be heuristics without theoretical guarantees as the Bellman equation is not applicable to them. Additionally, the practical applicability of these prior approaches also  suffers from various issues such as not being able to reach the goal state.  While some of these issues have been known before, in this work we investigate further shortcomings, and propose fixes for improving practical performance in many cases. We also present a  policy optimization approach using our Lexicographic Projection Optimization (LPO) algorithm that has the potential to address these theoretical and practical concerns. Finally, we demonstrate our proposed algorithms on benchmark problems.",
        "reference": "The authors discuss shortcomings of existing approaches for thresholded lexicographic ordered multiobijective problems in reinforcement learning.  Additionally, the authors provide a policy gradient algorithm that performs well on this class of problems. Strengths: The paper did an excellent job formalizing the problems existing with current methods (especially failure cases), as well as how the author's approach directly addresses these problems. Weaknesses: It's a tired objection, but it's somewhat difficult to situate the author's results with respect to the rest of the literature.  While this is certainly true of any novel work, for which there isn't much else to compare to, then the onus is exceptionally upon the authors to provide compelling results on difficult problems.  While the problems that the authors chose to present were great as educational devices demonstrating the utility of their methods, it's unclear how practically useful the author's algorithm is.  It would be great if the authors could further discuss how their algorithm might fare in a more difficult, less synthetic setting. Clarity and quality: Both extremely high Novelty: Methods in this space are sparse, as the authors discuss, and the authors approach is fairly novel (as a synthesis of several existing ideas in the literature). Reproducibility: The authors provide some code, and promise to release more. A solid submission tackling a fairly new space.  The manuscript could use more extensive, and more practically relevant experiments, though. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 114,
        "instruction": "HOW SAMPLING AFFECTS TRAINING: AN EFFECTIVE SAMPLING THEORY STUDY FOR LONG-TAILED IMAGE CLASSIFICATION: The long-tailed image classification problem has been very challenging for a longtime. Suffered from the unbalanced distribution of categories, many deep vision classification methods perform well in the head classes while poor in the tail ones. This paper proposes an effective sampling theory, attempting to provide a theoretical explanation for the decoupling representation and classifier for long-tailed image classification. To apply the above sampling theory in practice, a general jitter sampling strategy is proposed. Experiments show that variety of long-tailed distribution algorithms exhibit better performance based on the effective sampling theory. The code will be released soon later.",
        "reference": "This paper studies long-tailed image classification problem. The authors identify two key factors that affect the performance of long-tailed image classification: (1) the total number of effective samples  and (2) the effective sample utilization. Based on this finding, the authors proposes an effective sampling theory for theoretical explanation and propose a general jitter sampling strategy for practical implementation. Cons:  This paper is based on the observation that \"data redundancy hurts long-tailed image classification performance\" in Section 3. Is there any statistical or theoretical evidence on this point? This point needs to be better justified with evidence, otherwise, the proposed methods will be vacuous.  The presentation in Section 3 needs to be improved. The first 3 paragraphs in Section 3.1 is hard to follow and the definitions are mixed together. Maybe consider formally defining \"effective sampling\", \"effective samples\", and \"effective sample proportion\". Besides, how do you select substructures s1 s2 from image x1 x2? How are the effective samplings selected from the full dataset? How a1 ... am are computed? Many details are missing when reading this section.  Could you please explain why \"the maximization of the total number of effective samples and the total balance of effective sample utilization between categories can never be achieved theoretically at the same time, due to the existence of sample redundancy.\" above Section 3.2?  The authors need to highlight the difference between the effective sampling method with existing re-sampling and re-weighting methods.  Experiment section is also hard to follow. The authors are suggested to briefly introduce the experiment setup, highlight the key results of the experiment, then provide a discussion and take-home messages for readers. The paper is not well written, both its clarity and quality need to be improved.  The efficient sampling methods sound interesting, but whether the method is novel compared to existing methods is hard to justify. The authors are suggested to compare their method to existing sampling strategies to highlight their contributions. No code to verify the reproducibility. In general, I found the efficient sampling idea interesting. However, the motivation for this efficient sampling is not strong enough, which requires either statical or theoretical evidence. The Paper is hard to read. There are many types and the paper's organization in Sections 3 and 4 might also need to be improved. The method section is not very informative, and details including how effective samples are selected are not carefully introduced. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 115,
        "instruction": "Manipulating Multi-agent Navigation Task via Emergent Communications: Multi-agent corporations struggle to efficiently sustain grounded communications with a specific task goal. Existing approaches are limited in their simple task settings and single-turn communications. This work describes a multi-agent communication scenario via emergent language in a navigation task. This task involves two agents with unequal abilities: the tourist (agent A) who can only observe the surroundings and the guide (agent B) who has a holistic view but does not know the initial position of agent A. They communicate with the emerged language grounded through the environment and a common task goal: to help the tourist find the target place. We release a new dataset of 3000 scenarios that involve multi-agent visual and language navigation. We also seek to address the multi-agent emergent communications by proposing a collaborative learning framework that enables the agents to generate and understand emergent language and solve tasks. The framework is trained with reinforcement learning by maximizing the task success rate in an end-to-end manner. Results show that the proposed framework achieves competing performance in both the accuracy of language understanding and the task success rate. We also discuss the explanations of the emerged language.",
        "reference": "The paper proposes a multi-agent navigation framework as a new benchmark to assess the emergent language that is thus evolved between the agents. The paper is not clearly written and some sections are hard to understand. The related works do capture how the framework proposes new challenges that are not already captured in previous work. Besides a similar work that also uses communication between agents [2] perform a similar analysis and comparisons to that work would be help in highlighting the difference with the proposed framework. Besides the scope of the proposed framework is limited as it can only handle artificial messages that have a much smaller length, vocabulary, and sources of variation. In contrast, previous work have already proposed similar benchmarks that involve real images with natural language descriptions [1]. [1] de Vries et al 2018. Talk the Walk: Navigating New York City through Grounded Dialogue. [2] Singh et al. 2019. Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks. The work is not completely novel as there are other better frameworks present as highlighted above. The paper appears to be a work in progress without much emphasis put on literature survey and comparisons to prior work. The paper also misses out on performing extensive empirical evaluation of the current state-of-the-art methods used for multi-agent navigation tasks. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 1: The contributions are neither significant nor novel. 2: The contributions are only marginally significant or novel. NO. 1: strong reject 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 116,
        "instruction": "Transport with Support: Data-Conditional Diffusion Bridges: The dynamic Schr\u00f6dinger bridge problem provides an appealing setting for posing optimal transport problems as learning non-linear diffusion processes and enables efficient iterative solvers. Recent works have demonstrated state-of-the-art results (eg, in modelling single-cell embryo RNA sequences or sampling from complex posteriors) but are typically limited to learning bridges with only initial and terminal constraints. Our work extends this paradigm by proposing the Iterative Smoothing Bridge (ISB). We combine learning diffusion models with Bayesian filtering and optimal control, allowing for constrained stochastic processes governed by sparse observations at intermediate stages and terminal constraints. We assess the effectiveness of our method on synthetic and real-world data and show that the ISB generalises well to high-dimensional data, is computationally efficient, and provides accurate estimates of the marginals at intermediate and terminal times. ",
        "reference": "In this paper, the authors propose to add sparse constraints to the original Schrodinger bridges through optimal control. Specifically, the paper assumes that there exist some intermediate sparse samples during the diffusion process. By modifying the Iterative Proportional Fitting procedure (IPFP) method with spare intermediate constraints, the Iterative Smoothing Bridge (ISB) method is proposed. Experiments show that the ISB method can help the forward and backward drift functions successfully evolve toward the intermediate observations. Strength  The proposed problem may be important in many different applications with sparse intermediate observations, especially in the medical area. It is reasonable to use L2 loss to handle the forward and backward drifts. Experimental results are convincing.  Weakness  No convergence guarantee of the proposed method. For step 2 and 4, since there only exists sparse intermediate observations, to make the algorithm converge, it seems that a large number of samples is needed to make the method converge. It is reasonable to assume both g and \u03b2 the same in both equation (5) and (6)? I may miss this part in the paper. Empirically, how to define g? The second paragraph of Step 1 in Sec. 3.1 is not very clear. Clarity and Quality  Sec. 3.1 is somewhat ambiguous. The authors need to take more effort on the writing to make the logic much clearer.  The results of IPFP should be included in Fig. 3 for comparison. There are also some typos: In the fourth-to-last line of page 4, bl,\u03d5 should be bl\u22121,\u03d5. In the second line under equation (7), it should be fl\u22121,\u03b8 and gl+1,\u03d5 In the experiment of Single-cell embryo RNA sequences, why is the PCA used? What happens if the experiment is conducted on the original data?    Novelty  The paper proposes an interesting problem, and the solution seems work.  Reproducibility  Without source code, the work is hard to reproduce. Generally, the paper proposes an interesting problem. But the ambiguity in writing and implementation makes it hard to follow. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 117,
        "instruction": "Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning: By driving optimizers to converge to flat minima, sharpness-aware learning algorithms (such as SAM) have shown the power to achieve state-of-art performances. However, these algorithms will generally incur one extra forward-backward propagation at each training iteration, which largely burdens the computation especially for scalable models. To this end, we propose an efficient training scheme, called Randomized Sharpness-Aware Training (RST). Optimizers in RST would perform a Bernoulli trial at each iteration to choose randomly from base algorithms (SGD) and sharpness-aware algorithms (SAM) with a probability arranged by a predefined scheduling function. Due to the mixture of base algorithms, the overall count of propagation pairs could be largely reduced. Also, we give theoretical analysis on the convergence of RST. Then, we empirically study the computation cost and effect of various types of scheduling functions, and give directions on setting appropriate scheduling functions. Further, we extend the RST to a general framework (G-RST), where we can adjust regularization degree on sharpness freely for any scheduling function. We show that G-RST can outperform SAM in most cases while saving 50\\% extra computation cost.\n",
        "reference": "This paper presents new training methods called RST and G-RST to extend the geometry inspired training method SAM and improve its computational efficiency. Based on randomized gradient boosting RST randomly selects between SGD and SAM with probability based on parameterized Bernoulli distribution. The authors explore different parameterization schemes for such a scheduling function and analyze their effect on computations and performance trade-off. The authors also develop RST\u2019s convergence properties for non-convex stochastic cases where the classes of objective functions are smooth and strongly convex. The paper evaluates RST and G-RST on multiple image classification tasks showing that the proposed methods can save 50% computations while performing on-par or even better than the original SAM. The paper is written in a very clear and professional way. The paper is very solid with balanced views and analysis. The paper provides convergence analysis which somehow lacks in SAM literature. The resulting algorithm is simple and effective. I literally didn\u2019t find any flaws in the paper but thought adding more experiments on large scale and different domains could make the paper even stronger. This paper is very well written overall. The paper is also original to some extent in the sense that although there exist SAM variants attempting to improve on SAM\u2019s computational aspect, unlike most of these works this work develops based on matured techniques and optimisation characteristics and provides very well thought-out and reliable results. Highly recommended for interested readers on SAM literature. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 118,
        "instruction": "Self-Supervised Off-Policy Ranking via Crowd Layer: Off-policy evaluation (OPE) aims to estimate the online performance of target policies given dataset collected by some behavioral policies. OPE is crucial in many applications where online policy evaluation is expensive. However, existing OPE methods are far from reliable. Fortunately, in many real-world scenarios, we care only about the ranking of the evaluating policies, rather than their exact online performance. Existing works on off-policy ranking (OPR) adopt a supervised training paradigm, which assumes that there are plenty of deployed policies and the labels of their performance are available. However, this assumption does not apply to most OPE scenarios because collecting such training data might be highly expensive. In this paper, we propose a novel OPR framework called SOCCER, where the existing OPE methods are modeled as workers in a crowdsourcing system. SOCCER can be trained in a self-supervised way as it does not require any ground-truth labels of policies. Moreover, in order to capture the relative discrepancies between policies, we propose a novel transformer-based architecture to learn effective pairwise policy representations. Experimental results show that SOCCER achieves significantly high accuracy in a variety of OPR tasks. Surprisingly, SOCCER even performs better than baselines trained in a supervised way using additional labeled data, which further demonstrates the superiority of SOCCER in OPR tasks.",
        "reference": "This paper proposes a new method for ranking of offline RL policies with off-policy evaluation (OPE). The ranking is produced with a model that 1) learns a pairwise policy representation with a transformer architecture, 2) uses a crowd layer to aggregate OPE scores of other methods. In the experimental results the authors show that their method is able to outperform the other baselines. The ablation studies show the importance of various components of the proposed method. Strengths:  The paper is well written and easy to follow. The details of the method and the experiments are clearly explained. The figures are informative and well explained. The idea of using pairwise policy representation sounds interesting and novel. The experimental results where the proposed method outperformed the other baseline is very encouraging. The experimental results studies the problem with different settings.  Weaknesses:  To my mind, the experimental results are lacking an adequate baseline that is comparable with the proposed method. A baseline would be comparable if it also uses other existing OPE methods to aggregate the results. For example, I can imagine several easy baselines in this case: 1) take the average OPE scores of all methods and produce a ranking out of them, 2) use a majority voting scheme to aggregate the rankings, 3) there are many rank aggregation methods that could be considered, for example, [1].  As the proposed method aggregates the ranks from the existing policies, the computational cost for it is much higher than for any other method and it includes the costs of all other methods. This should also be discussed.  Another limitation in the current experiments is that as far as I understand the method needs to be trained for every new set of policies and the environment from scratch. Then, it is tested on its own training set (no validation or test set). Do I understand the setting correctly? Would the policy representations generalize across different sets of policies? Suppose a new policy is added to the set of policies, can the previous results be re-used? To me, the method would be useful in practice if it can show signs of such generalization. I do not understand why transformers are the best architecture in the given policy representation design. As the states (equation 1) are chosen as just a set (not ordered), what is the advantage of using a transformer which is known to be the best suited for sequential data? Did the authors consider other architectures (possibly simpler, e.g., MLP) here?  Other comments:  Several times the authors mention that in practice finding the best policy is the main objective. In that case, it would be more logical to consider off-policy policy selection (OPS) problem formulation and as the quality metric measure the regret @1. How would the method perform in that case?  I still do not understand the role and training of the \"aggregation token\" very well, maybe this could be explained further. In the first part of section 4.2 the authors say that they \"show how the policy ranking can be reduced to binary classification\". I think this is a common way to approach the ranking problem (but the text sounds now like this is one of the contributions). Some work could be references here, for example, [2].  [1] Fast and Accurate Inference of Plackett\u2013Luce Models. Lucas Maystre, Matthias Grossglauser. NIPS 2015. [2] Preference Learning with Gaussian Processes. Wei Chu, Zoubin Ghahramani. ICML 2005. Clarity: good. Quality: good. Novelty: the paper combines several existing components and aggregates the results of the existing methods, but the idea of the aggregation is reasonably novel. Also, using pairwise instead of direct policy representation sounds novel to me. Reproducibility: the paper provides sufficient details on the methodology as the space permits. Are the authors planning to open source the code? I am leaning toward rejecting this paper mainly because I find the experiments lacking comparable baselines that would benefit from aggregating the results of the existing methods in the same way as the proposed method. Also, I would also like to see some generalization of the method or policy representation to the unseen policies that would make the method scalable to real world problems.  Updated my score after rebuttal in the light of new empirical results. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 119,
        "instruction": "HyPHEN: A Hybrid Packing Method and Optimizations for Homomorphic Encryption-Based Neural Network : Private Inference (PI) enables users to enjoy secure AI inference services while companies comply with regulations. Fully Homomorphic Encryption (FHE) based Convolutional Neural Network (CNN) inference is promising as users can offload the whole computation process to the server while protecting the privacy of sensitive data. Recent advances in AI research have enabled HE-friendly deep CNN like ResNet. However, FHE-based CNN (HCNN) suffers from high computational overhead. \nPrior HCNN approaches rely on dense packing techniques that aggregate as many channels into the ciphertext to reduce element-wise operations like multiplication and bootstrapping.\nHowever, these approaches require performing an excessive amount of homomorphic rotations to accumulate channels and maintain dense data organization, which takes up most of the runtime.\nTo overcome this limitation, we present HyPHEN, a deep HCNN implementation that drastically reduces the number of homomorphic rotations.\nHyPHEN utilizes a novel convolution algorithm, RAConv, utilizing replication-based data organization, which leads to a significant reduction in rotation count.\nFurthermore, we propose hybrid gap packing method for HyPHEN, which gathers sparse convolution results into a dense data organization with a marginal increase in the number of rotations.\nHyPHEN explores the trade-off between the computational costs of rotations and other operations, and finds the optimal point minimizing the execution time.\nWith these optimizations, HyPHEN takes 3.8-4.9$\\times$ less execution time than the state-of-the-art HCNN implementation and brings the runtimes of ResNet inference down to 1.38-14.86s using a GPU-accelerated HEAAN library.",
        "reference": "The paper presents a new HCNN implementation to perform private inference (PI) called HyPHEN. It proposes a replication-based convolution method (RAConv), which is innovatively alternated with the channel-aligned convolution method (CAConv). It also proposes a hybrid packing method, which is a combination of two existing packing methods: duplicate and multiplex packing. Both these methods reduce the overall number of homomorphic rotations and lead to 3-4 times lower latency compared to the baseline, with comparable accuracy based on ResNet architectures with RNS-CKKS implementation on the CIFAR-10 dataset. Strengths:  -\tFigures explaining convolutional operations -\tWell-described problem statement Limitations:  Main contributions of the paper, namely, RAConv and hybrid packing method have limited novelty. RAConv is not entirely different from CAConv and the hybrid packing method is the mixture of two existing packing methods. The resulting HyPHEN implementation can be described as a minor/incremental improvement over existing approaches.   Only the SISO case is considered. There is no mention about other scenarios (e.g., MIMO) and whether the proposed techniques can also be applied to other scenarios.  Results on larger datasets with higher resolution images (say 224 x 224 x 3) must be included. This is critical to determine if the proposed methods can make real-world private inference scenarios feasible. The paper is clear and concise. The quality of the work is reasonable. Experiments are minimal but clear. Novelty is limited. Key training details and code are not available, which makes it difficult to assess reproducibility. The structure of the paper is good and easy to understand. Moreover, obtained results (e.g., speedup in throughput) are better than the state-of-the-art. However,  the novelty is limited and including experiments for complex settings (e.g., larger datasets) would enhance the paper. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 120,
        "instruction": "Causal Inference for Knowledge Graph Completion: The basis of existing knowledge graph completion (KGC) models is to learn the correlations in data, such as the correlation between entities or relations and scores of triplets. Since the world is driven by causality rather than correlation, correlation-driven KGC models are weak in interpretation and suffer from the data bias issue. In this paper, we propose causal KGC models to alleviate the issues by leveraging causal inference framework. Our models are intuitive and interpretable by utilizing causal graphs, controllable by using intervention techniques and model-agnostic. Causal graphs allow us to explain the causal relationships between variables and the data generation process. Under the causal graph, data bias can be seen as confounders. Then we block the bad effect of confounders by intervention operators to mitigate the data bias issue. Due to the difficulty of obtaining randomized data, causal KGC models pose unique challenges for evaluation. Thus, we show a method that makes evaluation feasible. Finally, we show a group theory view for KGC, which is equivalent to the view of causal but further reveals the relationships between causal graphs. Experimental results show that our causal KGC models achieve better performance than traditional KGC models.",
        "reference": "This paper proposed a causal KGC model to alleviate the data bias issues that come from the data collection process, and further proposed an evaluation method to evaluate the causal KGC models on observation datasets. Moreover, the authors also provided a different perspective of viewing KGC from group theory. Numerical experiments showcased that the causal KGC models outperform traditional KGC models in most cases. Strength: It's an interesting ideal of trying to incorporate causal inference techniques into traditional KGC models, and the group theory perspective of KGC is quite novel. Weakness: 1.I agree that in some sense, data bias can be treated as confounders in the causal model, however, I do not think it's appropriate to simply define the confounders as the \"artificially designed confounders\" and \"learnable confounders\" in this paper. Confounders are the type of variables that have causal impact over both treatment and outcome variables, but in the definition of \"artificially designed confounders\" in this paper, they are essentially treated as the variables that are impacted by treatment variables. In other words, the directions of certain arcs in the causal graph changed.2. I appreciate the attempt of explaining KGC from the group theory perspective. However, I cannot see if there's an potential application or new insights from such explanation.  Other comments: 1. Too much identical sentences in Abstract, Introduction and Conclusion.2. In multiple places, you wrote \"In this paper, we propose causal KGC models to alleviate the issues ...\" Here you should be more specific by saying \"data bias issues\".3. You mentioned \"causal graph\" is multiple places of this paper. However, in the causal KGC model, all causal graphs are essentially identical and they have the same topology as the graph (b) in Figure 1. The reason why causal graph in traditional causal inference is important is because, it gives people a direct way of viewing the relationship between variables, which can help people identify what are confounders, what are instrumental variables etc. By introduction causal model into KGC, here different triplets (h,r,t) will still have no connecting arcs between them, except the newly added confounder variables (Zh,Zr,Zt). For that reason, I do not think you should highlight the concept of \"causal graph\" in this paper at all.4. You mentioned a few times that \"the main feature of causal is invariance and symmetry (Arjovsky et al., 2020)\". However, I did not see that from your cited literature. Why is the main feature of causal is symmetry? A causes B means something totally different from B causes A.5. First sentence in Section 2.1, \"let \u03f5 denote(s) and ... denote(s)\".6. Bottom of page 4: \"P(y\u2223h,\u2026,zt) evaluates...\" this is an incomplete sentence.7. Bottom of page 4: \"confounders (is)\" should be \"are\".8. When constructing the \"learnable confounders\", how to learn those neural networks?9. On page 5, zt\u2032 should be zt\u2032.10. Please add references to those ranking metrics MRR and H@N.11. Should not use the word Metric as the name of your evaluation metric.12. In Section 3.3, \"should invariant\" -> \"should be an invariant\".13. In Section 3.3, \"which may (do) not match ...\"14. Last sentence of Section 5.2: I cannot see the reason from the degree of data bias. Please explain more. This paper is poorly written, bears with various typos and incomplete sentences, and the technical details are hard to follow. Overall, I suggest the authors to rewrite the Introduction and Background sections completely, and try not to use repeated sentences in Abstract, Introduction and Conclusion. Based on my comments in the previous sections, even though this paper has certain merit of introducing causal model into KGC, but overall I do not think the contribution is significant enough to make this paper be accepted as the ICLR conference proceeding. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 121,
        "instruction": "Formal Specifications from Natural Language: We study the generalization abilities of language models when translating natural language into formal specifications with complex semantics. In particular, we fine-tune language models on three datasets consisting of English sentences and their corresponding formal representation: 1) regular expressions (regex), frequently used in programming and search; 2) First-order logic (FOL), commonly used in software verification and theorem proving; and 3) linear-time temporal logic (LTL), which forms the basis for industrial hardware specification languages. Our experiments show that, in these diverse domains, the language models maintain their generalization capabilities from pre-trained knowledge of natural language to generalize, e.g., to new variable names or operator descriptions. Additionally, they achieve competitive performance, and even outperform the state-of-the-art for translating into regular expressions, with the benefits of being easy to access, efficient to fine-tune, and without a particular need for domain-specific reasoning.",
        "reference": "The paper focuses on the problem of translating natural language into formal specifications, in particular, regular expressions (regex), first order logic (FOL), and linear temporal logical (LTL). Instead of training deep models from scratch for this problem, the authors investigate whether fine-tuning pretrained language models achieves similar performance, and how well it generalizes to for example new variable names or operator descriptions. The authors consider six datasets in their work, two for each domain: they use existing benchmark datasets Regex-synthetic and Regex-turk, and synthetically generate the datasets for FOL (FOL-mnli and FOL-codesc) and LTL (LTL-pattern and LTL-synthesis) which are also contributions of the paper.  The authors use the T5 model and compare training from scratch to fine-tuning pretrained T5 model for their experiments. On the regex datasets, they report a 6% improvement over SOTA results on both synthetic and turk datasets when fine-tuning the T5 model. On FOL-mnli, they also show a 7% improvement when fine-tuning compared to training from scratch. On LTL datasets, training from scratch and fine-tuning gives similar results. They evaluate generalization by considering nouns that were not present during fine-tuning for regex, new variable names and operator descriptions for LTL. They also conduct cross testing, where they train on one dataset for a domain and evaluate on the other dataset. They show that while the models have acceptable performance for regex, their performance drastically decreases for FOL and LTL datasets. The paper is well written and well motivated, the authors clearly describe the problem and the approach for data generation and evaluation. However, the experimental section is a bit limited and the authors seem to draw strong conclusions from experiments on small datasets (see questions below).  The small version of T5 model is used for the baseline experiments, but the base version is used for fine-tuning experiments. Is the comparison fair? How do the baseline experiments compare to fine-tuning the small version? The generalization test for regex is investigated on a very small set, what can really be inferred from this experiments? Similarly for investigating the generalization to new operator descriptions for LTL, the set of alternatives is very small. For generalization to new operator descriptions for LTL, the performance of the model goes from 100% to 53%. Where does it fail? Overall, the paper and method are clearly written. The paper is of limited technical novelty, however, the authors do investigate an interesting problem empirically. The results are reproducible if the datasets for FOL and LTL are released. The problem investigated is interesting. However, given the questions raised above, and parts of the evaluation being limited, I don't believe the paper is ready as is. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 122,
        "instruction": "DELTA: Diverse Client Sampling for Fasting Federated Learning: Partial client participation has been widely adopted in Federated Learning (FL) to efficiently reduce the communication burden. However, an improper client sampling scheme will select unrepresentative subsets, which will cause a large variance in the model update and slows down the convergence. Existing sampling methods are either biased or can be further improved to accelerate the convergence. In this paper, we propose an unbiased sampling scheme, termed DELTA, to alleviate this problem. In particular, DELTA characterizes the impact of client diversity and local variance and samples the representative clients who carry valuable information for global model updates. Moreover, DELTA is a provably optimal unbiased sampling scheme that minimizes the variance caused by partial client participation and achieves better convergence than other unbiased sampling schemes. We corroborate our results with experiments on both synthetic and real data sets.",
        "reference": "This paper proposes a new method to improve previous (cluster-based) important client sampling methods in federated learning. The new method is motivated by the insight that it would be beneficial to select clients from diverse groups. Convergence analysis are also provided and the authors claim they improve over existing ones. At last, experiments on FEMNIST and CIFAR-10 are provided to validate the performance. Strength  The idea of sampling from diverse gradient groups is a novel idea and seems to be promising. The authors make the proposed sampling algorithm to be unbiased.  Weakness  The writing of this paper should be improved. There are many vague statements. For example, in the introduction, the authors explain why previous works are not good. But all the explanations are just intuitions or conjectures. They cannot be used to support the observations in Figure 2. We do not know whether these conjectures are true or not (e.g. whether cluster-IS really select clients with small gradients and whether this is the core reason causing slow convergence). Also, many mathematical expressions are wrong. For example, in equation 2 and 6, the function f should also have some subscripts because its form changes when we sample different clients. Also, in equation (5), the authors define E|\u2207f(x)|2=E|\u2207f~(x)|2+\u03c72. However, in equation (15), they wrote E|\u2207f~(x)|2=E|\u2207f(x)|2+\u03c72. It is obvious these two equations are conflict with each other. The proof may not be correct. I didn't find any proof details for Theorem D.2, which I suspect is wrong due to the above mistakes. Remark 3.2 (3) is not accurate. \"Chen (2020) ... with additional gradient similarity bound\". This sounds like this paper did not do this and remove this assumption. But in fact, the authors define it in Assumption 3 and also use it. The quality of this paper should be improved. The authors should polish the writing with rigorous statements and check the correctness of the theoretical results. I suspect that this paper has mistakes in the proof. The theoretical results may not be valid. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 123,
        "instruction": "Incremental Predictive Coding: A Parallel and Fully Automatic Learning Algorithm: Neuroscience-inspired models, such as predictive coding, have the potential to play an important role in the future of machine intelligence. However, they are not yet used in industrial applications due to some limitations, such as efficiency. In this work, we propose incremental predictive coding (iPC), a variation of the original model derived from the incremental expectation maximization algorithm,  where every operation can be performed in parallel without external control. We show both theoretically and empirically that iPC is more efficient than the original algorithm by Rao and Ballard, with performances comparable to those of backpropagation in image classification tasks. This work impacts several areas, as it has general applications in computational neuroscience and machine learning, and specific applications in scenarios where automatization and parallelization are important, such as distributed computing and implementations of deep learning models on analog and neuromorphic chips. ",
        "reference": "This paper describes a variant of predictive coding, named incremental predictive coding (iPC), based on incremental EM, which it is argued should be considered a biologically plausible approach to learning in the brain.  The complexity of iPC is considered in relation to back-propagation (BP), and a CPU implementation is provided.  Further, the generalization performance is investigated on a number of datasets, and the algorithm is shown to perform well in comparison to BP and PC. Strengths:  The biological plausibility argument is interesting, and in general the argument is convincing that some form of 'localized EM' algorithm is more plausible than BP or PC alternatives, while retaining convergence and generalization properties. The experimentation convincingly demonstrates that iPC should be considered a viable alternative to BP generally, at least for simple architectures and specialized hardware.  Weaknesses:  I'm mainly concerned about the paper's novelty - essentially, iPC is equivalent to iEM applied to a hierarchical Gaussian model.  The theoretical properties are described elsewhere (e.g. Karimi 2019) and the biological plausibility argument is hard to evaluate, although likely to be worth pursuing further. There is little theoretical novelty, since the time-complexity analysis (Theorem 1) essentially follows simply by definition.  As discussed by the authors, the comparison of training-loss convergence rates in terms of 'non-parallel matrix multiplications' is an interesting result, but this is investigated solely empirically (Fig. 2 right). The clarity, quality and reproducibility are mainly good (I spotted a few typos - for instance, in Eq. 4, the conditional in the second expression should read 'p(x^(l-1) | x^(l))', and the Gaussian formulation in the third expression should include the prior and the x's).  As noted above, the novelty is an issue for me. An interesting investigation of an algorithm that may have relevance in neuroscience, and deserves further attention.  Potentially, the paper may be of interest to those working in neuroscience and optimization. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO.Details Of Ethics Concerns: None. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 124,
        "instruction": "Learning Geometric Representations of\u00a0Interactive Objects: We address the problem of learning geometric representations from observations perceived by an agent operating within an environment and interacting with an external object. To this end, we propose a representation learning framework that extracts the state of both the agent and the object from unstructured observations of arbitrary nature (e.g., images). Supervision comes from the performed actions alone, while the dynamics of the object is assumed to be unknown. We provide a theoretical foundation and formally prove that an ideal learner is guaranteed to infer an isometric representation, disentangling the agent from the object. Finally, we investigate empirically our framework on a variety of scenarios. Results show that our model reliably infers the correct representation and outperforms vision-based approaches such as a state-of-the-art keypoint extractor.   \n",
        "reference": "The paper addresses the task of estimating the states (translation) of the agent and the object that it interacts with using image evidence. Authors assume that the states and properties of the agent and the object are unknown but the actions are known. The key contributions of the work is the formulation that allows decoupling of agent and object features in the latent space. Strengths:  The paper is well written and easy to follow. I like the theoretical framework proposed by the authors and the subsequent learning framework built on top. Proposed experiments confirm the hypothesis put forward by the authors.  Weaknesses:  Definition 1: \\psi is composed of (\\psi_{int}, \\psi_{ext}). It is clear that \\psi_{int} is equivariant, i.e. z'{int} = z{int} + a But \\psi_{ext} doesn't appear to be equivariant. z'{ext} != z{ext} + a As the state of the object after the contact would depend on the shapes of the object and angle of incidence. How is \\psi_{ext}) and equivariant function. In practice authors move the object at a random location, for the first experiment. Specific to this experiment: Doesn't moving the object to a random location greatly simplify the task, as now we just need to disentangle agent (whose movement follows action) and object (which is moved randomly)? More challenging is the real setting when both agent and object move according to the applied action.  Based on previous comment, Theorem 4.1 seems to apply to \\psi_{int} only and not \\psi_{ext}. This can also be seen in the loss terms. L_{int} only makes \\psi_{int} equivariant and L_{ext}+L_{cont} just enforce condition 3 on \\psi_{ext}. There is no term ensuring linearity on \\psi_{ext}, which is also not true in the real word. What makes z_{ext} and z'_{ext} consistent with each other under action a.  The idea behind eq. 7,8 is to train a proxy encoder \\psi_{cont} using contrastive learning and use it's prediction to generate pseudo labels, by partitioning the dataset, to optimise eq. 6. Authors do so because directly optimising eq. 6, collapses the optimisation to one of the terms (L-, L+). Why can't we enforce InfoNCE directly on z_{int} to regularise the space and spread samples in the latent space?  According to theorem 4.1, if our model has learnt to satisfy the 3 conditions then: \\forall s_i \\in D_{test} \\psi(w(s_i)) - s_i = h Why can't we evaluate how much does our model deviate from this constant h. L_{test} = var(\\psi(w(s)) - s). This value should ideally be 0. Why is current metric better?   Clarifications:  Definition 1: Does the equivariance constrain make the latent representation z_{int} linear? Does this affect/ limit the properties that the model can capture? Is linear representation space sufficient for complex properties? In the current formulation the model is reasoning about the translation of the agent and the object, which is linear. Thus enforcing linearity on the latent space makes sense. What if the model has to learn non-linear properties like rotations? Will it affect the quality of performance?  Minor:  Caption in Fig. 1: Text uses 's' to describe states of the agent and the object, but the figure caption uses 'z'. 'z' is later used as representation of observation and not the state itself. Why consider only translation? Rigid objects can rotate too. Modelling rotations from image observations can be challenging. Sec 3: What is n? Is n=3? Translations in 3D Euclidean space? Assumption 3.1: (.) is an operator that applies actions 'a' on state 's' to give the new state s' At the end of paragraph a is applied, using (.) operator, to the observation o. What is the domain of operator (.), state or observation? Maybe out of scope for this work but it should be mentioned that state change can happen with a previous contact/interaction. Eg: if you push an object (like a ball) it can continue to move or stop when the contact is removed. The paper is well written and I enjoyed the problem setting and the accompanying formulation. The experiments support the claims made in the paper. I'm not an expert in this domain, so I didn't fully follow why is \\psi equivariant and not just \\psi_{int}. This is important as Theorem 4.1 (the main contribution) requires \\psi to be equivariant. See weakness section for details. Apart from this I like the work. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO.Details Of Ethics Concerns: I do not see immediate ethical concerns arising from this work. 8: accept, good paper 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 125,
        "instruction": "Improve distance metric learning by learning positions of class centers: Deep metric learning aims at learning a deep neural network by letting similar samples have small distances while dissimilar samples have large distances. To achieve this goal, the current DML algorithms mainly focus on pulling similar samples in each class as closely as possible. However, pulling similar samples only considers the local distribution of the data samples and ignores the global distribution of the data set, i.e., the center positions of different classes. The global distribution helps the distance metric learning. For example, expanding the distance between centers can increase the discriminant ability of the extracted features. However, how to increase the distance between centers is a challenging task. In this paper, we design a genius function named the skewed mean function, which only considers the most considerable distances of a set of samples. So maximizing the value of the skewed mean function can make the largest distance larger. We also prove that the current energy functions used for uniformity regularization on centers are special cases of our skewed mean function. At last, we conduct extensive experiments to illustrate the superiority of our methods.",
        "reference": "This paper presents a method to expand the distance between centers of different classes. However, the contribution is not clear, and the article is an unfinished work. In section 2, nothing is revisited but the title. The writing is terrible. There are too many spelling and grammatical errors in Section 3. In general, this paper is far from acceptance. Strength: the studied problem is interesting. Weaknesses: the contribution is not clear, and the article is an unfinished work. In section 2, nothing is revisited but the title. The writing is terrible. There are too many spelling and grammatical errors in Section 3. In general, this paper is far from acceptance. Terrible. As I stated above. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 1: strong reject 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 126,
        "instruction": "The guide and the explorer: smart agents for resource-limited iterated batch reinforcement learning: Iterated (a.k.a growing) batch reinforcement learning (RL) is a growing subfield fueled by the demand from systems engineers for intelligent control solutions that they can apply within their technical and organizational constraints. Model-based RL (MBRL) suits this scenario well for its sample efficiency and modularity. Recent MBRL techniques combine efficient neural system models with classical planning (like model predictive control; MPC). In this paper we add two components to this classical setup. The first is a Dyna-style policy learned on the system model using model-free techniques. We call it the guide since it guides the planner. The second component is the explorer, a strategy to expand the limited knowledge of the guide during planning. Through a rigorous ablation study we show that combination of these two ingredients is crucial for optimal performance and better data efficiency. We apply this approach with an off-policy guide and a heating explorer to improve the state of the art of benchmark systems addressing both discrete and continuous action spaces.",
        "reference": "This algorithm proposes an algorithm for iterated batch reinforcement learning. The algorithm uses model-free RL to learn a guide policy, and then uses decision-time planning to improve the policy. The decision-time planning uses some exploration method and a rollout procedure to get a good action. Strength: The paper is overall well-written and studies an important problem. Weakness: The paper is very similar to the AlphaZero algorithm by Silver et al. (2017), therefore, I don't think this paper has enough novelty. The guide policy is similar to the prior policy in AlphaZero as the authors mentioned in Section 3.2. Instead of learning the guide during MCTS training, this paper learns the guide using simpler model-free RL algorithms. The decision-time planning is also similar to AlphaZero. In fact, AlphaZero uses a more advanced MCTS technique whereas this paper uses a simple rollout procedure and chooses the action that leads to the best return. In other words, this paper only uses one step of MCTS. Therefore, I think this paper is a simple modification of the AlphaZero algorithm and thus it is not novel enough as an ICLR paper. The paper is overall clearly written and easy to follow. The experiments are conducted using simple agents in simple environments, so I think it is likely that the experiments can be reproduced. The paper lacks novelty since it is very similar to AlphaZero, and in fact, it is almost a simplified version of AlphaZero. The experiments are performed only in simple environments. Therefore, I don't think this paper makes a significant contribution to the community. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 127,
        "instruction": "How (Un)Fair is Text Summarization?: Creating a good summary requires carefully choosing details from the original text to accurately represent it in a limited space. If a summary contains biased information about a group, it risks passing this bias off to readers as fact. These risks increase if we consider not just one biased summary, but rather a biased summarization algorithm. Despite this, little work has measured whether these summarizers demonstrate biased performance. Rather, most work in summarization focuses on improving performance, ignoring questions of bias. In this paper we demonstrate that automatic summarizers both amplify and introduce bias towards information about under-represented groups. Additionally, we show that summarizers are highly sensitive to document structure, making the summaries they generate unstable under changes that are semantically meaningless to humans, which poses a further fairness risk. Given these results, and the large scale potential for harm presented by biased summarization, we recommend that bias analysis be performed and reported on summarizers to ensure that new automatic summarization methods do not introduce bias to the summaries they generate.",
        "reference": "This paper investigates the presence bias introduced by text summarization models. The authors propose to measure bias in two dimensions: content and structure. They define content bias as tendency to mention a specific group (e.g., gender, religion, etc) in a text, whereas structural bias refers to bias as a result of structural features of the text (position, sentiment, and style). In this paper, they limit the scope of investigation to one type of content bias (underrepresentation) and two types of structure bias (position and style). To measure content bias, a representation score R(T, g) is proposed to gauge the proximity of a group g to a text T. For position bias, the content of model-generated summaries and reference summaries is compared according to its position in the input documents. Finally, for style bias, the authors measure the impact of paraphrasing sentences in the original document, under the assumption that the same content should be selected regardless of the style of writing. Those criteria were applied to a synthetic dataset generated by GPT-2 and the CNN/DM news summarization dataset. Based on the results, the authors conclude that summarization models exhibit preference for certain groups over others, amplify patterns of bias, and demonstrate sensitivity to the structure of articles. Strengths:  The paper addresses an important and underexplored topic in the automatic summarization. The experiments show that under similar conditions of positioning in the source documents, summarization systems demonstrate preference for groups, for instance, by amplifying content related to men (versus women).  Weaknesses:  The content bias analysis is based on a synthetic dataset generated by GPT-2, so that the summary inference was performed on out-of-domain data. If summarization models are sensitive to paraphrasing and structural features, then synthetic documents may also exhibit artifacts that influence the results. It would be important to perform a similar analysis on selected documents from CNN/DM. Alternatively, a human evaluation experiment using the synthetic data could provide more evidence that the bias is really introduced by the summarization models. The position bias analysis, the authors state that \"clearly amplified by MatchSum, PreSumm, and Azure, as shown in Fig. 4b.\" However, it is not clear if the observed pattern is caused by model preference or simply by truncation of inputs. If we observe the figure in Appendix E, the distribution for PEGASUS and BART are more similar to the reference summaries distribution. In contrast, MatchSum, Presumm, and Azure decrease the frequency sharply between 0.4 and 0.6. Interestingly, Presumm and MatchSum use the same backbone model with a maximum input length of 512 tokens, whereas PEGASUS and BART support inputs of 1024 tokens, which might explain their capacity to select more content from the later positions of the articles. For reference, CNN/DM input documents have 766 words on average (\"Extractive Summarization as Text Matching\", Zhong et al., 2020). Do the quantile calculation in the figures take into account this truncation effect? No details or citation is provided for the paraphrasing model. Just the link for its checkpoint at https://huggingface.co/tuner007/pegasus_paraphrase. Authors mention that they \"conduct manual analysis on a randomly selected subset of the article\" but no further details are provided. Experiments are based only on CNN/DM, which is well known for its lead bias for important content. Experiments on additional datasets could make the claims stronger.  Minor comments:  Typo in page 6: \"Overall, t hese results show a pattern of unpredictability...\" The work well-motivated and addresses a relevant problem in language generation. However, the methods lack clarity in some respects (like the paraphrasing approach) and some conclusions are not strongly supported by empirical evidence (see weaknesses 1 and 2 above). The authors mention the release of dataset and code, which helps in the reproducibility. The claims that summarization models exhibit content bias are weakly supported by the experiments. In terms of structural bias, this paper finds that summarization models trained on news articles have lead bias, which is a well-known fact in the literature. For the lack of novelty and solid evidence for the claims, the recommendation is for the paper rejection. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 128,
        "instruction": "Simulating Task-Free Continual Learning Streams From Existing Datasets: Task-free continual learning is the subfield of machine learning that focuses on learning online from a stream whose distribution changes continuously over time. However, previous works evaluate task-free continual learning using streams with distributions that change only at a few distinct points in time. In order to address the discrepancy between the definition and evaluation of task-free continual learning, we propose a principled algorithm that can permute any labeled dataset into a stream that is continuously nonstationary. We empirically show that the streams generated by our algorithm are less structured than the ones conventionally used in the literature. Moreover, we use our simulated task-free streams to benchmark multiple methods applicable to the task-free setting. We hope that our work will make it more likely that task-free continual learning methods are able to better generalize to real-world problems.",
        "reference": "This paper presents a framework for task-free continual learning. The framework consists of a method for converting any dataset to a task-free continual learning problem where information on the class is not required and is not explicit, but rather examples from different classes can be observed at any step of the learning sequence. This method relies on assigning distributions to each class so that these distributions can be later used to select examples from a particular class, or set of classes, for a particular batch for learning. To assign these distributions, the authors propose to sample mean and standard deviations from a Beta distribution. Experiments are run over four different benchmark datasets and a range of baseline continual learning methods. Strengths:   The paper pursues an interesting research direction which considers continual learning without assumptions on how classes are organised into tasks. Task information is not required for learning nor for inference purposes. This is certainly a more realistic scenario of continual learning, which as the authors exemplify may be applicable to a variety of domains. The paper is clearly written and well presented.  Weaknesses:  Although I appreciate the relevance of pursuing real task-free continual learning, I find the proposed method quite straightforward and not fully sound. For example, although you mention some reasons to select the Beta distribution over some other possible distributions for sampling on each class, these arguments seem quite trivial and do not necessarily imply that other distributions are not entirely applicable. For a framework such as the intended one, I would expect to see a thorough analysis of how different class distribution assignments work for the purpose of task-free continual learning. Another aspect that impacts the soundness of the proposed method is the amount of explanations on section 3.2 around maximum entropy principle and the beta distribution itself, and the lack of intuition about how class distribution assignment actually works.  A second weakness that I see in this paper is regarding the experiments: from Figure 1, it seems that the proposed method tends to allocate examples from the same class quite contiguously; with the small batch sizes used in the experiments, it is very likely that examples from a particular class will be all observed on the same batch. I would expect to see more systematic experiments that clearly show that the task-free approach works for scenarios where examples from the same class are observed at just a few, some or many steps over the learning sequence. I would also like to see more systematically how important hyperparameters such as the batch size affect task-free continual learning. Finally, beyond the definitions of task-free continual learning and the reported experiments, I consider the contribution of this paper quite limited. For an ICLR paper, I would that the proposed framework would also include a method that could leverage better this task-free scenario. The paper is clearly written and well presented. The quality can be substantially improved in the sense of the provided intuitions, the explanations and the contribution itself (please see 'Strengths and Weaknesses' section of this review). The novelty relies on the definition of task-free continual learning, but the proposed framework lacks novelty in terms of a method that would work well for this scenario - which is not proposed at all. The reproducibility is far (a researcher working in this area could eventually reproduce the results). As noted in the \"Strengths and Weaknesses\" section, although I appreciate the definition of the task-free continual learning scenario and the attempts at experimentally measuring how different baselines work in this setting, I consider that the contribution of this paper lacks novelty and is not sufficient for ICLR since there is no method being proposed for exploiting the task-free learning scenario. Therefore, my recommendation for this paper is a rejection, but I encourage the authors to improve their contribution by incorporating a method for task-free learning as part of their framework. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 1: The contributions are neither significant nor novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 129,
        "instruction": "A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration: The cross-entropy objective has proved to be an all-purpose training objective for autoregressive language models (LMs). However, without distinguishing problematic tokens, LMs trained using cross-entropy exhibit text degeneration problems. To address this, unlikelihood training has been proposed to reduce the probability of unlikely tokens predicted by LMs. But unlikelihood does not explicitly consider the relationship between the label tokens and unlikely token candidates, thus showing marginal improvements in degeneration. We propose a new contrastive token learning objective that inherits the advantages of cross-entropy and unlikelihood training and avoids their limitations. The key idea is to teach a LM to generate high probabilities for label tokens and low probabilities for negative candidates. Comprehensive experiments on language modeling and open-domain dialogue generation tasks show that the proposed contrastive token objective yields much less repetitive texts, with a higher generation quality than baseline approaches, achieving the new state-of-the-art performance on text degeneration.",
        "reference": "This paper proposes a contrastive learning method to balance the learning of positive and negative tokens in text generation tasks (e.g., language modeling and open-domain dialogue generation tasks). Strength   The paper is easy to follow and the idea is intuitive. Several case studies are given which are encouraged.  Weaknesses  The novelty is rather thin. This paper is an incremental work on UL: 1) The core idea that penalizes the previously generated tokens has been proposed by UL; 2) I totally disagree with the author's claim that Comparing Eq.(5) to Eq. (4), we see that UL only considers the probabilities of negative tokens, Equation 4 of the UL paper clearly shows that UL has jointly considered the probabilities of positive and negative tokens (i.e., a likelihood term for a positive token and an unlikelihood term for negative tokens). However, the authors try to hide this important detail and only write the likelihood term of negative tokens in Equation 4 of the current paper. The experiment is insufficient. The authors have cited many machine translation papers and also state that We performed experiments on fine-tuning LMs for reducing their repetition rates, which can be beneficial for related tasks such as abstractive summarization, machine translation, and image captioning. Therefore, why not simply perform the proposed method in these tasks? PPL is not a reliable metric for evaluating text generation tasks.  The metrics (especially the recently proposed neural metrics ) in abstractive summarization, machine translation, and image captioning are more competent to give a more reasonable evaluation, which can make the proposed method more convincing. A small suggestion: if you think your proposed method is simple and still can be accepted by a top-tier conference, the method has to be very powerful or very general. Unfortunately, the proposed method does not show its effectiveness. Clarity  This paper is very clear.   Quality This paper does not meet the bar of ICLR. The method is not novel and the experiments are pretty limited.   Novelty Thin. Unlikelihood learning has made the most contributions.   Reproducibility Good. The authors have uploaded the code. Given the thin novelty and insufficient experiments, I suggest rejecting this paper. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 130,
        "instruction": "Enriching Online Knowledge Distillation with Specialist Ensemble: Online Knowledge Distillation (KD) has an advantage over traditional KD works in that it removes the necessity for a pre-trained teacher. Indeed, an ensemble of small teachers has become typical guidance for a student's learning trajectory. Previous works emphasized diversity to create helpful ensemble knowledge and further argued that the size of diversity should be significant to prevent homogenization. This paper proposes a well-founded online KD framework with naturally derived specialists. In supervised learning, the parameters of a classifier are optimized by stochastic gradient descent based on a training dataset distribution. If the training dataset is shifted, the optimal point and corresponding parameters change accordingly, which is natural and explicit.\nWe first introduce a label prior shift to induce evident diversity among the same teachers, which assigns a skewed label distribution to each teacher and simultaneously specializes them through importance sampling. Compared to previous works, our specialization achieves the highest level of diversity and maintains it throughout training. Second, we propose a new aggregation that uses post-compensation in specialist outputs and conventional model averaging. The aggregation empirically exhibits the advantage of ensemble calibration even if applied to previous diversity-eliciting methods. Finally, through extensive experiments, we demonstrate the efficacy of our framework on top-1 error rate, negative log-likelihood, and notably expected calibration error.",
        "reference": "The authors propose to diversify the models in an ensemble forming the teachers in distillation. The specific setup the authors consider is  on-line distillation (where the teachers are trained in parallel with the student) and peer-based (where the models share parts of their weights).  The proposed approach to achieve the aforementioned diversification of the teachers is based on varying the prior distributions of the labels in the train data used for training each teacher. Then, as an efficient approach to this, importance sampling is employed to avoid sampling from the data separately for each teacher. Then, before averaging the teacher outputs for the student supervision, the \"Post-Compensated Softmax\" loss correction is employed from a previous work. The comparison against selected baselines shows improvements. Ablations are provided where the authors measure the effect of the teacher's diversity and the number of teachers within the proposed framework. Strenghs:  The paper is mostly clear.  The Figure 1 is a very nice summary of the work.  Weaknesses:  One limitation is that the setup seems very specialized (on-line distillation, peer networks). I don\u2019t see why the technique is specific for that setup - diversifying the teachers (which is the main idea behind the authors\u2019 method) could be well applied to off-line distillation or non-peer networks. The baselines are not explained. It is not clear why these are chosen over other options (there are a lot of distillation methods one could compare to). The ablation section is very useful to have, but I would like to see ablations for why using each of the steps suggested by the authors\u2019 is useful. For example, does PC-Softmax  help? Is importance sampling actually good compared to sampling the data instead? (checking this even on a small dataset would be useful) What about the effect of the peer network choice in the light of your method, as opposed to using completely separate networks? When averaging the teachers, why not weigh them differently depending on what labels they were trained on? Since each teacher focuses on a different set of labels, they should be experts with respect to such labels. It seems that averaging the teacher's predictions is naive and ignores useful information about the teachers. As an application of label prior shift for diversifying the teachers, Why not try it in non-on-line distillation? Is there anything special about on-line distillation that diversifying the teachers is particularly interesting compared to off-line distillation? The paper is mostly clear and the method is explained well so it should be reproducible. The novelty is somewhat limited as methods from previous works are combined without a clear explanation why using them is the best option. Some specific questions:  What is the motivation for the label prior shift for training models in an ensemble? MoEs are mentioned, but I missed an explanation if that particular technique was employed there and what its success was.  \u201cnotably expected calibration error\u201d \u2013 why is this finding notable? The use of the methods described by the authors looks interesting and promising. I suggest conducting more ablations and adding explanations for the decisions to strengthen the paper before resubmission. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 131,
        "instruction": "Improved Gradient Descent Optimization Algorithm based on Inverse Model-Parameter Difference: A majority of deep learning models implement first-order optimization algorithms like the stochastic gradient descent (SGD) or its adaptive variants for training large neural networks. However, slow convergence due to complicated geometry of the loss function is one of the major challenges faced by the SGD. The currently popular optimization algorithms incorporate an accumulation of past gradients to improve the gradient descent convergence via either the accelerated gradient scheme (including Momentum, NAG, etc.) or the adaptive learning-rate scheme (including Adam, AdaGrad, etc.). Despite their general popularity, these algorithms often display suboptimal convergence owing to extreme scaling of the learning-rate due to the accumulation of past gradients. In this paper, a novel approach to gradient descent optimization is proposed which utilizes the difference in the model-parameter values from the preceding iterations to adjust the learning-rate of the algorithm. More specifically, the learning-rate for each model-parameter is adapted inversely proportional to the displacement of the model-parameter from the previous iterations. As the algorithm utilizes the displacement of model-parameters, poor convergence caused due to the accumulation of past gradients is avoided. A convergence analysis based on the regret bound approach is performed and the theoretical bounds for a stable convergence are determined. An Empirical analysis evaluates the proposed algorithm applied on the CIFAR 10/100 and the ImageNet datasets and compares it with the currently popular optimizers. The experimental results demonstrate that the proposed algorithm shows better performance than the popular optimization algorithms.",
        "reference": "This paper proposes an adaptive learning rate method to improved the stochastic gradient descent algorithm by leveraging the difference of two consecutive iterations. More concretely, the authors proposes that the learning rate of each parameter should be inversely proportional to the difference between the current and the previous iteration of the this parameter. The authors claim the such an updating scheme, when combined with SGD, can converge under mild assumptions. Empirical studies also show that it can achieve lower training loss and better accuracy on CIFAR-10/100 and Tiny-ImageNet. Strength:  The idea is simple and easy to understand. It is also easy to implement.  Weaknesses:  There are a bunch of notation errors/overloading in the paper that make it hard to follow. Examples:  1.1. Vectors should be in RL rather than R, e.g.,  p,q\u2208R, \u03b8\u2208R, etc.  1.2. The equation immediately after eqn (7) should be J(\u03b8\u2217)=arg\u2061min\u03b8J(\u03b8).   1.3. While eqn (8) uses Mk, the following sentence uses M. And there is no definition about \u03bck in the next sentence.   1.4. \\beta is used for the Lipschitz constant above eqn (8), and then overloaded as the maximum eigenvalue of the input correlation matrix in eqn(9).  Given the the notation systems are so messy, it is hard to believe the proof is correct.  The algorithm involves parameters K and \u03bc0, but there is no guidance on how to choose them.  The empirical studies are not convincing either.  4.1. In Figure 2, the proposed algorithm converges obviously slower than quite a few other algorithms. Although it seems to converge to a point with lower loss, it is unclear why it is the case. Also, the proposed algorithm is unstable compared to others, as its curve is bumpy.  4.2. The algorithm is only evaluated on three small image datasets. It is unclear if the conclusion can generalize to other domains/networks. The clarity and quality of the paper is low, as the notation system is messy. Novelty is limited as the idea of adaptive learning is widely explored. The idea is easy to implement, so it should be reproducible. The paper is not well written that it is hard to tell if the proof is correct. Besides, the idea is not very novel and the empirical study is not comprehensive either, making it less convincing that the proposed algorithm is effective. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 132,
        "instruction": "Variational Learning ISTA: Compressed sensing combines the power of convex optimization techniques with a sparsity inducing prior on the signal space to solve an underdetermined system of equations. For many problems, the sparsifying dictionary is not directly given, nor its existence can be assumed. Besides, the sensing matrix can change across different scenarios. Addressing these issues requires solving a sparse representation learning problem, namely dictionary learning, taking into account the epistemic uncertainty on the learned dictionaries and, finally, jointly learning sparse representations and reconstructions under varying sensing matrix conditions.\nWe propose a variant of the LISTA architecture that incorporates the sensing matrix into the architecture. In particular, we propose to learn a distribution over dictionaries via a variational approach, dubbed \\ac{VLISTA}, which approximates a posterior distribution over the dictionaries as part of an unfolded LISTA-based recovery network. Such a variational posterior distribution is updated after each iteration, and thereby adapts the dictionary according to the optimization dynamics. As a result, \\ac{VLISTA} provides a probabilistic way to jointly learn the dictionary distribution and the reconstruction algorithm with varying sensing matrices. We provide theoretical and experimental support for our architecture and show that it learns calibrated uncertainties.",
        "reference": "The authors present two unrolled ISTA-type algorithms for compressed sensing. The first approach is amenable to adaptive measurements matrix while learning the sparsifying dictionary per layer. The second approach called as VLISTA places a probability distribution the dictionaries. This provides a probabilistic way to learn dictionaries which could be useful in out of distribution (OOD) detection problems. Strengths  The paper is well-written and easy to follow. The main ideas are well explained. Allowing to learn the dictionary in a measurement adaptive environment is quite interesting. Also, the probabilistic formulation of the problem, which leads to VLISTA, uses ideas of Bayesian deep learning in the unrolled optimization framework, and thus it has its own interest.  Weaknesses  I understand that assigning Gaussian distributions in the prior and posterior simplifies things a lot regarding the implementation of the algorithm. However, isn't quite restrictive in practice? I wonder if other distributions can be modeled in that framework. E.g., using hierarchical models that encode heavy-tailed distributions. In the experiments,  Section 4.1, it seems that A-DLISTA outperforms ISTA and LISTA. However, it is not clear what is the measurement matrix that is given as input to these algorithms. Does LISTA know that the measurement matrix changes over time? It is not well explained why VLISTA works so much worse than A-DLISTA. In the OOD experiment, there is no comparison with any other competing method. Why BSC algorithm is not included in the experiments? In Theorem 1, since \u03b8t,\u03b3t are learned by the algorithm. Hence for the sufficient condition given in (7)  to be satisfied, it is important \u03b8t, and \u03b3t converge as t ( here denotes the number of layers) grows. Can the authors provide some insight regarding the convergence of these parameters? Moreover, it would be interesting for the authors to show experimentally that this condition is satisfied at a certain number of layers T.  Minor  There seems to be an issue with statement 1 in Theorem 1. The conclusion should be supp(zt)\u2286supp(z\u2217).  It seems there is a typo there. The main ideas are clearly presented in the paper. Learning dictionaries in an adaptive measurements environment seems to be novel (though I might miss works that might have recently been published on that topic). The authors present two approaches for learning sparse representations and dictionaries under the unrolled optimization framework. The ideas are simple but interesting. The authors provide a sufficient condition for the algorithm to converge to the correct support and provide an error bound. Experimental results demonstrate the efficiency of the proposed algorithms on simulated data and on MNIST dataset. The paper could be significantly improved if the authors could better motivate the selection of the Gaussian distributions for the priors and posteriors and provide more insight into how restrictive these assumptions are in practical settings. The experimental section could also be improved by providing experimental results on datasets such as CIFAR dataset and more extensive results of the probabilistic method (VLISTA).   Post-rebuttal update: I wanted to thank the authors for their time and effort in addressing the reviewers' comments. I am pretty satisfied with the responses, and I  appreciate the changes they made to the paper. I  thus raise my score to 6. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 133,
        "instruction": "Moment Distributionally Robust Probabilistic Supervised Learning: Probabilistic supervised learning assumes the groundtruth itself is a distribution instead of a single label, as in classic settings. Common approaches learn with a proper composite loss and obtain probability estimates via an invertible link function. Typical links such as the softmax yield restrictive and problematic uncertainty certificates. In this paper, we propose to make direct prediction of conditional label distributions from first principles in distributionally robust optimization based on an ambiguity set defined by feature moment divergence. We derive its generalization bounds under mild assumptions. We illustrate how to manipulate penalties for underestimation and overestimation. Our method can be easily incorporated into neural networks for end-to-end representation learning. Experimental results on datasets with probabilistic labels illustrate the flexibility, effectiveness, and efficiency of this learning paradigm.",
        "reference": "The paper proposes an algorithm to compute a DRO objective where the uncertainty set is defined with deviations in moments of the representation. The algorithm allows one to compute the minimax DRO loss using a primal-dual formulation and reasoning about the stationary points. The derivation in the paper is theoretically interesting. My main concern is this: While the paper guarantees \u03b8 is learned well with \u03d5 fixed, it is unclear to me why I can learn a representation with the given objective. The reason is that changing \u03d5 changes the uncertainty set for the setup DRO problem! Without the ability to use a fixed uncertainty set from some known restriction, what is the value of the proposed method? The experiments seem to support this concern as the authors say \"All the methods become vulnerable for large pnoise possibly because of the backbone neural network model.\" Weaknesses/Questions:  Both corrollary 2 and theorem 3.2, along with the stated uncertainty set are from ((Li et al., 2022). Why do the authors claim \" We propose a distributionally robust probabilistic supervised learning method\"? The main contribution seems to be the algorithm. What can go wrong when let \u03b2=0? Theoretically, it seems like when learning \u03d5, it seems possible to end up in a minima where \u03d5=0. Can the authors explain why this won't happen when the objective is optimized? Is mixing log-likelihood and brier score used in an application? Can the authors point to a case? The experiments in table 1 do not seem to favor the proposed method much; softmax is better or similar. The paper is cleanly written by the novelty seems to be the algorithm to compute the DRO alone, which I believe is a useful thing. While I think the ability to compute minimax loss is great, I am unclear about some details and what the value of the paper is. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 3: reject, not good enough 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 134,
        "instruction": "CLEP: Exploiting Edge Partitioning for Graph Contrastive Learning: Generative and contrastive are two fundamental unsupervised approaches to model graph information. The graph generative models extract intra-graph information whereas the graph contrastive learning methods focus on inter-graph information. Combining these complementary sources of information can potentially enhance the expressiveness of graph representations, which, nevertheless, is underinvestigated by existing methods. In this work, we introduce a probabilistic framework called contrastive learning with edge partitioning (CLEP) that integrates generative modeling and graph contrastive learning. CLEP models edge generation by cumulative latent node interactions over multiple mutually independent hidden communities. Inspired by the ``assembly'' behavior of communities in graph generation, CEGCL learns community-specific graph embeddings and assemble them together to represent the entire graph, which are further used to predict the graph's identity via a contrastive objective. To relate each embedding to one hidden community, we define a set of community-specific weighted edges for node feature aggregation by partitioning the observed edges according to the latent node interactions associated with the corresponding hidden community. With these unique designs, CLEP is able to model the statistical dependency among hidden communities, graph structures as well as the identity of each graph; it can also be trained end-to-end via variational inference. We evaluate CLEP on real-world benchmarks under self-supervised and semi-supervised settings and achieve promising results, which demostrate the effectiveness of our method. Various exploratory studies are also conducted to highlight the characteristics of the inferred hidden communities and the potential benefits they bring to representation learning.",
        "reference": "In this paper, author introduce a probabilistic framework called contrastive learning with edge partitioning (CLEP) that integrates generative modeling and graph contrastive learning. CLEP models edge generation by cumulative latent node interactions over multiple mutually independent hidden communities. Weaknesses  I don't understand why the author need to use generative model in contrastive learning (CL) and the ELBO that the author propsed seems unrelated to the CL. Can the author explain that.  The pipline of the over framework is hard to follow. How to generate the embeddings for downstream tasks?  The authors claim the community information is important in their case. why is that? How do the author verify that the improvement is comes from adding community information.   To introduce the community information, this work is similar to clustering base GCL.  Related work [1,2] should be compared.   [1] Prototypical Graph Contrastive Learning [2] Graph InfoClust: Leveraging cluster-level node information for unsupervised graph representation learning Clarity: Fair Quality: Fair Novelty: Limited Reproducibility: Not Sure It quite hard to follow the author's idea, It is seems not significant to introducing the generative model in the context of CL. Overall pipline is not clear. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 135,
        "instruction": "Meta-Learning the Inductive Biases of Simple Neural Circuits: Animals receive noisy and incomplete information, from which we must learn how to react in novel situations. A fundamental problem is that training data is always finite, making it unclear how to generalise to unseen data. But, animals do react appropriately to unseen data, wielding Occam's razor to select a parsimonious explanation of the observations. How they do this is called their inductive bias, and it is implicitly built into the operation of animals' neural circuits. This relationship between an observed circuit and its inductive bias is a useful explanatory window for neuroscience, allowing design choices to be understood normatively. However, it is generally very difficult to map circuit structure to inductive bias. In this work we present a neural network tool to bridge this gap. The tool allows us to meta-learn the inductive bias of neural circuits by learning functions that a neural circuit finds easy to generalise, since easy-to-generalise functions are exactly those the circuit chooses to explain incomplete data. We show that in systems where the inductive bias is known analytically, i.e. linear and kernel regression, our tool recovers it. Then, we show it is able to flexibly extract inductive biases from differentiable circuits, including spiking neural networks. This illustrates the intended use case of our tool: understanding the role of otherwise opaque pieces of neural functionality, such as non-linearities, learning rules, or connectomic data, through the inductive bias they induce.",
        "reference": "The authors present a meta-learning framework to infer a system's or circuit's inductive bias. In their work, the authors claim that their method connects architectural design choices to function space features. This work demonstrates the usability of the method in inferring the inductive bias for relatively simple models, i.e. linear and kernel regression as well as for shallow neural networks and spiking neural networks. The authors also leverage their framework and coupled with an adversarial training strategy, they demonstrate a potential application of learning functions that distinguish the inductive bias of two models with different architectures. Although current experimental evidence is limited to low-dimensional toy problems, the proposed method seems promising in understanding the inductive bias enforced by different architectural choices in high-dimensional end-to-end learning. Strengths:  The paper is very well-written (barring a few typos) and it is easy to follow and understand the core tenets of the proposed framework. The authors present incremental evidence to test out different hypothesis pertaining to their proposal. In doing so, they enable the reader to develop the intuitions behind what the expect from the proposed framework, when put in practice. The idea seems simple and elegant, yet powerful and highly applicable to systems identification problems in both the fields of machine learning and neuroscience.  Weakenesses:  A key weakness of the work is its limited empirical validation or experimental evidence, specifically for non-toy problems. Although the intuitions presented in toy tasks are clear and illustrate the usefulness of the method, it remains to be seen how well the bootstrapped learning framework performs in high dimensions.  A considerable failure mode in high-dimensional bootstrapped learning, something that this work proposes with the use of a learner that uses outputs of the meta-learner and vice versa, is dimension collapse. Specifically, it has been shown (for self-supervised bootstrapped learning) that often the functions learned are low-rank and therefore, the outputs do not span the entire space. Owing to this possible failure mode, it is reasonable to wonder whether this framework would be as applicable in high-dimensions as it is for low dimensionality problems. I was a bit confused about the utility of the proposed method for biological circuits, specifically how does the proposal fit in when attempting to understand the inductive bias of biological circuits. Firstly, the framework expects the learner to be given some output. How would such a training protocol look like in a particular circuit in a behaving animal? It would be nice to clarify this in the main text as a major motivation revolves around using the framework for biological circuits. Secondly, is it necessary that the system requires gradients to learn? In my understanding, the framework would work if there is some learning happening in the learner. Or could the framework still work without any plasticity in the learner? Have the authors tried testing the limits of their proposal by varying the learning rate for the learner.  The authors present their framework and demonstrate that it works for low-dimensional toy settings but it is not clear why or how it does so. It would be nice to have some intuitive explanation or insights from learning theory or dynamical systems that conveys how this meta-learning setup converges to the inductive bias functions of a learning system. The writing is generally clear and easy to understand, except in the discussions. The order of points while describing the issues with practical applications seems to have been reversed later in the discussions. Furthermore, it is not very clear why access to the gradients of the learning system is required.  The proposal seems to have merit and seems to be a promising direction in general. It is also a novel framework (to my knowledge). But given the lack of theoretical backing and empirical evidence in challenging problems in high-dimensions, I am unsure how this method scales with task (and network) complexity.  The authors plan to release their code on github, which should improve the overall reproducibility of the work. Although the general idea seems exciting and promising, I have my doubts over the scalability of this method. Unless authors can provide more evidence from higher dimensional problems (maybe something with MNIST where you try to visualize canonical functions it learns over the pixel space), the utility of the method seems to be limited. Moreover, it is not very clear from the discussions how this method can be used in computational neuroscience. Therefore, my current assessment is marginally below the threshold but I am happy to revisit my rating if the authors address some of my concerns raised above. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 136,
        "instruction": "Accelerating spiking neural network training using the $d$-block model: There is a growing interest in using spiking neural networks (SNNs) to study the brain \\textit{in silico} and in emulating them on neuromorphic computers due to their lower energy consumption compared to artificial neural networks (ANNs). Significant progress has been made in directly training SNNs to perform on a par with ANNs in terms of accuracy. However, these methods are slow due to their sequential nature and require careful network regularisation to avoid overfitting. We propose a new SNN model, the $d$-block model, with stochastic absolute refractory periods and recurrent conductance latencies, which reduces the number of sequential computations using fast vectorised operations. Our model obtains accelerated training speeds and state-of-the-art performance across various neuromorphic datasets without the need for any regularisation and using fewer spikes compared to standard SNNs.",
        "reference": "This work extends the 1-block model in Taylor et al. (2022) to the d-block model. Compared with the LIF model, the d-block model achieves accelerated computing on GPU by using fewer sequential operations. Strength: The proposed model indeed enables accelerated computing on GPU, and achieves sota results on some benchmarks. Weaknesses:  This work is quite a naive extension of Taylor et al. (2022). If the authors of this work and Taylor et al. (2022) are the same, I highly recommend the authors to combine the two works into one.  The authors need to describe the training method explicitly in the main content. The proposed model can be treated as a modified LIF model, which is irrelevant to the training methods. From appendix A.3.5, which is not detailed, I guess that the existing surrogate gradient method is adopted. The accelerated training on GPU is due to the model's parallel computing nature, not due to some novel training method. The authors should make it clear.  Can the d-block model be implemented on neuromorphic hardware in an event-driven manner? First, the 1-block model is equivalent to the single-spike LIF model, so I do not worry about it. But for the d-block model, the spikes from the first 3 time steps are used in the 4th time step. Is it implementable? will it be implementable on future neuromorphic hardware? Clarity: The authors do not describe the used training methods. Audiences need to guess what they do. Quality: The model achieves good performance and training efficiency. But it will be useless if it cannot be implemented on neuromorphic chips. Novelty: This work is a naive extension of Taylor et al. (2022). Reproducibility: Good. If the authors want me to raise the score, they should convince me about the novelty and the implementability of the model. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 137,
        "instruction": "RG: OUT-OF-DISTRIBUTION DETECTION WITH REACTIVATE GRADNORM: Detecting out-of-distribution (OOD) data is critical to building reliable machine learning systems in the open world. Previous works mainly perform OOD detection in feature space or output space. Recently, researchers have achieved promising results using gradient information, which combines the information in both feature and output space for OOD detection. However, existing works still suffer from the problem of overconfidence. To address this problem, we propose a novel method called ``Reactivate Gradnorm (RG)'', which exploits the norm of the clipped feature vector and the energy in the output space for OOD detection. To verify the effectiveness of our method, we conduct experiments on four benchmark datasets. Experimental results demonstrate that our RG outperforms existing state-of-the-art approaches by 2.06\\% in average AUROC. Meanwhile, RG is easy to implement and does not require additional OOD data or fine-tuning process. We can realize OOD detection in only one forward pass of any pretrained model.",
        "reference": "The authors revisit gradient-based OOD detection from the perspective of backpropagation and extend the decomposition G(x)=UV of GradNorm to more loss functions. Here G is a gradient-based detection score, V is the feature norm and U represents output information. According to this decomposition, the authors suggest exploiting suitable U and V for OOD detection. They take U as the energy-based score and derive a V measurement by assuming OOD features follow a standard Gaussian distribution. Experiments on four ImageNet benchmarks demonstrate the effect of the proposed method and discuss the impact of hyperparameters. Strength:  This work proposes a new idea to use the decomposition of gradient-based OOD detection. The proposed method is simple and easy to implement. The empirical result in Table 7 is good.  Weakness:  The proposed method is not well-motivated. The novelty of the proposed method is unclear. The content is poorly organized. This work lacks a hyperparameter selection method based on ID data. The novelty and Quality are fair. This work contributes some new ideas. It has minor technical flaws and some typos. The errors are fixable. The clarity is poor. The content should be carefully reorganized. I list my main questions in this section.  In the abstract, the motivation is 'However, existing works still suffer from the problem of overconfidence'. Why the proposed score in (8) can overcome the problem? Could you provide more analysis and comparisons to GradNorm and other OOD detection scores? In Section 3.1, does \"ground truth\" mean ground truth for classification or ground truth for OOD detection? Can you point out a loss function that corresponds to your proposed score in (8)? Can we understand the proposed score as an enhancement for the energy-based score? Why did you name it \"Reactivate Gradnorm\"? Is it because the proposed score follows the decomposition G(x)=UV? What is the purpose of introducing Section 3.3? The main result with ResNetv2-101 (Table 1) is not as good as the result with ResNet 50 (Table 7). Why use different pre-trained models in this section? In Table 7, is it unfair to compare Reactivate GradNorm to KNN? Would it be more convincing to compare Reactivate GradNorm with KNN+ReAct?  The OOD detection task in this work is a one-sample hypothesis testing problem, i.e., only ID data is accessible. Therefore, the hyperparameters in your score should be determined by the ID data. Table 3 and Table 4 only consider one ID data. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 138,
        "instruction": "Gandalf : Data Augmentation is all you need for Extreme Classification: Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on the problem setting with short-text input data, and labels endowed with short textual descriptions called label features. Short-text XMC with label features has found numerous applications in areas such as prediction of related searches, title-based product recommendation, bid-phrase suggestion, amongst others. In this paper, we propose Gandalf, a graph induced data augmentation based on label features, such that the generated data-points can supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations. While most recent advances (such as SiameseXML and ECLARE) in XMC have been algorithmic, mainly aimed towards developing novel deep-learning architectures, our data-centric augmentation approach is orthogonal to these methodologies. We demonstrate the generality and effectiveness of Gandalf by showing up to 30% relative improvements for 5 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3 million labels. ",
        "reference": "Extreme classification (or deep information retrieval) has been a popular research field that matches an input text (query) to a label that often is also a text. This paper focuses on a sub-field where both the input text and the label text are short. The proposed training algorithm is a data augmentation algorithm that generates similar examples to the observation based on the label to label similarities. Authors claim this new data augmentation technique was able to improve significantly on benchmark datasets. Strength  Since Gandalf is a pure data augmentation technique that can be applied to some set of algorithms easily. (however, it\u2019s a bit arguable for some other sets of algorithms that can do similar techniques more organically. see weakness #1). Authors try to set up a reasonable data-augmentation baseline, LabelMix, influenced by the popular \u201cMix-up\u201d technique.  Weakness  Selecting k most confusing examples via label similarities is not novel. It is sometimes called \u201chard-negatives\u201d that is widely adopted in this field (for example, DPR [1], RocketQA [2] and ANCE [3]). The main difference is to use dense label space or to use the label relationship graph (they are quite interchangeable, but the dense space has nicer properties). It is also partially observed in Table 3 as using the soft-labels from the label graph further drastically increases the performance over hard labels. Defining the label graph solely based on the label co-occurrences might have its limitations because it does not use any textual understanding of each label. Commonly, the dense label space is trained jointly with the actual retrieval task [1,2,3] and hence captures deeper understanding of the label to label similarities based on both bipartite co-occurrence relationships and textual understanding. Moreover, solely depending on co-occurrence relationships can be very noisy when we have large label space (e.g. thousands of millions or billions). The work was not evaluated against common dense retrieval algorithms such as [1,2,3].  [1] Karpukhin, Vladimir, et al. \"Dense passage retrieval for open-domain question answering.\" arXiv preprint arXiv:2004.04906 (2020). [2] Qu, Yingqi, et al. \"RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering.\" arXiv preprint arXiv:2010.08191 (2020). [3] Xiong, Lee, et al. \"Approximate nearest neighbor negative contrastive learning for dense text retrieval.\" arXiv preprint arXiv:2007.00808 (2020). Clarity  The paper can be improved on their clarity on how their proposal is presented. It is quite unclear how the actual training objectives are defined (e.g. loss functions) and how the newly generated examples are integrated into the objective. Algorithm 1 should be written in pseudo-code for those who do not have prior Python knowledge. The paper does not convey a significant novelty in this field and was not sufficiently compared to many core techniques in this field.  Post rebuttal response: I thank authors to provide detailed feedback. I also read other reviewers' feedback and concluded this paper does require further development to contain further contribution in this field. I still concerns the novelty of this paper (similar to Reviewer g2YZ). Here are some detailed points that the author replied.  Gandalf is only a method to leverage Label-Text/Features as data points whose ground truth labels are unknown.  This actually limits the novelty of the method. Gandalf's novelty is limited in introducing a heuristic method to generate additional labels.   Contrast to Hard Negative Mining   Thanks for the detailed description. I wonder then what's the actual benefit of using Galdalf's soft-labels. We often found strong negatives that are closer to the decision boundary helps achieving more discriminative models especially when the label space is large (XMC). Wouldn't it be more beneficial Galdalf to focus on closer but not positive labels? 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 139,
        "instruction": "Attention Flows for General Transformers: In this paper, we study the computation of how much an input token in a Transformer model influences its prediction. We formalize a method to construct a flow network out of the attention values of encoder-only Transformer models and extend it to general Transformer architectures, including an auto-regressive decoder. We show that running a maxflow algorithm on the flow network construction yields Shapley values, which determine a player's impact in cooperative game theory. By interpreting the input tokens in the flow network as players, we can compute their influence on the total attention flow leading to the decoder's decision. Additionally, we provide a library that computes and visualizes the attention flow of arbitrary Transformer models. We show the usefulness of our implementation on various models trained on natural language processing and reasoning tasks.",
        "reference": "This work extends the attention flow method proposed in Abnar and Zuidema 2020 to encoder-decoder and decoder-only transformers. The major contribution is based on the observation that later predicted words have more incoming edges than earlier words, such that to ensure positional independence, this work proposes a method to normalize maxflow values. In addition, this work draws connection between maxflow attention and Shapley values by defining payoffs as the sum of maxflows and showing the equivalence under this definition. Experiments on several tasks show that this method is able to gain insights into token importance for a prediction task. Strengths:  Extends attention flow to encoder-decoder and decoder-only transformers. Empirical analysis of token importance on several tasks.  Weaknesses:  The change to Abnar and Zuidema 2020 seems incremental. The connection to Shapley values is drawn by defining value function to be based on maxflows, so the equivalence is not surprising at all. A more interesting connection would be to use the actual log probability of the target token as payoff values and see if there's a correlation. Related to 2, it is not clear if attention maxflows reflect actual token importance. It would be more convincing if the maxflow values can be compared against feature importance obtained using other methods (such as Shapley values using log probability of target token as payoffs, or simply gradient-based saliency maps). For example, does the first-token bias found by maxflow hold for Shapley values? The analysis on individual attention heads doesn't make that much sense to me, since the same head id across different layers does not mean they have anything in common. This paper is original and well-written. However, I think the connection to Shapley values in its current form is over-complicating things and it's not necessary for the understanding of the paper. My major concerns are: 1. the change to the original attention flow paper is incremental; and 2. a correlation study with feature importance found by other methods (such as Shapley values) is missing and it's not clear if the found maxflow values mean anything. Therefore, I'm leaning towards rejecting this paper. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 140,
        "instruction": "Convergence Rate of Primal-Dual Approach to Constrained Reinforcement Learning with Softmax Policy: In this paper, we consider primal-dual approach to solve constrained reinforcement learning (RL) problems, where we formulate constrained reinforcement learning  under constrained Markov decision process (CMDP). We propose the primal-dual policy gradient (PD-PG) algorithm with softmax policy. Although the constrained RL involves a non-concave maximization problem over the policy parameter space, we show that for both exact policy gradient and model-free learning, the proposed PD-PG needs iteration complexity of $\\mathcal{O}\\left(\\epsilon^{-2}\\right)$ to achieve its optimal policy for both constraint and reward performance. Such an iteration complexity outperforms or matches most constrained RL algorithms. For the learning with exact policy gradient, the main challenge is to show the positivity of deterministic optimal policy (at the optimal action) is independent on both state space and iteration times.\nFor the model-free learning, since we consider the discounted infinite-horizon setting, and the simulator can not rollout with an infinite-horizon sequence; thus one of the main challenges lies in how to design unbiased value function estimators with finite-horizon trajectories. We consider the unbiased estimators with finite-horizon trajectories that involve geometric distribution horizons, which is the key technique for us to obtain the theoretical results for model-free learning.",
        "reference": "This paper considers primal-dual approach to solve constrained reinforcement learning (RL) problems, where we formulate constrained reinforcement learning under constrained Markov decision process (CMDP). +: The analysis seems correct.  -: The related work is misrepresented. The generative model should give additive samples, and not multiplicative.  -: The approach is standard, and novelty seems limited.  -: The constraint violations in the literature have been improved to zero violations, which should be incorporated.  -: Based on the table presented, the results do not seem the state of the art.  -: The paper is giving iteration complexity, and not sample complexity. The results are clear, while the approaches are standard. This paper considers primal-dual approach to solve constrained reinforcement learning (RL) problems, where we formulate constrained reinforcement learning under constrained Markov decision process (CMDP). The paper needs significant modifications to be at the level of ICLR paper as mentioned in weaknesses. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 141,
        "instruction": "Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning: We consider the problem of offline reinforcement learning where only a set of system transitions is made available for policy optimization. Following recent advances in the field, we consider a model-based reinforcement learning algorithm that infers the system dynamics from the available data and performs policy optimization on imaginary model rollouts. This approach is vulnerable to exploiting model errors which can lead to catastrophic failures on the real system. The standard solution is to rely on ensembles for uncertainty heuristics and to avoid exploiting the model where it is too uncertain. We challenge the popular belief that we must resort to ensembles by showing that better performance can be obtained with a single well-calibrated autoregressive model on the D4RL benchmark. We also analyze static metrics of model-learning and conclude on the important model properties for the final performance of the agent.",
        "reference": "The paper uses a standard off-policy moder-based reinforcement learning algorithm and uses a set of dynamics models (autoregressive models, ensembles & mixture density models) on a single benchmark.  They then test the algorithms using a set of metrics and found that auto-regressive models appear to give improved performance Strengths:  the use of auto-reggressive models and its comparison to  standard ensembling methods appears novel  Weaknesses:  Significance : The methodology the authors present (offline model-based RL with a parameterized policy) is standard. The algorithms for model learning (deep ensembles, auto-regressive models) are known and testing on a single benchmark does not provide any insight into their advantages and disadvantages  Limited experiments: The authors say in the conclusion \"In this paper, we ask what are the best dynamic system models, estimating their own uncertainty,\"  -- but all tests are done on a single benchmark. Limited comparisons: There is more work on using uncertainty-aware models in offline model-based RL, such as Gaussian Processes, Bayesian Deep Learning or using Dropout-mechanisms The paper is written quite clear The insights from the paper do not provide much novelty compared to previous work, mainly based on the limited evaluation. The comparison between models is interesting, but only done on a single benchmark. Overall, I cannot recommend acceptance because I unfortunately do not see a clear contribution and significance of the presented work. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 1: The contributions are neither significant nor novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 142,
        "instruction": "Robust Training through Adversarially Selected Data Subsets: Robustness to adversarial perturbations often comes at the cost of a drop in accuracy on unperturbed or clean instances. Most existing defense mechanisms attempt to defend the learner from attack on all possible instances, which often degrades the accuracy on clean instances significantly. However, in practice, an attacker might only select a small subset of instances to attack, $e.g.$, in facial recognition systems an adversary might aim to target specific faces. Moreover, the subset selection strategy of the attacker is seldom known to the defense mechanism a priori, making it challenging to attune the mechanism beforehand. This motivates designing defense mechanisms which can (i) defend against attacks on subsets instead of all instances to prevent degradation of clean accuracy and, (ii) ensure good overall performance for attacks on any selected subset. In this work, we take a step towards solving this problem. We cast the training problem as a min-max game involving worst-case subset selection along with optimization of model parameters, rendering the problem NP-hard. To tackle this, we first show that, for a given learner's model, the objective can be expressed as a difference between a $\\gamma$-weakly submodular and a modular function. We use this property to propose ROGET, an iterative algorithm, which admits approximation guarantees for a class of loss functions. Our experiments show that ROGET obtains better overall accuracy compared to several state-of-the-art defense methods for different adversarial subset selection techniques.",
        "reference": "The paper considers a threat model where an attacker attacks a subset of the test data of a classification system. To develop a defense, the formulate the learning problem for the classier as a min-max optimization where the loss is minimized over the most adversarial subset of a particular cardinality. The authors show that the inner optimization that requires a subset selection (for a given \u03b8) us NP-hard. To solve this efficiently, they leverage a stochastic distorted greedy algorithm by (Harshaw et. al. 2019) and is able to give an approximation guarantee for the overall adversarial learning function for a particular class of loss functions. The authors then conduct a suite of experiments to showcase the effectiveness of their approach against existing defenses in the threat model they consider. Things I liked  The paper addresses an interesting problem, clearly explains the difficult of the problem and proposes a solution with guarantees when certain assumptions hold.  The authors conduct a reasonable set of experiments and provide logical choices of the baselines, attack methods, etc.  The mix max over the hyper-parameter choice is nice little trick.   Things that need clarification / improvement  The empirical results are weak in several aspects.   The proposed method, ROGET, mostly outperforms other defenses (which are designed for the entire test-set) on clean test-data accuracy. It is at times the worst performing method of perturbed test data. There are also sentences like \"Tuning \u03c1 would easily improve the robustness, as shown in additional experiments in Appendix H\" which surprises me on why would the authors not show their best results as part of the main paper? If there is some knowledge about the true subset selection strategy at validation time, other methods (designed for all test attacks) outperforms ROGET. As the amount of test data poisoned grows, the accuracy gains become minimal.   Given the inner optimization is itself a difficult problem that admits an approximation, the authors should highlight the time/resource costs incurred by their defense vs. existing defenses. Beyond accuracy, this is an important metric to consider.  There is no clear association between the assumptions made in proofs and the function used in practice. For example, is categorical-cross entropy a Polyak-Lojasiewicz Loss function? If not, the approximation guarantees do not hold here. This disrupts the flow of the paper somewhat. The paper addresses an interesting setting and seems reproducible. The theoretical development of the work is nice, but empirical results are weak. See above. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 143,
        "instruction": "Beyond Reward: Offline Preference-guided Policy Optimization: In this work, we study offline preference-based reinforcement learning (PbRL), which relaxes the two fundamental supervisory signals in standard reinforcement learning (online accessible transition dynamics and rewards). In other words, the agent is provided with fixed offline trajectory transitions and human preferences between pairs of trajectories. Due to the orthogonality property of rewards and dynamics, one common practice is combining prior PbRL-based reward learning objectives with off-the-shelf offline RL algorithms to bridge preference modeling and offline learning. However, such two isolated optimizations require learning a separate reward function and thus place an information bottleneck on reward learning (the bridge). As an alternative, we propose offline preference-guided policy optimization (OPPO), an end-to-end offline PbRL formulation, which jointly learns to model the preference (for finding the optimal task policy) and the offline data (for eliminating OOD). In particular, OPPO introduces an offline hindsight information matching objective and a preference modeling objective. Then, iterating the two objectives over, we can directly extract a well-performing decision policy, avoiding a separate reward learning. We empirically show that OPPO can effectively model the offline preference and outperform prior competing baselines (including the offline RL algorithms performed over the true reward function).",
        "reference": "This paper looks at the problem of offline RL from human preferences. Instead of first learning a reward model from human preferences and then training with offline RL, this paper learns a context embedding from trajectories, and then learns a policy that is conditioned on the context. Then, to optimize for the human preferences, it learns the optimal context, which can be fed into the policy. Generally I like this research direction, and I think that learning context from offline data can be useful for other tasks as well, such as learning options  Unfortunately, it doesn\u2019t look like the method convincingly outperforms existing baselines. However, it does perform competitively, and I think that the fact that it doesn\u2019t learn a reward model is a nice aspect. I like the experiments showing that the learned optimal context aligns with preferences.  A question: what is the main difference between learning a reward model and learning a context + the optimal context? It seems almost the same thing. Could they be thought of as the same thing in a broader unified perspective?  Why would we expect OPPO to perform better than learning a reward model? The method seems like a clear extension of HIM, but seems novel enough. The algorithm 1 is confusing. It seems reproducible enough. I think this is a nice idea, and it should be useful for the community. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 6: marginally above the acceptance threshold 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 144,
        "instruction": "Neural Probabilistic Logic Programming in Discrete-Continuous Domains: Neural-symbolic AI (NeSy) methods allow neural networks to exploit symbolic background knowledge. NeSy has been shown to aid learning in the limited data regime and to facilitate inference on out-of-distribution data. Neural probabilistic logic programming (NPLP) is a popular NeSy approach that integrates probabilistic models with neural networks and logic programming. A major limitation of current NPLP systems, such as DeepProbLog, is their restriction to discrete and finite probability distributions, e.g., binary random variables. To overcome this limitation, we introduce DeepSeaProbLog, an NPLP language that supports discrete and continuous random variables on (possibly) infinite and even uncountable domains. Our main contributions are 1) the introduction of DeepSeaProbLog and its semantics, 2) an implementation of DeepSeaProbLog that supports inference and gradient-based learning, and 3) an experimental evaluation of our approach.",
        "reference": "This work proposes a neural probabilistic programming language that supports both discrete and continuous variables, called DeepSeaProbLog. An implementation of DeepSeaProbLog that allows inference and gradient-based learning is further proposed, by leveraging a reduction to weighted model integration and differentiation through a weighted model integral. Empirical evaluations on DeepSeaProbLog are presented on a neural-symbolic object detection task, variational auto-encoder with a difference constraint in the latent space, and neural hybrid Bayesian networks. The motivation of this work to enable neural probabilistic programming to work in mixed discrete-continuous domains is tempting since it would allow for expressive modeling for real-world problems. The use of weighted model integration tool is novel to me and it is the key to tackle the mixed discrete-continuous domain challenge. The connection between the proposed DeepSeaProbLog and the existing work on neural probabilistic programming is nicely explained. Still, here are some of my concerns/suggestions:  Missing references to some of the current literature on weighted model integration solvers such as [1,2,3]. I think this work would benefit from a discussion on the choice of WMI solvers for performing inference in DeepSeaProbLog. For example, how different WMI solvers would support different inference performances of DeepSeaProbLog. The proof of Prop 4.1 refers to Zuidberg Dos Martires et al. (2019) while it is unclear which results in Zuidberg Dos Martires et al. (2019) is related to the conclusion that Eq C.3 is indeed a weighted model integration problem. Missing comparison in the pure discrete setting. When DeepSeaProbLog is applied to a pure discrete setting, there should be a bunch of neural probabilistic programming benchmarks as well as baselines for comparison. The authors might want to put such an empirical comparison to illustrate the discrete reasoning capability of DeepSeaProbLog in such settings. In Sec 3.2, it seems that one limitation of DeepSeaProbLog is that each distributional fact must define a different random variable. I wonder why such an assumption is necessary. Also, are there any distributional assumptions on the continuous variables? It seems that the continuous variables are all assumed to be Gaussian. In the neural-symbolic VAE experiment, it would be more convincing to include an ablation study where the VAE has no difference constraint but is still trained with difference as addition input. This ablation study is necessary since it might be possible that the VAE might simply learn the digit pair conditioned on the difference label and such an ablation study would help to see how much the DeepSeaProbLog help improve accuracy. Another issue in the neural-symbolic VAE experiment is that when it shows that DeepSeaProbLog is able to answer conditional generative queries, only one example is presented. This can be further improved by presenting some metrics such as accuracy to measure the performance of answering such queries.  [1] P. Morettin, A. Passerini, and R. Sebastiani. Efficient weighted model integration via SMT-based predicate abstraction. In IJCAI, 2017. [2] Z. Zeng, P. Morettin, F. Yan, A. Vergari, and G. Van den Broeck. Probabilistic inference with algebraic constraints: Theoretical limits and practical approximations. In NeurIPS, 2020. [3] Z. Zeng, P. Morettin, F. Yan, A. Vergari, and G. Van den Broeck. Scaling up hybrid probabilistic inference with logical and arithmetic constraints via message passing. In ICML, 2020. This work is overall well-written and the contribution is solid and novel to me. The proposed DeepSeaProbLog is novel to me. However, my main concern is the empirical evaluation not being extensive and not so convincing. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. 4: The contributions are significant, and do not exist in prior works. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 145,
        "instruction": "FedGC: An Accurate and Efficient Federated Learning under Gradient Constraint for Heterogeneous Data: Federated Learning (FL) is an important paradigm in large-scale distributed machine learning, which enables multiple clients to jointly learn a unified global model without transmitting their local data to a central server. FL has attracted growing attentions in many real-world applications, such as multi-center cardiovascular disease diagnosis and autonomous driving. Practically, the data across clients are always heterogeneous, i.e., not independently and identically distributed (Non-IID), making the local models suffer from catastrophic forgetting of the initial (or global) model. To mitigate this forgetting issue, existing FL methods may require additional regularization terms or generates pseudo data, resulting to 1) limited accuracy; 2) long training time and slow convergence rate for real-time applications; and 3) high communication cost. In this work, an accurate and efficient Federated Learning algorithm under Gradient Constraints (FedGC) is proposed, which provides three advantages: i) High accuracy is achieved by the proposed Client-Gradient-Constraint based projection method (CGC) to alleviate the forgetting issue occurred in clients, and the proposed Server-Gradient-Constraint based projection method (SGC) to effectively aggregate the gradients of clients; ii) Short training time and fast convergence rate are enabled by the proposed fast Pseudo-gradient-based mini-batch Gradient Descent (PGD) method and SGC; iii) Low communication cost is required due to the fast convergence rate and only gradients are necessary to be transmitted between server and clients. In the experiments, four real-world image datasets with three Non-IID types are evaluated, and five popular FL methods are used for comparison. The experimental results demonstrate that our FedGC not only significantly improves the accuracy and convergence rate on Non-IID data, but also drastically decreases the training time. Compared to the state-of-art FedReg, our FedGC improves the accuracy by up to 14.28% and speeds up the local training time by 15.5 times while decreasing 23% of the communication cost.",
        "reference": "This paper studies the problem of alleviating convergence issues in FL in the face of non-iid data. In particular, the authors propose methods which align the gradient and the direction of update with the past update at the server, and align the server update with the aggregated client model update. The alignment is done through a projection operation. Finally, empirical results on public datasets illustrate the efficacy of the approach. Strengths:  The paper is well written and easy to follow. The problem being considered is relevant and of sufficient interest to the FL community in general. The method proposed utilizes inequality constrained optimization to enforce alignment between local and global update directions. The alignment is done at both the level of the client and server which alleviates non-iidness to a certain extent. Technically speaking, the approach is quite general and original in its form. The experimental results show that the proposed algorithm outperforms other well known baselines.  Weakenesses:  The alignment at the client level requires the server sending the aggregated model update from the past round to each client, which undermines the privacy-preserving aspect of FL quite a bit. Moreover, the optimization problem to be solved at hand is a constrained optimization with additional computational overhead on the clients, with no guarantees of existence of a feasible space.  It's also not clear how C is chosen and how it affects convergence. The discussion in Step 1 in page 4 is erroneous. Unlike what the authors discuss, mini-batch SGD evaluates gradients at the same model iterate for multiple batches and then averages them. It doesn't take update steps after each gradient evaluation as suggested by the authors. Overall, step 1 of the algorithm is no different than standard local mini-batch SGD, where the client takes multiple local steps. The question of feasibility of the constrained optimization becomes even more acute in case of the server, as it combines k different constraints. It's not clear to me especially as to what is done, if the intersection of such constraints leads to an empty set, i.e., there's no feasible solution. In sufficiently non-iid data, how does one go about such scenarios. The algorithm is devoid of any theoretical guarantees in terms of convergence. It's not clear if the aligned update directions thus obtained lead to a direction of sufficient descent. Finally the experimental results leave a lot wanting. Number of local epochs is chosen to be 5 for the proposed algorithm, while it is taken to be 1 arbitrarily for all other baselines for the Handwritten digits dataset. Similarly, while learning rate tuning is done for the proposed algorithm, all other baselines are assigned a fixed learning rate without any tuning. Hence, the experimental comparisons are not fair and no claims regarding the superiority of the proposed approach can be made. The paper is well written and easy to follow. The algorithm and the solution is original, but lacks any sort of theoretical guarantees and the empirical comparisons are not fair. The paper considers a relevant problem of interest in FL to tackle non-iidness and proposes to tackle it with gradient alignment via projections. There are a lot of questions concerning the algorithm both in terms of feasibility and convergence, which is not covered in the paper. Finally, experimental results are weak in terms of fairness of comparing with other baselines. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 146,
        "instruction": "Compression-aware Training of Neural Networks using Frank-Wolfe: Many existing Neural Network pruning approaches either rely on retraining to compensate for pruning-caused performance degradation or they induce strong biases to converge to a specific sparse solution throughout training. A third paradigm, \u2019compression-aware\u2019 training, obtains state-of-the-art dense models which are robust to a wide range of compression ratios using a single dense training run while also avoiding retraining. In that vein, we propose a constrained optimization framework centered around a versatile family of norm constraints and the Stochastic Frank-Wolfe (SFW) algorithm which together encourage convergence to well-performing solutions while inducing robustness towards convolutional filter pruning and low-rank matrix decomposition. Comparing our novel approaches to compression methods in these domains on benchmark image-classification architectures and datasets, we find that our proposed scheme is able to yield competitive results, often outperforming existing compression-aware approaches. In the case of low-rank matrix decomposition, our approach can require much less computational resources than nuclear-norm regularization based approaches by requiring only a fraction of the singular values in each iteration. As a special case, our proposed constraints can be extended to include the unstructured sparsity-inducing constraint proposed constraint by Pokutta et al. (2020) and Miao et al. (2022), which we improve upon. Our findings also indicate that the robustness of SFW-trained models largely depends on the gradient rescaling of the learning rate and we establish a theoretical foundation for that practice.",
        "reference": "The paper proposes a novel framework for compression-aware training of neural networks. The proposed method uses norm constraints, for two types of pruning (1) convolutional filter pruning (2) low-rank matrix decomposition, expressed via updates of the Stochastic Frank-Wolfe (SFW) algorithm efficiently. Pros:   The proposed framework is interesting and beneficial to the community. The authors provide sufficient intuition and motivation behind the use of the sparsity-inducing norm constraints and how they can be effectively realized via SFW. The presentation is clear and the math is easy to follow.  All the claims are supported well by empirical studies on benchmark datasets. The baselines seem sensible; although I must admit that I'm not too familiar with the related works in the compression-aware setting  Comments on the robustness study: One of the interesting sections in the paper is the study on the robustness of the pruned model. The experimental study and the authors' discussion on the benefits of using the rescaled learning rate are insightful, especially at higher compression rates. With gradient scaling, the authors are able to show the 1/\\sqrt(T) convergence result (FW gap) for SFW. However, they still collectively don't provide enough convincing arguments for the robustness claims, in my opinion. The writing is clear and the ideas are easy to follow. The authors have agreed to release the code if accepted. Overall, I think this is a good paper. The ideas presented are novel and are well-supported by ample experimental evidence and some theoretical results. The proposed method and discussions are relevant and useful to the community. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 147,
        "instruction": "MBrain: A Multi-channel Self-Supervised Learning Framework for Brain Signals: Brain signals are important quantitative data for understanding physiological activities and diseases of human brain. Meanwhile, rapidly developing deep learning methods offer a wide range of opportunities for better modeling brain signals, which has attracted considerable research efforts recently. Most existing studies pay attention to supervised learning methods, which, however, require high-cost clinical labels. In addition, the huge difference in the clinical patterns of brain signals measured by invasive (e.g., SEEG) and non-invasive (e.g., EEG) methods leads to the lack of a unified method. To handle the above issues, in this paper, we propose to study the self-supervised learning (SSL) framework for brain signals that can be applied to pre-train either SEEG or EEG data. Intuitively, brain signals, generated by the firing of neurons, are transmitted among different connecting structures in human brain. Inspired by this, we propose to learn implicit spatial and temporal correlations between different channels (i.e., contacts of the electrode, corresponding to different brain areas) as the cornerstone for uniformly modeling different types of brain signals. Specifically, we capture the temporal correlation by designing the delayed-time-shift prediction task; we represent the spatial correlation by a graph structure, which is built with the goal to maximize the mutual information of each channel and its correlated ones. We further theoretically prove that our design can lead to a better predictive representation. Extensive experiments of seizure detection on both EEG and SEEG large-scale real- world datasets demonstrate our model outperforms several state-of-the-art time series SSL and unsupervised models.",
        "reference": "The paper proposes an SSL framework for EEG and evaluate for seizure detection. Strengths:  Nicely written Baselines are chosen well Outperforms all baselines for both F1 and F2 (weaknesses highlight why this is not really valid) Evaluated on both SEEG and EEG datasets Includes clinical collaboration further signifying the significance of the work The domain adaptation experiment in Table 2 is also a great way to show the significance of the work  Weaknesses:  Split table 3 into two tables? Domain adaptation not shown on TUSZ dataset Including the same subject in both testing and training invalidates the results The writeup is clear and the work has sufficient novelty. Code is included for reproducibility. The paper is nicely written with good results on two different datasets. The demonstrated application is important and the strong performance indicates the significance of the work. However, including the same subjects in training and testing invalidates the results. It is not clear if that was done for other approaches as well. Even if it was, both should be done by selecting different subjects for training and testing. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 148,
        "instruction": "Group-Disentangling Conditional Shift: We propose a novel group disentanglement method called the Context-Aware Variational Autoencoder (CxVAE). Our model can learn disentangled representations on datasets with conditional shift. This phenomenon occurs when the conditional distribution of the instance-level latent variable $\\mathbf{z}$ given the input observation $\\mathbf{x}$ changes from one group to another (i.e. $p_i(\\mathbf{z}|\\mathbf{x}) \\neq p_j(\\mathbf{z}|\\mathbf{x})$, where $i,j$ are two different groups). We show that existing methods fail to learn disentangled representations under this scenario because they infer the group $\\mathbf{u}$ and instance $\\mathbf{z}$ variables separately. CxVAE overcomes this limitation by conditioning the instance inference on the group variable $q(\\mathbf{z}|\\mathbf{x},\\mathbf{u})$. Our model has the novel ability to disentangle ambiguous observations (those with incomplete information about the generative factors), which we evaluate on the task of fair comparisons between student test scores. Additionally, we demonstrate empirically that conditional shift is the cause of our model's improved performance.",
        "reference": "The paper proposes a context aware variational auto encoder which modified the structure of previous C-VAE. The evaluation is on synthetic data only. Strength:   the work touches a fundamental problem.  Weakness:   Only synthetic experiments are conducted.  The VAE only tested with MLP.   The data generated is in low dimensional and not very persuasive. The paper is clear, but lacking of intuition. For example,   why we need to add un into distribution Q? Any intuition for doing that?  What is the proof detail for eqn 8-10? Some equation is wield. Eqn 3-4 are also the same equation.  It is unclear how to implement the proposed ELBO loss in real world?  Which reparameter trick are you using?  Why the method was only tested in synthetic data? How about high dimensional real world images? For example, the GVAE tested in image data. It is conventional to show in some real world high dimensional data. I think the paper is lacking of intuition and details at this stage. In addition, the experiment is insufficient (only synthetic data used). 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 149,
        "instruction": "When and Why Is Pretraining Object-Centric Representations Good for Reinforcement Learning?: Unsupervised object-centric representation (OCR) learning has recently been drawing a lot of attention as a new paradigm of visual representation. This is because of its potential of being an effective pretraining technique for various downstream tasks in terms of sample efficiency, systematic generalization, and reasoning. Although image-based reinforcement learning (RL) is one of the most important and thus frequently mentioned such downstream tasks, the benefit in RL has surprisingly not been investigated systematically thus far. Instead, most of the evaluations have focused on rather indirect metrics such as segmentation quality and object property prediction accuracy. In this paper, we investigate the effectiveness of OCR pretraining for image-based reinforcement learning via empirical experiments. For systematic evaluation, we introduce a simple object-centric visual RL benchmark and verify a series of hypotheses answering questions such as \"Does OCR pretraining provide better sample efficiency?\", \"Which types of RL tasks benefit most from OCR pretraining?\", and \"Can OCR pretraining help with out-of-distribution generalization?\". The results suggest that OCR pretraining is particularly effective in tasks where the relationship between objects is important, improving both task performance and sample efficiency when compared to single-vector representations. Furthermore, OCR models facilitate generalization to out-of-distribution tasks such as changing the number of objects or the appearance of the objects in the scene.",
        "reference": "The paper provides an empirical evaluation of whether object-centric representation pre-training is useful for RL learning. They find that OCR pre-training generally delivers better and more data-efficient model, also allowing generalization to unseen settings (e.g., an unseen number of objects). The paper presents a nice analysis on the environments and tasks it studies. It shows that OCR outperforms end-to-end distributed representations on relational tasks and scenes with many objects but not on simple object goal tasks, which aligns with our expectation of OCR. However, my main concern is that the experiments only concern two simple synthetic environments. I am not sure if this can be considered as a comprehensive study on whether OCR pre-training is effective for reinforcement learning. The experiments are als similar to the experiments in COBRA (Watters et al.), which also uses Spriteworld. It would be good if the authors could highlight the difference. Watters et al. COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration  Minor question: Why would OCR models outperform the GT model in second figure in Figure 3? I would image GT as an upper-bound. How is the GT state embedded? What does the ground truth state include? The paper is very clear. Most implementation details are provided. Overall, while I think the authors provide nice analysis on the studied environment. However, I am not sure if the experiments can be considered as comprehensive enough to support some of the rather general claims the authors made. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 150,
        "instruction": "Face reconstruction from facial templates by learning latent space of a generator network: Face recognition systems are increasingly deployed in different applications. In these systems, a feature vector (also called facial embeddings or templates) is typically extracted from each face image and is stored in the system's database during the enrollment stage, which is later used for comparison during the recognition stage. In this paper, we focus on the template inversion attack against face recognition systems and propose a new method to reconstruct face images from facial templates. Within a generative adversarial network (GAN)-based framework, we learn a mapping from facial templates to the intermediate latent space of a pre-trained face generation network, from which we can generate high-resolution realistic reconstructed face images. We show that our proposed method can be applied in whitebox and blackbox attacks against face recognition systems. Furthermore, we evaluate the transferability of our attack when the adversary uses the reconstructed face image to impersonate the underlying subject in an attack against another face recognition system. Considering the adversary's knowledge and the target face recognition system, we define five different attacks and evaluate the vulnerability of state-of-the-art face recognition systems. Our experiments show that our proposed method achieves high success attack rates in whitebox and blackbox scenarios. Furthermore, the reconstructed face images are transferable and can be used to enter target face recognition systems with a different feature extractor model. ",
        "reference": "The authors of this paper propose  new method to reconstruct high-resolution realistic face images from facial templates in a FR system. They focus on the template inversion attack against face recognition systems. Strength  The motivation is very clear. It is well written and the experimentation seems correct. Five different attacks and evaluated the vulnerability of SOTA FR systems to the proposed method are defined. The novelty and scope appear limited and incremental. The main contribution in the paper is in using results in StyleGAN in  FR systems. The authors promise to make publicly available the experimentation code. The paper is clearly written and easy to read and understand. But the novelty and scope appear limited. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. Yes, Legal compliance (e.g., GDPR, copyright, terms of use)Details Of Ethics Concerns: The authors should explain how to mitigate the potential issues deriving from the proposed attack method. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 151,
        "instruction": "A sparse, fast, and stable representation for multiparameter topological data analysis: Topological data analysis (TDA) is a new area of geometric data analysis that focuses on using invariants from algebraic topology to provide multiscale shape descriptors for point clouds.  One of the most important shape descriptors is persistent homology, which studies the topological variations as a filtration parameter changes; a typical parameter is the feature scale.\n\nFor many data sets, it is useful to consider varying multiple filtration parameters at once, for example scale and density.  While the theoretical properties of one-parameter persistent homology are well understood, less is known about the multiparameter case.  Of particular interest is the problem of representing multiparameter persistent homology by elements of a vector space for integration with traditional machine learning.\n\nExisting approaches to this problem either ignore most of the multiparameter information to reduce to the one-parameter case or are heuristic and potentially unstable in the face of noise.  In this article, we introduce a general representation framework for multiparameter persistent homology that encompasses previous approaches. We establish theoretical stability guarantees under this framework  as well as efficient algorithms for practical computation, making this framework an applicable and versatile tool for TDA practitioners. We validate our stability results and algorithms with numerical experiments that demonstrate statistical convergence, prediction accuracy, and fast running times on several real data sets. ",
        "reference": "This paper discusses a framework of multiparameter persistence, which has been a long-standing problem in the field of topological data analysis with persistence homology, and proposes three types of specific representations (GS representations) with stability guarantee.   The numerical results demonstrate the convergence rates with respect to the sample size and the effectiveness in classification accuracies with real-world data. Strength:  A framework for representing multiparameter persistence modules is proposed, covering some previous methods.  The proposed methods with specific representations have a theoretical guarantee of stability, which is an important property of a topological feature in applications.  In the two multiparameter cases of the measure bifiltration and Cech complex with density, the theoretical analysis gives convergence rates of the proposed GS representations, and the rates are also confirmed numerically.  The experimental results with real-world data sets demonstrate better classification accuracies than the existing multiparameter representations.  Weakness:  The significance of the proposed framework and method is not clearly presented in the paper.   While the multiparameter persistence is a more general approach, the practical advantage in data analysis over 1-parameter persistence is not necessarily clear in the paper.  For the real-world data, more basic comparisons with 1-parameter persistent homology as well as other relevant methods should be included.   The advantage over the existing multiparameter methods is not sufficiently explained, although it is somehow discussed.  One of the advantages should be stability.  In comparison with Carriere & Blumberg (2022),  the authors just cite Botnan & Lesnick (2018) without details.  As an important contribution, the reason for having stability, unlike the others, should be explained more clearly and demonstrated with some basic numerical examples.   The significance of the theoretical analysis in Section 4.1 is unclear.   As discussed in the paper, the dependence of constant \u03b4 is not clear enough.  While the authors say that the empirical results do not show bad behavior by choice of \\delta, more detailed discussions are necessary about whether this dependence on \u03b4 appears as an artifact by the theoretical derivation or an inevitable factor by the method.  In the upper bound (8), the bandwidth parameter h remains constant, and the bound involves a constant factor.  Can we say that the first term 1/n is the convergence rate?   Some parts of the paper are tough to understand for standard readers in the machine learning field.  Some examples are: In the last paragraph of page 3, \"indecomposable\" module is undefined.  In Figure 1, it is not clearly explained what is depicted in the middle and right figures.  More explanations are needed.   The paper may not be easy to read for most researchers in the machine learning field.   While I understand the difficulty of explaining persistent homology to non-experts, the authors could demonstrate better the importance of the problem to be solved in this paper.  Also, I recommend the authors include a brief introduction about the basic notions of persistence homology in Supplementary Materials. The paper is generally well-written, but some parts should be improved for clarity.  See Weakness.   The paper contains sufficiently novel contributions to multiparameter persistent modules in topological data analysis. I think that the paper contains significant contributions to the problem of multiparameter persistent modules, which is one of the important issues in topological data analysis.  However, the presentation of the paper has much room for improvement, as detailed in Strength and Weaknesses. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 4: The contributions are significant, and do not exist in prior works. 4: The contributions are significant, and do not exist in prior works. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 152,
        "instruction": "Improving Protein Interaction Prediction using Pretrained Structure Embedding: The prediction of protein-protein interactions (PPIs) is a critical problem because the knowledge of PPIs unravels the cellular behavior and its functionality. So far most previous works on PPI predictions mainly focused on sequence and network information and ignored the structural information of protein physical binding. We design a novel method, called xxx, which can leverage pretrained structure embedding and can be transferred to new ppi predictions. Experimental results on PPi predictions show that our pretrained structure embedding leads to significant improvement in PPI prediction comparing to sequence and network based methods. Furthermore, we show that embeddings pretrained based on ppi from different species can be transferred to improve the prediction for human proteins. ",
        "reference": "In this paper, authors propose to adopt pre-trained structure prediction models to extract meaningful structural embeddings for protein-protein interaction (PPI) prediction. Such structural embeddings are fed into a graph neural networks to formulate the possible interaction between different proteins. Two datasets of PPI networks (antibody-target interaction and membrane protein interaction) are used to validate the effectiveness of the proposed method. Pros:  The motivation is well founded, since unlike link prediction in social networks, similar proteins may not interact and interacting proteins may not be similar. Thus, explicit usage of structural embeddings should be critical in accurate protein-protein interaction prediction. Additional experiments are conducted to verify the transferability of PPI prediction models across different species.  Cons:  Limited contribution and novelty. The pre-trained structural embedding is directly adopted from OmegaFold, and it seems that the main contribution in the methodology aspect is using graph neural networks with structural embeddings as inputs for PPI prediction. Since initial structural embeddings may have different length, authors adopt mean pooling along the sequence dimension to obtain fixed-length representation of each protein. Such protein-level embedding may fail to capture local structures actually participating the interaction with other proteins. The results of \u201cPSE4PPI-SAGE\u201d and \u201cPSE4PPI-GAT\u201d on ATI and H-PPI datasets are not consistent in Table 2 and 3. Please clarify.  Some minor issues:  Table 1. Why the number of unique nodes in the ATI dataset is only 2? Table 2, 3, and 4. It would be better to use bold numbers to highlight the best results in each column. Based on my understanding, by default, OmegaFold does not take an actual MSA as inputs. Instead, it generates a pseudo MSA by randomly masking a certain ratio of amino-acid residues in the original sequence multiple times. Please clarify which type of MSA is used in the proposed method. Additional proofreading may be needed to correct typos and grammatical mistakes in the current manuscript. Clarity: Mostly satisfying. Some minor issues have been listed above. Quality: Limited contribution. Novelty: Somehow limited. I was expecting proposing a novel protein structure pre-training method and then validate it on the PPI prediction task, rather than directly adopting OmegaFold\u2019s embeddings and feeding into standard graph neural networks for prediction. Reproducibility: Good. Detailed hyper-parameter settings are described. The overall motivation is reasonable and convincing, as similar proteins do not necessarily interact with each other, and additional structure information should be considered. However, the proposed method is of limited novelty and contribution, which may be less qualified to be accepted by ICLR. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 153,
        "instruction": "Batch Normalization and Bounded Activation Functions: Since Batch Normalization was proposed, it has been commonly located in front of activation functions, as proposed by the original paper. Swapping the order, i.e., using Batch Normalization after activation functions, has also been attempted, but it is generally not much different from the conventional order when ReLU is used. However, in the case of bounded activation functions like Tanh, we discovered that the swapped order achieves considerably better performance on various benchmarks and architectures than the conventional order. We report this remarkable phenomenon and closely examine what contributes to this performance improvement in this paper. One noteworthy thing about swapped models is the extreme saturation of activation values, which is usually considered harmful. Looking at the output distribution of individual activation functions, we found that many of them are highly asymmetrically saturated. The experiments inducing a different degree of asymmetric saturation support the hypothesis that asymmetric saturation helps improve performance. In addition, we found that Batch Normalization after bounded activation functions has another important effect: it relocates the asymmetrically saturated output of activation functions near zero. This enables the swapped model to have higher sparsity, further improving performance. Extensive experiments with Tanh, LeLecun Tanh, and Softsign show that the swapped models achieve improved performance with a high degree of asymmetric saturation. ",
        "reference": "This paper conducts an empirical analysis of the interaction between batch normalization and bounded activation functions. Specifically, the paper compares the architecture using batch normalization after a bounded activation(Swap model) and the architecture using a bounded activation after batch normalization(Convention model). Motivated by the observation that the swap model outperforms the convention model significantly when a bounded activation is used, the authors designed experiments to identify the reasons for these performance differences. The paper shows that in terms of asymmetric saturation, the Swap model and the Convention model behave differently and argues that high sparsity induced from the asymmetric saturation has a strong association with the generalization performance. Strengths  The authors discover that in the Swap model with a bounded activation, each feature map is saturated on one side of the asymptotic value of the bounded activation.  Weaknesses  It is a bit confusing whether the asymmetric saturation has a strong association with generalization performance. Although there is another noticeable observation that the saturation is very low in higher block depths, this is not discussed at all. Can you explain more how to exclude the possibility that low saturation at higher blocks or the combination of both could be a reason for better generalization?   It is also confusing whether the sparsity has a strong association with generalization performance Since the sparsity metric is sl=1\u2212tl where tl is the saturation metric, layerwise sparsity can be obtained from Figure 3. The relation between the sparsity of the Swap model and the Convention model is different depending on which layer is considered. In such case, it seems a bit of a stretch to draw a conclusion that the higher the sparsity is the better the generalization is.  In a sense, this contradicts with the authors' argument 'Our saturation metric can dismiss the channel properties due to the summarization of channels in the layer.' Can we say that different sparsity distributions over layers with the same average sparsity will have similar generalization performance?   The coverage of the analysis is a bit limited.  The analysis is claimed to be valid with bounded nonlinearity and without residual connection, excluding many widely used architectures. Also, it seems difficult to generalize or apply the claim of the paper to commonly used cases. Even though it is subjective, it does not seem that the Swap model with Tanh performs comparably to the Convention model with ReLU.    Questions  Can you elaborate on ' Because Tanh has non-linearity in everyplace except the origin, it can not follow the design of residual connection proposed'? Does that mean that Tanh has gradient 1 at the origin? What does it mean by nonlinear in everyplace? What does it mean by 'following the design of residual connection'?   In Table 1, with ReLu, the Convention model is better than the Swap model. Have you considered or performed a similar analysis to understand this reversed behavior? What is 'the center of the function'? This term is not defined precisely. The center of the domain of the function or the center of the image of the function? NWDBN is not explained until Figure 8 and is frequently used before Figure 8. Even though I guess that NWDBN may stand for No Weight Decay Batch Normalization, acronyms should be explained when it is first used. What is the formula of LeCun Tanh? There are many typos for LeCun Tanh. Even though the observation that the Swap model with a bounded activation is interesting, the arguments that connect the observation and other experiments to the conclusions are not convincing. It seems that the training details are well-provided enough to enable reproducibility. It is interesting to know that with a bounded activation, the order between BN and the activation causes drastically different qualitative behavior. However, the presentation of the idea can be improved further by replacing vaguely defined terms and expressions. The arguments supporting the conclusions seem weak. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 154,
        "instruction": "MEDOE: A Multi-Expert Decoder and Output Ensemble Framework for Long-tailed Semantic Segmentation: Long-tailed distribution of semantic categories, which has been often ignored in conventional methods, causes unsatisfactory performance in semantic segmentation on tail categories. In this paper, we focus on the problem of long-tailed semantic segmentation. Although some long-tailed recognition methods (e.g., re-sampling/re-weighting) have been proposed in other problems, they are likely to compromise crucial contextual information in semantic segmentation. Therefore, these methods are hardly adaptable to the problem of long-tailed semantic segmentation. To address this problem, we propose a novel method, named MEDOE, by ensembling and grouping contextual information. Specifically, our MEDOE is a two-sage framework comprising a multi-expert decoder (MED) and a multi-expert output ensemble (MOE). The MED includes several ``experts\", each of which takes as input the dataset masked according to the specific categories based on frequency distribution and generates contextual information self-adaptively for classification. The MOE then ensembles the experts' outputs with learnable decision weights. As a model-agnostic framework, MEDOE can be flexibly and efficiently coupled with various popular deep neural networks (e.g., Deeplabv3+, OCRNet, and PSPNet) to improve the performance in long-tailed semantic segmentation. Experimental results show that the proposed framework outperforms the current methods on both Cityscapes and ADE20K datasets by up to 2\\% in mIoU and 6\\% in mAcc.",
        "reference": "This paper focuses on the topic of long tail in semantic segmentation. It designs a model-agnostic multi-expert decoder and output framework, making certain improvements for some classical segmentation models. A diverse data distribution-aware loss function is proposed for preventing over-confidence of minority categories. Besides, it advocates mAcc as a more important metric to evaluate the performance for body and tail categories in long-tailed semantic segmentation. The paper demonstrates some analyses to prove such opinion. Strength\uff1b 1.\tThis paper makes a survey on the topic of long-tail problem in semantic segmentation. 2.\tIt proposes a model-agnostic multi-expert decoder to resolve such a problem and makes certain improvements for some classical segmentation models. 3.\tA diverse data distribution-aware loss function is proposed for preventing over-confidence of minority categories. 4.\tThe paper conducts some analyses to give the ideal results for the proposed method, showing a promising direction of future research. Weakness; 1.\tThis paper claims that it is the first to explicitly focus on the long-tailed semantic segmentation. Yet, There are already some published papers which concentrate on such topic (eg, Region Rebalance for Long-Tailed Semantic Segmentation ,cvpr2022) 2.\tThe paper duplicate the decoder head twice, bringing a large quantity of extra params. I am not pretty sure that whether the improvements are resulted from such a change.  3.  The overall pipeline does not differ much from some multi-expert methods in long-tail classification/detection. I would recommend to compare with some of them, as they also conduct segmentation experiments such as \"Distribution Alignment: A Unified Framework for Long-tail Visual Recognition\" 4. The paper propose that the mAcc metric is a more important metric for long-tail segmentation, giving degenerated mIoU results and better mAcc results in some experiments. Yet, this problem is also mentioned by paper \"Region Rebalance for Long-Tailed Semantic Segmentation in cvpr2022\" and has been well addressed. This paper is of good clarity and quality in writing, and is well-organized. Yet, it should have compared with some existing methods in long-tailed classification, which also provide semantic segmentation results. Besides, the novelty is rather limited. Such a method is widely used in multi-expert long-tail classification/detection task. As for the results, I believe that such results can be easily reproduced. This paper presents the long-tail problem in semantic segmentation and claims that they are the first to explicitly focus on the long-tailed semantic segmentation. Yet, there are already some published papers focusing on such topic (eg, Region Rebalance for Long-Tailed Semantic Segmentation, cvpr2022). Besides, the main idea of this paper is widely adapted by multi-expert long-tailed methods. The paper also advocates that the mAcc metric is more important for long-tail segmentation, and gives degenerated mIoU results. Yet, the paper, Region Rebalance for Long-Tailed Semantic Segmentation, also encounters such a problem and well address this problem. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 155,
        "instruction": "TransFool: An Adversarial Attack against Neural Machine Translation Models: Deep neural networks have been shown to be vulnerable to small perturbations of their inputs known as adversarial attacks. In this paper, we consider the particular task of Neural Machine Translation (NMT), where security is often critical. We investigate the vulnerability of NMT models to adversarial attacks and propose a new attack algorithm called TransFool. It builds on a multi-term optimization problem and a gradient projection step to compute adversarial examples that fool NMT models. By integrating the embedding representation of a language model in the proposed attack, we generate fluent adversarial examples in the source language that maintain a high level of semantic similarity with the clean samples and render the attack largely undetectable. Experimental results demonstrate that, for multiple translation tasks and different NMT architectures, our white-box attack can severely degrade the translation quality for more than 60% of the sentences while the semantic similarity between the original sentence and the adversarial example stays very high. Moreover, we show that the proposed attack is transferable to unknown target models and can fool those quite easily. Finally, our method leads to improvement in terms of success rate, semantic similarity, and fluency compared to the existing attack strategies both in white-box and black-box settings. Hence, TransFool permits to better characterize the vulnerability of NMT systems and outlines the necessity to design strong defense mechanisms and more robust NMT systems for real-life applications.",
        "reference": "This paper defines a new optimization objective function which combines the fluency, similarity and translation error to adversarially attack machine translation models. A gradient projection algorithm is applied to solve this optimization. Experiment results show the proposed method outperforms baselines. The transferability is also examined. Strength  The proposed method is simple and straightforward.  The experiment results show superior performance in attack success rate and significant decrease in translation quality.  This work also demonstrates the capability and efficiency of blackbox attack.  weaknesses  Existing adversarial attack on NMT aims at improving the robustness of these models. However, in this work, achieving a high attack success rate seems to be the goal. Therefore, I\u2019m wondering if TransFool is as effective as baselines in improving robustness. Or what is the desired use case for this method? Translation models use beam search or similar mechanisms to generate high-quality output. Is the proposed attack still effective when using these mechanisms? Missing human validation. Although automatic metrics show significant decrease in translation quality, I\u2019m not convinced that the algorithm triggers incorrect translation. Maybe the translation is correct but has a very low BLEU or chrF score (i.e., the attack method is attacking the automatic metrics instead of the NMT model, which could also be an interesting finding). The space after figures and tables are being squeezed too much. The novelty of the proposed method is somewhat incremental.  The experiment part is strong in terms of datasets and models. The automatic evaluation metrics also show strong performance.  I think adding a human evaluation would reveal lots of insights. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 156,
        "instruction": "Protein Sequence Design in a Latent Space via Model-based Reinforcement Learning: Proteins are complex molecules responsible for different functions in the human body. Enhancing the functionality of a protein and/or cellular fitness can significantly impact various industries. However, their optimization remains challenging, and sequences generated by data-driven methods often fail in wet lab experiments. This study investigates the limitations of existing model-based sequence design methods and presents a novel optimization framework that can efficiently traverse the latent representation space instead of the protein sequence space. Our framework generates proteins with higher functionality and cellular fitness by modeling the sequence design task as a Markov decision process and applying model-based reinforcement learning. We discuss the results in a comprehensive evaluation of two distinct proteins, GPF and His3, along with the predicted structure of optimized sequences using deep learning-based structure prediction.",
        "reference": "The manuscript presents a procedure for protein engineering using a model-based reinforcement approach building on the ESM2 language model. The authors demonstrate that the high dimensional ESM2 representation can be mapped to a lower dimensional representation space which is suitable for optimization, and that full amino acid sequences can be reconstructed from this reduced representation with meaningful accuracy. They then propose a off-policy reinforcement learning procedure to learn how to make updates in the representation space. Finally, the authors evaluate the method on two protein engineering datasets, and report convincing results. The paper is well written, the method well described, and the results are convincing. Please see below for detailed comments to specific parts of the paper. Clarity The paper is clearly written, with excellent figures to support the story. Quality The quality of the paper is high. Reproducibility The authors have not made source code available as part of their submission, making it difficult to fully assess reproducibility. The method is, however, presented at a level of detail that should make it possible to reproduce the results. I strongly encourage the authors to share their source code upon acceptance of their paper. Detailed comments Page 1. \"To tackle this problem, in this paper we propose...\" This paragraph, and especially this sentence, suggests that latent space optimization of proteins is new, ignoring recent work on Bayesian Optimization of proteins on latent spaces, such as \"Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders\" (ICML 2022). This paragraph should therefore be rephrased. This same ICML2022 reference should also be added to the \"Biological sequence design\" subsection on the next page, and ideally compared to in the results section. I am not affiliated with this paper in any way - but merely suggesting it because the methods are closely related, and both are contestants to constitute the current state of the art - so comparing them head-on would be relevant to the community. Page 3: \"k is predicted from a representation q with dimensions (L, E), and\". Page 4: \"As a result of performing the action a_t, the agent receives the reward r_t.\". Page 5: \"a dense reward, defined as r_t= f(s_t)\".  The last two statements seem to be at odds with the first statement. Originally, f is described as acting on a q representation of size (L,E), but later, the reward is calculated based on s_t, which is only E-dimensional. Please clarify. Page 4. \"The dataset proposed in (Sarkisyan et al., 2016) is used to train the GFP encoder-decoder and its functionality predictor.\" It would be helpful if you could discuss somewhere in the paper how many parameters are involved in estimating the task-specific projection to lower dimensions (I assume that this is just ExR) and whether this estimation becomes problematic for smaller datasets that the one you studied here. Likewise for the oracle - although one could presumably use a general oracle instead of a task specific one in this case. Page 4. \"Datasets were split into train and test sets.\" How? Just uniformly? Page 5. \"We compare with four optimization methods\" I assume these are all optimized on the same oracle(?). As mentioned earlier, it would be beneficial to the community if you compared directly to the ICML2022 method here as well - if at all possible. Page 6. \"The sequence alone cannot easily convey structural information about a protein\u2019s functional sites, making it unsuitable for guiding the optimization process.\" I'm surprised by these results. In Table 2, you show that Random mutations work well as an optimization strategy on His3. Why does it fail completely in the experiment in Table 3? Wouldn\u2019t you at least expect to recover the most important sites in the protein? Does this result perhaps primarily indicate a weakness in your policy optimization rather than the representation itself? Page 7. \"...is compared to random perturbation and BO,\" When writing \u201cBO\u201d, are you referring to a general Bayesian Optimization procedure - or the specific one that you cite earlier (Swersky et al., 2020)? Since you are using a continuous latent space here, I assume this is now a different BO - in which case you should make the distinction clear - and explain what the setup is - is it a standard GP-based BO with a expected improvement acquisition function? I would also suggest that the authors make it clear that this is just one particular (and perhaps particularly simple) choice of BO. Page 7. \"Fig. 2 shows that the functionality predictor trained without negative examples incorrectly predicts a high value for non-functional sequences (mean=4.002)\" Isn\u2019t it odd that the oracle predicts a higher average functional value than any value it has observed during training? Does this indicate something is flawed with the oracle? Page 7. \"Fig. 3(d) presents optimization steps taken by the trained policy, which now shows that large (optimistic) perturbations taken by the policy often lead to failure in optimization.\" It was not quite clear to me what this figure is meant to illustrate. Are the large steps failure modes of the learned policy? (i.e. should it have learned to prevent such steps?) Page 7. Discussion It would be helpful if the authors somewhere in the paper discussed the source of improvement over e.g. Bayesian Optimization. The BO and reinforcement learning literatures have quite different terminology and it can be a bit difficult to see exactly which components make a difference in practice.  Is it the fact that a policy is learned vs the fixed acquisition function typically used in a BO setting?  Minor Page 1. \"The first cause is that the optimization process is usually performed by generating candidate sequences through amino acid substitutions\" Slightly odd statement, since any method (including the one proposed here), will ultimately use amino acid substitutions (or insertions/deletion). Perhaps writing \"sequences directly through amino acid substitutions\" would be better? Page 3. \"a 2-dimensional vector representation q \u2208 Q of dimensions (L, E)\" Slightly confusion that the representation is both 2 dimensional and has dimension L,E. Consider rephrasing. Page 4. \"Negative examples are defined as random sequences with a zero functionality value.\" Do you standardize the functionality values in any way? Otherwise, 0 seems like an arbitrary value. Page 5. \"We set three experimental rewards\". At this point in the text it is not clear whether these losses will be used simultaneously or as alternatives. Perhaps add \"alternative\" here to make this clear. Page 5. \"The performance evaluation metric is calculated as the mean log-fluorescence intensity from the top K generated sequences.\" For clarify, perhaps make it clear that this is according to the oracle, and not the ground truth values. Page 5. \"CbAsoptimize\" Missing space The paper proposes a new method for protein engineering. It is well-written, carefully documents its claims, and demonstrates convincing results. I have only minor suggestions for edits to the paper. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 157,
        "instruction": "The GANfather: Controllable generation of malicious activity to expose detection weaknesses and improve defence systems.: Criminal activities are typically adversarial in nature, where an attacker and a defence system are constantly adapting to each other's behaviour. If the defence systems are helped by automated detection methods, then those methods need to be updated frequently. In practice, this means that the defence systems are always one step behind the attackers. For example, in anti-money laundering systems, new labels representing suspicious activity are frequently delayed by weeks or months and some money laundering activity may never be found, leading to detection systems that are inaccurate and resulting in an estimated undetected \u20ac0.7-3 trillion being laundered annually.\n\nTo tackle the problem of missing or delayed labels in adversarial settings, we propose The GANfather, an adversarial and label-free method to both (1) generate a variety of meaningful attacks, as guided by a custom, user-defined objective function; and (2) train a defence system to detect such attacks. Optionally, we can ensure that the generated attacks escape an existing detection system, revealing current weaknesses which the new defence system actively corrects. Our method is inspired by generative adversarial networks (GANs), but unlike GANs we nudge our generator to produce out-of-distribution data using a loss function that characterises criminal activity. Importantly, our method does not require any labelled examples.\n\nWe test our framework in two real-world use-cases, namely injection attacks in recommendation systems and anti-money laundering. In the former, we show how an injection attack with a limited number of generated fake profiles is sufficient to successfully recommend an item to a large number of users. These generated injection attacks are more effective in recommending the target item than naive \u2018bombing\u2019 strategies and harder to detect. In the latter, the generated attacks are able to simulate money laundering and move cumulative amounts close to 250 thousand dollars through a network of accounts without being detected by existing systems. We also show how we can train a new defence system that captures all these synthetic attacks, potentially saving millions of dollars in detected criminal activity. Our method is generic and applicable in a variety of adversarial domains, exposing current liabilities with the generated data and strengthening the defence systems against current and future malicious attacks.",
        "reference": "The authors propose a GAN based approach to generate attacks and also claim to enhance the detection of illicit activity in various domains. They use three type of loss functions to train a generator to generate the attacks. The domain they focus on is money laundering and recommendation systems. The authors claim there methods applies to other settings The idea here is not novel as there are a number of papers that use a GAN like structure for generating OOD samples (or adversarial examples) - see references below, of course, mostly focussing on image domain. This paper does not work on image domain, which is actually a strength of the paper. But, I did not see any insights that are generalizable or even some domain specific hard structural problem that is addressed (i.e., the loss functions used are quite natural and probably too simple). I am not sure why these objective were chosen, is there some citation to support these? I was also hoping to see some quantitative measure of how good the attack is, which in the paper is just shown qualitatively I do not understand the part about discriminator - a line in the paper says that \"the discriminator eventually learns to distinguish synthetic attacks from real data\", but isnt a GAN supposed to eventually make the discriminator not be able to distinguish (by learning a good generator)? Along same lines, I do not understand what is going on in Figure 4? It seems rather like adversarial training that a neural network is trained with OOD data and is able to then detect those. But, then the generator can be trained again to bypass this new discriminator? Baluja, S., & Fischer, I. (2018). Learning to Attack: Adversarial Transformation Networks. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). Liu, A., Liu, X., Fan, J., Ma, Y., Zhang, A., Xie, H., & Tao, D. (2019). Perceptual-Sensitive GAN for Generating Adversarial Patches. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01) Xiao, C., Li, B., Zhu, J. Y., He, W., Liu, M., & Song, D. (2018). Generating adversarial examples with adversarial networks. In 27th International Joint Conference on Artificial Intelligence, IJCAI 2018 (pp. 3905-3911). International Joint Conferences on Artificial Intelligence. The paper is written clearly and I have not checked the code for reproducibility, but the authors present enough details. I believe the novelty is limited - see weaknesses listed. My recommendation is based on my perception of limited novelty and lack of new generalizable principles to be taken away from this work. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 158,
        "instruction": "Proximal Validation Protocol: Modern machine learning algorithms are generally built upon a train/validation/test split protocol. In particular, with the absence of accessible testing set in real-world ML development, how to split out a validation set becomes crucial for reliable model evaluation, selection and etc. Concretely, under a randomized splitting setup, the split ratio of the validation set generally acts as a vital meta-parameter; that is, with more data picked and used for validation, it would cost model performance due to the less training data, and vice versa. Unfortunately, this implies a vexing trade-off between performance enhancement against trustful model evaluation. However, to date, the research conducted on this line remains very few. We reason this could be due to a workflow gap between the academic and ML production which we may attribute to a form of technical debt of ML. In this article, we propose a novel scheme --- dubbed Proximal Validation Protocol (PVP) --- which is targeted to resolve this problem of validation set construction. Core to PVP is to assemble a \\emph{proximal set} as a substitution for the traditional validation set while avoiding the valuable data wasted by the training procedure. The construction of the proximal validation set is established with dense data augmentation followed by a novel distributional-consistent sampling algorithm. With extensive empirical findings, we prove that PVP works (much) better than all the other existing validation protocols on three data modalities (images, text, and tabular data), demonstrating its feasibility towards ML production.",
        "reference": "This paper identifies the problem of 'split tradeoff', where common train/validation splits for data result in either less data to train on or poorer model evaluation. Instead, the paper introduces Proximal Validation Protocol (PVP), which, instead of splitting the validation set from a train set, creates a validation set through a combination of: 1) data augmentation on the train data and 2) a devised sampling scheme to preserve the distribution of the augmented validation set to be close to the train set. PVP allows one to use the whole training data in order to train models while still having a useful validation set for model evaluation, thereby avoiding the split tradeoff problem.  Experiments on three data modalities: tabular, image and natural language seem to demonstrate that PVP improves both the test performance of trained models, as well as reducing the gap between test and validation performance (i.e. validation is a better evaluation of model performance), whilst maintaining a competitive variance across different runs of their method (with different validation sets). Strengths:  The paper tackles a clear issue (how best to choose a validation set) which has received little attention in the ML literature. The proposed scheme, using data augmentation to generate a validation set, appears to be novel and goes some way to solving the issue of 'split tradeoff' problem.  Weaknesses:  The paper seems to only use compare validation sets in terms of how well they evaluate the performance of a fixed model (the hyperparameters/architectures in section A.4 all appear to be fixed and pre-defined), whereas the purpose of validation sets for model evaluation in ML, to me, is to be able to select between different models/hyperparameters. I think it is very important for the contribution of this paper to demonstrate having better model evaluation via PVP's validation set does actually lead to better model selection e.g. better hyperparameter tuning. If this is already shown in the experiments then it should be made clearer/explicit.  More generally, the paper is not presented clearly, at least to me. This is particularly the case in the experiments section, which is particularly problematic given that the paper focuses on showing the empirical strengths of PVP, rather than theoretical justification. This makes it hard to give a proper evaluation of the paper's contributions.   To start with, the 'F1' score is not properly defined, there is some alluding to it being the (top-1?) test accuracy in definition 3.1, which is what I take the definition to be (correct me if wrong). In any case, it is not clear what the values in table 3 and 4 represent: the word 'deterioration' alludes (but not in a clear way, and should be properly defined in any case) to the difference in each metric between using a proximal validation set vs. standard validation set (in table 3), and using random sampling vs. distributional-consistent sampling (in table 4). The reason why this lack of clarity is confusing is that if I understand correctly, there should be no difference between the F1 scores (test accuracies) in the settings presented in tables 3/4. This is because the choice of validation set shouldn't affect the test performance (for a fixed model trained on the same training data, see point 1 above), and hence that column should read as 0.  I'd also like to see more thorough empirical evaluation, e.g. to what extent does PVP depend on the choice of data augmentation. What happens to performance if we do just say additive Gaussian noise as our data augmentation? There are many choices that one can make for the data augmentation: is the empirical performance of PVP robust to this choice? Likewise, there seems to be a non-standard softmax-like weighting using for the class center in equation 6, is there a reason for this? Can the authors provide an ablation to this choice of weighting (say to just a standard uniform weighting)?  PVP seems to be designed for classification settings (particularly the distribution-preserving sampling), does it also extend to other settings say regression, or non-supervised learning regimes like self-supervised or unsupervised? Clarity is discussed in weakness above. The propsed PVP idea appears original to me.  The paper is generally of decent quality, though the writing could be improved (suggestions below):  Some quite unusual language is used for a scientific paper e.g.  'plague', 'entertained', 'frustratingly', 'profoundly'. I don't think this contributes much to the message of the paper and would reword. 'combo' -> 'combination' deterioation -> deterioration 'The split-relied framework and suffer the train/validation split tradeoff': doesn't make sense to me. I am recommending weak reject. The problem identified is important and the proposed solution appears novel, but the experiments are both unclear and (as far as I understand) lacking in thoroughness, as detailed in my weakness section. I'm happy to be corrected though. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 159,
        "instruction": "Help Me Explore: Combining Autotelic and Social Learning via Active Goal Queries: Most approaches to open-ended skill learning train a single agent in a purely sensorimotor environment. But because no human child learns everything on their own, we argue that sociality will be a key component of open-ended learning systems. This paper enables learning agents to blend individual and socially-guided skill learning through a new interaction protocol named Help Me Explore (HME).\nIn social episodes triggered at the agent's demand, a social partner suggests a goal at the frontier of the agent's capabilities and, when the goal is reached, follows up with a new adjacent goal just beyond. In individual episodes, the agent practices skills autonomously by pursuing goals it already discovered through either its own experience or social suggestions. The idea of augmenting an individual goal exploration with social goal suggestions is simple, general and powerful. We demonstrate its efficiency on two notoriously hard exploration benchmarks: continuous mazes and a 5-block robotic manipulation task. With minimal social interventions, an HME-agent outperforms the purely social agent deprived of its autonomy, and the purely individual agent which fails to solve hard exploration problems.",
        "reference": "This paper argues that artificial agents may benefit from socially-guided skill discovery. This paper presents a protocol for active learning that enables an agent to query a partner for a goal. The partner is an auxiliary agent that maintains a model of the probability that an agent could reach goals in the goal-space. The paper claims that this agent outperforms existing baselines, including curriculum-based agents. Strengths:  This paper takes care to embed its understanding social agents within the broader interdisciplinary literature.  Concerns:  In figure 2a, the success rate of multiple agents is depicted across episodes. There are a few aspects of this plot that I find concerning  i. I am concerned that the experiment was run over hours rather than a standardised number of time-steps or episodes. By limiting the learning by wall-time, both the algorithm and the implementation of an agent are being evaluated. This means that code-level implementation choices that are independent of the learning method being evaluated will influence the reported performance. This is not a fair comparison. ii. I am concerned that the results are reported over 5 seeds. Reporting the standard deviation over results for so few seeds is misleading. Moreover, given the results are overlapping in many locations, it's difficult for me to conclude that HME outperforms the comparators. iii. I am struggling to understand the star notation in figure 2a. In this case, I'm not sure what the \"left-most\" algorithm is. I am assuming it is HME-50. It's unusual to report statistical differences across time-series data as presented. It would be better to report over summary statistics (error after learning). I have doubts that with 5 seeds that the data meets the requirements for a T-test (e.g., normality). The paper should provide information that demonstrates that this data meets the assumptions for the tests that have been done.  you're more likely to find a statistically significant difference when you're making many comparisons. To account for this, you should use a Bonferroni correction (or similar). In that case, it would require dividing by the number of comparisons made. As a result alpha will be less than 0.05, I believe. This would mean that the evaluation episodes for which there is a statistically significant difference would decrease.  Reporting the actual P values would be helpful in this case. iv. At this point, I might wonder why the confidence intervals were not plotted, given the comparisons made. If you have the confidence intervals, why would standard deviation be reported for the error bars? v. It is claimed that \"non-social baselines reach a plateau of around 0.75\". Examining the figure, it seems that all but go-exp ss achieve scores higher than 0.75, and that the average final performance of most are somewhere around 0.8-0.9. This looks like a misrepresentation of the data.  vi. It looks like HME-50 and HME-50b perform equitably in terms of success rate. This suggests that there is little difference in final performance between suggesting goals that are exclusively beyond the agent's perceived skill level, and suggesting goals that are at the frontier of the agent's skill.  some aspects of the evaluation strategy are unclear  The agent is evaluated on \"interesting evaluation goals that humans could easily describe\". What makes a goal interesting or easy to describe isn't elaborated on, making it challenging to assess whether this is a reasonable evaluation strategy.  Figure 2b could be interpreted in several ways  I am struggling to understand what is being reporting in figure 2b. To my understanding, It counts the number of positions that are reached by the agent that are---according to the SP agent---adjacent to unknown configurations (configurations that have yet to be achieved?). In this case, HME-50 B reaches more stepping stone configurations across most episodes with lower variance(?) than HME-50. If I understand this correctly, it means that the agent is reaching more positions that are perceived by the SP to be at the frontier of the underlying agent's ability.  it is concluded that \"this suggests that the role of the frontier goal is to enable the agent to reach the beyond goal from the first time.\" If I understand this claim correctly, we cannot conclude this from the data presented. All we can say is that the agent that is proposing goals beyond its skill level is able to reach the frontier more regularly.  A plot that depicts the number of new goals an agent reaches, or the rate at which the frontier expands, would be better able to support the claims being made in the paper, to my understanding. i. Figure 3 is mentioned twice in the text, from what I can tell. There is no Y axis labeled, and there is no description in the caption: I cannot tell what is being plotted here without hunting through the text. In the text an average success metric, and a social request metric are defined. I don't know what it means for the agents to have a score of 600 in one of the categories. I do not know what the error bars are.  Labelling is absent from figure 5 as well. ii. The axes are different for each sub-plot, making them difficult to visually compare. iii.  \"As shown in the top row of Figure 3, ACL agents do not even discover the hardest configurations.\" In the top row of figure 3, I see HME-50 included. It would be helpful if you direct the reader's attention to specific sub-figures.  It is unclear why 50 was the beta parameter chosen for the baseline comparisons. Examining figure 5  seems to suggest that HME-20 has the best performance (although there is higher variance).  (stylistic suggestion) Figure 4 is challenging to interpret. In this figure, the success rate for all beta values chosen is presented. In this case, I cannot readily see the difference between the beta values---especially for the less complex stacking classes---because all of the lines overlap. Grouping the lines by category and presenting all the beta values on a single plot would help with the interpretability. Right now, it is challenging for me to see the difference between the success rates. For instance, one plot with HME 20, 50, 100, 200 plotted for the success rate of S4.  Frequently the claims of the paper extend beyond the truth.   In a couple of locations very strong claims are made.  agents must learn to organize their own learning trajectories by prioritizing goals with the objective of maximizing long-term skill mastery  Must is a very strong word. Agents may benefit from prioritising goals, but it's not a necessity. Certainly many sparse reward problems have been solved by other means.   [e-greedy action selection] is not enough to experience informative rewards in hard exploration problems  If an agent is following a random behaviour policy, it can experience reward in sparse environments (as defined in the paper). It might be highly improbable, but it is certainly not impossible.  Minor points:  Terms are used without being defined (e.g., SP is used several times before being defined in 3.2) Some of the citations feel a bit strange. For instance, Mnih et al. 2015 is used as a citation for epsilon-greedy action selection. Issues in clarity discussed above:  minor typographical issues claims extend beyond evidence ordering of ideas makes it challenging to comprehend axes on plots are left unlabeled, making it difficult to interpret empirical results terms are used before being introduced In several places, the claims of the paper extend beyond the evidence plots are not clearly explained, making it a challenge to interpret the empirical claims  For these reasons, I suggest a strong reject. 1: The main claims of the paper are incorrect or not at all supported by theory or empirical results. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 1: The contributions are neither significant nor novel. NO. 1: strong reject 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 160,
        "instruction": "AUTOMATIC CURRICULUM FOR UNSUPERVISED REIN- FORCEMENT LEARNING: Recent unsupervised reinforcement learning (URL) can learn meaningful skills without task rewards by carefully designed training objectives. However, most existing works lack quantitative evaluation metrics for URL but mainly rely on visualizations of trajectories to compare the performance. Moreover, each URL method only focuses on a single training objective, which can hinder further learning progress and the development of new skills. To bridge these gaps, we first propose multiple evaluation metrics for URL that can cover different preferred properties. We show that balancing these metrics leads to what a \u201cgood\u201d trajectory visualization embodies. Next, we use these metrics to develop an automatic curriculum that can change the URL objective across different learning stages in order to improve and balance all metrics. Specifically, we apply a non-stationary multi-armed bandit algorithm to select an existing URL objective for each episode according to the metrics evaluated in previous episodes. Extensive experiments indifferent environments demonstrate the advantages of our method on achieving promising and balanced performance over all URL metrics.",
        "reference": "For unsupervised reinforcement learning, this paper proposes (1) a set of metrics to evaluate the exploration and skill learning of the agent without specific downstream tasks; (2) an automatic curriculum to train the agent with different intrinsic rewards in different stages, formulated by multi-armed bandit. Strength:  The paper is well-written and easy to understand. The approach is novel and well motivated. The experimental results are clearly showcased.  Weaknesses:  The experiments are only based on a single 2D maze environment, which makes the results not convincing enough. There are other more realistic environments to further test on, e.g., https://arxiv.org/pdf/2110.15191.pdf. There is no results to show if the proposed unsupervised RL method can really help in downstream applications, although the proposed approach doesn't require assumptions on specific downstream task. The paper has great clarity. The proposed approach is novel, and the experiment settings are clear. The paper is well-written and the proposed approach is novel, but the experiments are only based on a single 2D environment makes the results not fully convincing. Besides, as an unsupervised approach, it's not tested in any downstream applications. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. Not applicable NO. 6: marginally above the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 161,
        "instruction": "Filtered Semi-Markov CRF: Semi-Markov CRF \\citep{semicrf} has been proposed as an alternative to the traditional Linear Chain CRF\\citep{crf} for text segmentation tasks such as Named Entity Recognition. In contrast to CRF, which treats text segmentation as token-level prediction, Semi-CRF considers spans as the task's basic unit, which makes it more expressive. However, Semi-CRF has two major drawbacks: (1) it has quadratic complexity over sequence length as it operates on every span of the input sequence, and (2) empirically, it performs worse than classical CRF for sequence labeling tasks such as NER. In our work, we propose Filtered Semi-Markov CRF, a Semi-CRF variant that addresses the aforementioned issues. Our model extends Semi-CRF by incorporating a filtering step for eliminating irrelevant segments, which helps in reducing the complexity and allows to dramatically reduce the search space. On a variety of NER benchmarks, we find that our approach outperforms both CRF and Semi-CRF models while being significantly faster. We will make our code available to the public. ",
        "reference": "The paper proposes a semi-Markov conditional random field model that integrates a filtering step to eliminate irrelevant segments when performing named entity recognition in text. According to the authors this helps reducing the complexity compared to a semi-Markov CRF and dramatically reduces the search space. Strengths Attempt to learn segmentation jointly with classification (here named entity recognition or NER) that does not rely on an expansion of the label space (as is commonly done in NER by specializing the labels into begin label, intermediate and outside label). Weaknesses     The model relies on an extra classifier that filters segments. The model is jointly trained by considering the filter classification loss, the segmentation loss and the NER classification loss. It is not clear from the paper how during training the computational complexity is reduced as initially all potential segments need to be considered in the inference step during training. That means that the complexity is equal to the complexity of the semi-Markov model.  The authors acknowledge that \u201cduring training, especially in the first stage, the graph size can be large since the filtering is poor.\u201d It is not clear how exactly the filtering is performed during training.     Because the segment filtering is the contribution of the paper, its approach (which should be explained in detail) should be separately evaluated, that is, its behavior and reduction in complexity during training and ablating the assumptions made here.     Results could have been evaluated on many more NER datasets.     The claim that the proposed filtering model drastically reduces the search space compared to a CRF and SemiCRF model is not seen in the model\u2019s throughput numbers, especially not during training. Moreover, the effect of dynamic programming on the model\u2019s throughput during inference on the test data could be investigated. Is dynamic programming also used in the decoding of the CRF and SemiCRF models? The clarity (and consequently the reproducibility) could be improved (see remark above). The originality is difficult to judge given that the details of the contribution are missing. The paper is below the threshold for acceptance at ICLR because of:  Important details and evaluations are missing.  Several of my questions were answered during the rebuttal, for which I thank the authors. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 162,
        "instruction": "Distance VS. Coordinate: Distance Based Embedding Improves Model Generalization for Routing Problems: Routing problems, such as traveling salesman problem (TSP) and vehicle routing problem, are among the most classic research topics in combinatorial optimization and operations research (OR). In recent years, with the rapid development of online service platforms, there has been renewed interest in applying this study to facilitate emerging industrial applications, such as food delivery and logistics services. While OR methods remain the mainstream technique, increasing efforts have been put into exploiting deep learning (DL) models for tackling routing problems. The existing ML methods often consider the embedding of the route point coordinate as a key model input and are capable of delivering competing performance in synthetic or simplified settings. However, it is empirically noted that this line of work appears to lack robustness and generalization ability that are crucial for real-world applications. In this paper, we demonstrate that the coordinate can unexpectedly lead to these problems. There are two factors that make coordinate rather `poisonous' for DL models: i) the definition of distance between route points is far more complex than what coordinate can depict; ii) the coordinate can hardly be sufficiently `traversed' by the training data. To circumvent these limitations, we propose to abandon the coordinate and instead use the relative distance for route point embedding. We show in both synthetic TSP and real-world food pickup and delivery route prediction problem that our design can significantly improve model's generalization ability, and deliver competitive or better performance with existing models. ",
        "reference": "This paper proposes an interesting observation: when learning to solve routing problems, the distance-based input is better than the coordination based (which is currently the most used one) input.  The basic intuition behind it is that distance is an essential property of the routing problems. To validate this observation, the authors employ the current SOTA transformer models and apply them to two models (1) TSP and (2) real-world FPD. They replace the input with two types: (1) the distance-based input and (2) the concatenation of the coordination input and distance-based input.  Moreover, they design some perturbations for the test data to test the robustness of the models. The results show that the distance-based input can improve the model's performance. Strength:     (1) The observation is very simple and interesting. If this is shown to be the correct one, this can help improve the current deep models for routing problems almost with zero overhead. Weaknesses:     (1) Given that the claims are for all the routing problems and all architecture. So the current experiments seem to be far from enough. It would be better to consider more problems and more architecture. At least, I think one GNN-based model is essential since, as the paper mentioned, the GNN model itself also considers the distance in the edge feature. Questions:     (1) For table 1, what does the left distribution means? Does it seem to be the city location's distribution?     (2) You tune the hyper-parameters of the original algorithm. What if you do not tune? And is the new input format sensitive to the hyper-parameters? This paper proposes a simple and interesting observation. However, current experiments seem not to be enough to fully support it or the authors may restrict it a little bit. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 163,
        "instruction": "Towards biologically plausible Dreaming and Planning: Humans and animals can learn new skills after practicing for a few hours, while current reinforcement learning algorithms require a large amount of data to achieve good performances. \nRecent model-based approaches show promising results by reducing the number of necessary interactions with the environment to learn a desirable policy. However, these methods require biological implausible ingredients, such as the detailed storage of older experiences, and long periods of offline learning. The optimal way to learn and exploit word-models is still an open question.\nTaking inspiration from biology, we suggest that dreaming might be an efficient expedient to use an inner model. We propose a two-module (agent and model) neural network in which \"dreaming\" (living new experiences in a model-based simulated environment) significantly boosts learning. We also explore \"planning\", an online alternative to dreaming, that shows comparable performances. Importantly, our model does not require the detailed storage of experiences, and learns online the world-model. This is a key ingredient for biological plausibility and implementability (e.g., in neuromorphic hardware).",
        "reference": "The authors propose a recurrent spiking network-based reinforcement learning algorithm, in which the network consists of two major parts (an agent and a model). The agent is trained to act using policy gradient, while the model is trained to predict the effect's of the agent's actions (both the reward and the state transition). The authors apply their approach to the game of Pong, showing that dreaming indeed improves performance. They also consider an alternative to dreaming (namely, planning), which showed similar performance. Strengths:  The paper addresses a number of highly relevant and important problems Using spiking networks for reinforcement learning is fundamentally challenging, but the authors manage to successfully handle the task The authors consider an expansion of their method to image-based inputs  Weaknesses:  The paper focuses on many targets at once, such as biological plausibility, applicability in robotics, suitability to neuromophic hardware, as well as general usefulness of the approach as a general-purpose reinforcement learning algorithm. I believe that while some of these goals are correlated, they are not the same. As a result, the paper produces an unfocused impression. I believe that narrowing down the range of claims and delving deeper into one of these topics would greatly benefit the paper. The range of applications considered is limited. The method works reasonably well when applied to problems that are fundamentally extremely low-dimensional, but it's not clear whether more complex systems can be managed in a similar manner. While the authors say that \"The approach described above allows, in principle, to apply our method to any other Atari games from pixels\", it remains unclear whether this suggestion is practical. The novelty of the approach is limited. While the use of spiking neural networks in this specific context is novel, the idea of using model-based simulations or dreaming to enhance learning is very well known. Again, I believe that additional, more focused results might help to mitigate the issue.  I clarify and expand upon some of these points in the main body of my review. ** Clarity ** The paper is generally well written and is a pleasure to read. I feel that the fact that the paper has many ambitious goals at once sometimes makes the paper harder to follow. For example, neither the title nor the abstract mention spiking networks, and create the impression that the idea of \"dreaming\" is the key invention of the paper. At the same time, arguably, it's the tricky adaptation of spiking networks to the task which is the main achievement of the paper. This and similar issues complicate reading and result in an impression that the paper attempts to balance a number of related, but not equivalent goals. ** Quality ** My main issue with the experimental planning is that the authors list many ambitious goals, but none of them (in my opinion) receives commensurate depth of exploration and justification. For example, consider the following paragraph: \"To our knowledge, there are no previous works proposing biologically plausible model-based reinforcement learning in recurrent spiking networks. Our work is a step toward building efficient neuromorphic systems for autonomous robots, capable of learning new skills in real-world environments. Even when the environment is no longer accessible, the robot optimizes learning by reasoning in its own mind. These approaches are of great relevance when the acquisition from the environment is slow, expensive (robotics) or unsafe (autonomous driving).\" While I agree that using spiking networks is indeed more biologically plausible than traditional artificial neural networks, I found the discussion of biological plausibility insufficiently deep. For example, the authors stress the fact that memory replay is biologically implausible, but I can not fully agree with that. Humans have reasonable episodic memory. Even though it is not infinite capacity, I don't believe that one can discard all episodic replay as biologically implausible. A deeper look is warranted into how the effects of dreaming in spiking networks is more similar to human dreaming than using, say, exponentially decaying replay buffers in traditional reinforcement learning algorithms. Additionally, since learning is not done online, the biological plausibility argument becomes even less convincing. The claims about the usefulness of the approach in autonomous driving and robotics similarly don't receive enough confirmation. While I understand the difficulties associated with training spiking networks and I don't expect a state-of-the-art performance across a range of image-based RL tasks, it is still crucial to look at how the performance scales with the size of the network and the dimensionality of the problem. For example, a toy task, similar to what is used in human experiments with multiple cue learning tasks (see https://psycnet.apa.org/record/2009-24669-008 for a review) could be a good testbed to see how the performance scales with the complexity of the task (It could also show that, potentially, spiking neural networks produce more human-like behaviour, and be used to investigate biological plausibility). Lastly, the main limitation of all model-based approaches is that what happens if the model can not easily fit the environment. In humans, we know that dreams do not copy our real experience, but are still apparently useful in learning. If the \"dreaming\" part of the contribution is key, a deeper look into what happens when the model can not properly fit the task is warranted. Additionally, perhaps a more systematic investigation (on simulated tasks) is warranted, to answer the question of what are the conditions when dreaming is useful. For example, are there task where even an important model is very useful? Are there cases where learning a model is easier/faster than learning a policy? And so on. ** Novelty/Impact ** While the approach is sufficiently novel, the novelty of the paper is not high enough to be its main selling point. The work is heavily building upon existing ideas, which puts more importance on the depth and thoroughness of experimental support. ** Reproducibility ** The approach is described clearly, but the code is not provided. I can imagine that some researchers might struggle with implementing the algorithms in an exactly the same way as the authors and reproducing the results. ** Typos/phrasing ** still, a clear and coherent understanding of the mechanisms that induce generalized beneficial effects is still missing - \"still\" is repeated. Taking inspiration from biology, an intriguing idea is that one possible usage of a learned model, is during periods in which the neural network is offline. -- I'd suggest rephrasing. E.g. \"Taking inspiration from biology, we explore an intriguing idea that a learned model can be used when the neural network is offline.\" Overall, the paper presents a number of fascinating ideas and has a huge potential. At present, unfortunately, I believe that the balance between breadth and depth was not optimal, which does not allow me to recommend the paper for acceptance. I believe that re-structuring the paper, either around its main technical achievement - adapting spiking networks to work in new circumstances, or around its main conceptual claim (biological plausibility) would greatly benefit the paper. In both cases, however, additional experiments would be needed to increase the impact of the contribution, and to more fully understand the properties of the proposed system. ** UPD ** After reading other reviews and the authors' responses, I leave my assessment unchanged. While some concerns were addressed, I believe that a more thorough reworking of the paper is necessary for it to be up to the standards of the ICLR conference. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 164,
        "instruction": "Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration: The high effectiveness of neural models of code, such as OpenAI Codex and AlphaCode, suggests coding capabilities of models that are at least comparable to those of humans. However, previous work has only used these models for their raw completion, ignoring how the model reasoning, in the form of attention weights, can be used for other downstream tasks. Disregarding the attention weights means discarding a considerable portion of what those models compute when queried. To profit more from the knowledge embedded in these large pre-trained models, this work compares multiple approaches to post-process these valuable attention weights for supporting code exploration. Specifically, we compare to which extent the transformed attention signal of CodeGen, a large and publicly available pre-trained neural model, agrees with how developers look at and explore code when each answering the same sense-making questions about code. At the core of our experimental evaluation, we collect, manually annotate, and open-source a novel eye-tracking dataset comprising 25 developers answering sense-making questions on code over 92 sessions. We empirically evaluate five attention-agnostic heuristics and ten attention-based post processing approaches of the attention signal against our ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement. Beyond the dataset contribution and the empirical study, we also introduce a novel practical application of the attention signal of pre-trained models with completely analytical solutions, going beyond how neural models\u2019 attention mechanisms have traditionally been used.\n",
        "reference": "This paper presents an eye tracking dataset for code understanding task in three programming languages (Python, C++, C#). The dataset contained 92 visual attention sessions of 25 developers. Using this eye tracking dataset, authors study the correlation between model attention and developer\u2019s attention. Authors empirically evaluate different attention-based post processing approaches of the model\u2019s attention signal against the ground truth of developers exploring code. Strengths:  The proposed dataset will be a nice contribution to the community as it can be used for additional code understanding tasks.  Proposed \u201cfollow-up attention\u201d is very intuitive and is shown to have better correlation with human attention than other approaches.  Weaknesses:  Authors study only a single code model Codegen and a single generic language model GPT-J. It\u2019s not clear how model accuracy, model size play a role on the correlation between human and model attention. In particular, do we observe better correlation for larger and/or more accurate models ? On reading comprehension tasks, [3] show that a more accurate model does not necessarily have a stronger correlation so it's important to study such correlation for different models.  I have reservations about the usefulness of this study. Paltenghi & Pradel (2021) have also studied correlation between human and model attention. Further, multiple prior works [1, 2, 3] have shown that human attention is aligned with several natural language understanding tasks. Authors do not discuss how their work is different from the existing works and in what ways it advances our understanding.  Some of the design choices are not clear. In particular, authors say that unlike  Paltenghi & Pradel (2021) they study attention at char level. Why? I think developers focus on the whole variable name instead of a single char in a variable name so it\u2019s not obvious why char level analysis is better than token level analysis.  References:   [1] Bensemann, J., Peng, A., Prado, D., Chen, Y., Tan, N., Corballis, P. M., ... & Witbrock, M. J. (2022, May). Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics (pp. 75-87). [2]  Eberle, O., Brandl, S., Pilot, J., & S\u00f8gaard, A. (2022, May). Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 4295-4309). [3] Sood, E., Tannert, S., Frassinelli, D., Bulling, A., & Vu, N. T. (2020). Interpreting attention models with human visual attention in machine reading comprehension. arXiv preprint arXiv:2010.06396. [4] Paltenghi, M., & Pradel, M. (2021, November). Thinking Like a Developer? Comparing the Attention of Humans with Neural Models of Code. In 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE) (pp. 867-879). IEEE. Clarity, Quality:  Paper is well written and is easy to follow. Experimental design is discussed in detail. While the paper cites the relevant literature in the code analysis domain, I think authors should also cite relevant NLP papers studying correlation between human and model attention.  Novelty:  The proposed dataset is novel.  To the best of my knowledge, the proposed \u201cfollow-up attention\u201d approach is also novel.  Reproducibility:  Authors plan to release the dataset so results presented in the paper should be reproducible. While I don't see any issue with the study design and findings, I have reservations about the usability of this work. In particular, I don't think that this work advances our understanding of the relationship between model attention and developer attention in a significant way. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 165,
        "instruction": "Scrunch: Preventing sensitive property inference through privacy-preserving representation learning: Many tasks that are commonly performed by devices attached to the Internet are currently being offloaded to the cloud, using the Machine Learning as a Service (MLaaS) paradigm. While this paradigm is motivated by the reduced capacity of mobile terminals, it also hinders privacy associated with the data exchanged over the network. Thus, the data exchanged among parties shall be conveniently anonymized to prevent possible confidentiality and privacy issues. While many privacy-enhancing algorithms have been proposed in the past, they are usually relying on very complex models that make difficult their applicability to real-world systems or envision too friendly attacker models. In this paper, we propose a deep learning system that creates anonymized representations for the data, while keeping the accuracy for the targeted MLaaS task high, assuming that the attacker can re-train an adversarial model. Our results show that the proposed algorithm i) is effective yet it uses a lighter approach than state-of-the-art ii) considers less friendly attacker models, and iii) outperforms the benchmark under different privacy metrics.",
        "reference": "This work studies the problem of privacy preserving representation learning. The goal here is to enable secure inference without revealing the client data. The central idea is to split a neural network into two parts \u2014 and sending the first part to client. The client applies raw data through the first part ofthe neural network (with some additional loss function) and then sends the encoded data to the server for final inference. The key idea is that by controlling the information at the encoding stage privacy of protected attributes is preserved. The experimental analysis compares this method with another method in prior art, namely Shredder, and the non-private counterpart. There two metrics of privacy employed \u2014 one to see how much information is revealed by the first part and second one to see the see how much information is revealed in the encoding step. I do not think the idea of splitting a network into two parts is a new one, especially in the context of privacy. The idea of adding additional constraints in the encoding process to control mutual information flow is an interesting take. Unfortunately, I am not convinced about its effectiveness. For a primarily emprical paper, the work considers rather simplistic datasets and only two of them. In the second dataset, they do not compare it with any baselines and in the first dataset it is compared with 1 baseline. There are lot of other experiments that can be performed and baselines that can be and should be compared with. For example, why not compare with fully homomorphic encryption based approaches? Overall I think the paper is rather incremental with no \u201caha\u201d factor that is needed in a conference such as ICLR. The paper is reasonable but there are some grammatical errors (perhaps through out the paper).. For instance,  \u201clast advances\u201d ==> \u201clatest advances\u201d page 1, intro, para 1 \"It is fundamental to remark\" .. you probably mean \"it is important to remark\"..  The novelty is quite incremental.. I think the paper is an incremental effort over the state of the art methods. The experimental section is rather limited for an experimental paper. It needs a lot more benchmarks and results on multiple datasets and baselines. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 1: The contributions are neither significant nor novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 166,
        "instruction": "GM-VAE: Representation Learning with VAE on Gaussian Manifold: We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent space consists of a set of diagonal Gaussian distributions. It is known that the set of the diagonal Gaussian distributions with the Fisher information metric forms a product hyperbolic space, which we call a Gaussian manifold. To learn the VAE endowed with the Gaussian manifold, we first propose a pseudo Gaussian manifold normal distribution based on the Kullback-Leibler divergence, a local approximation of the squared Fisher-Rao distance, to define a density over the latent space. With the newly proposed distribution, we introduce geometric transformations at the last and the first of the encoder and the decoder of VAE, respectively to help the transition between the Euclidean and Gaussian manifolds. Through the empirical experiments, we show competitive generalization performance of GM-VAE against other variants of hyperbolic- and Euclidean-VAEs. Our model achieves strong numerical stability, which is a common limitation reported with previous hyperbolic-VAEs.",
        "reference": "The paper introduces and studies a new type of hyperbolic latent space called Gaussian manifold in the context of variational auto-encoder in order to tackle numerical and sampling issues with existing hyperbolic latent spaces. To make this latent space works in practice, the authors designed a geometric transformation at the last encoder step and the first decoder step; as well as a novel distribution over Gaussian manifold called a pseudo Gaussian manifold normal distribution that makes use of information geometry, easy to sample from, and easy to compute KL divergence. Finally, comprehensive testing on empirical datasets are carried out to highlight the effectiveness of the scheme. Strengths  The proposed method is novel, interesting and makes use of clever engineering details that are rather intuitive. Empirical results clearly indicate that the proposed method is superior in terms of numerical stability to existing hyperbolic latent space methods, over certain datasets (such as binarized-Breakout). In fact, in my opinion, the main selling point of the method should be in terms of numerical stability and the authors may consider bringing the discussion in Appendix E into the main text.  Empirical results in terms of negative test likelihood do not suffer, compared to other methods.  The introduction to hyperbolic latent space is comprehensive and self-contained. Overall I find the language clear and the flow easy to follow, despite small typos.  Weaknesses  Some notations are not clearly defined. In figure 1, the calligraphic E space (I suspect is Euclidean space) is not defined and the calligraphic L^2 space is only defined in passing in subsection 3.3. I suggest that the authors at least make the notation in the figures and its caption self-contained.  What does the triangle in Table 1 mean? How is it different from a circle? Although a lot is mentioned about information geometry and the incorporation of closed form KL, the paper does not clearly mention (or hint at) specific mathematical guarantees for the proposed model. As a result, a lot of the justifications for the proposed method are heuristical and do not form concrete theoretical backings.  Empirical results are rather limited in quantity. There are only a total of 3 datasets and the authors only demonstrate better stability in 1 of these 3 datasets. The proposed methods also do not seem to scale as well as competing methods when increasing the latent space dimension. Clarity: The introduction and background sections are well written and comprehensive and the paper as a whole is self contained. However, some notations are not clearly defined, especially in figures.  Quality: The authors identify several challenges in the current use of hyperbolic latent space for VAE and proceed to tackle them with some success. However, I would like to see more empirical evidence that support the strengths of the method (currently one \u2153 datasets is in favor of the proposed method) Novelty: Aspects of the new hyperbolic structure have been studied for a while (as a statistical manifold). The novelty comes from the geometric transform and the proposed distribution on the Gaussian manifold. Reproducibility: Experiments in the papers are described in detail and the code is given in supplementary material. I have not run the code myself due to lack of time but I believe that the results are reproducible, or otherwise can be quickly shown to be wrong. Despite the paper containing some interesting and novel ideas on how to make the Gaussian manifold suitable for VAE setting, I find that the theoretical motivation for said ideas is heuristical at best and there is not much empirical evidence to demonstrate the strength of the proposed method. Therefore, I recommend rejection. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 167,
        "instruction": "Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples: \nAdversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.",
        "reference": "This paper a new adversarial training method to improve the robustness of deep learning classifiers in the field of computer vision. To do so, the authors derived a new loss function, which is the surrogate of an upper bound of the robust risk. Specifically, they point out the differences and connections between the proposed method and previous adversarial training method. Experiments on three datasets (CIFAR10, F-MINIST, SVHN) show that the proposed method outperforms other baselines in terms of clean classification accuracy and robust accuracy. Ablation studies are done to show the effect of different parts of the loss function. They also show that the proposed method can be combined with other adversarial training techniques, such as extra data, to further improve the performance. Finally, experiments on CIFAR10 show that the proposed method is helpful to improve the fairness of the classifier compared to TRADES (which is an important baseline). Strength:  Adversarial robustness is an important security issue in the field of deep learning. The proposed method pushes the SOTA performance of adversarial training to a new level. The proposed loss function is derived with a theoretical support. Extensive experiments show the empirical advantages of the proposed method from different aspects.  Weakness:  The novelty of the method is ok, but it is similar to previous methods like MART. The author does point out the differences between the proposed method and other methods, so should not be a big problem. It's good that for table 1 and table 2 the results are based on 3 runs with standard errors given, but for most results, the improvements seem marginal. CIFAR10, F-MNIST, SVHN are all relatively small datasets. Does the method also perform well on larger dataset? Clarity:  The paper is clearly written and easy to follow.  Quality:  The quality is good. The claims are clear and experiments are solid.  Novelty:  Somewhat novel though similar to some previous methods.  Reproducibility:  Code and implementation details are provided, so the reproducibility should be good. Though, I did not run the code to test. Overall it's a good paper with with theoretical justification and experimental support. Though, there are some weakness in terms of novelty and the limitation of the datasets used, it is a paper above the margin. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 168,
        "instruction": "Causal Explanations of Structural Causal Models: In explanatory interactive learning (XIL) the user queries the learner, then the learner explains its answer to the user and finally the loop repeats. XIL is attractive for two reasons, (1) the learner becomes better and (2) the user's trust increases. For both reasons to hold, the learner's explanations must be useful to the user and the user must be allowed to ask useful questions. Ideally, both questions and explanations should be grounded in a causal model since they avoid spurious fallacies. Ultimately, we seem to seek a causal variant of XIL. The question part on the user's end we believe to be solved since the user's mental model can provide the causal model. But how would the learner provide causal explanations? In this work we show that existing explanation methods are not guaranteed to be causal even when provided with a Structural Causal Model (SCM). Specifically, we use the popular, proclaimed causal explanation method CXPlain to illustrate how the generated explanations leave open the question of truly causal explanations. Thus as a step towards causal XIL, we propose a solution to the lack of causal explanations. We solve this problem by deriving from first principles an explanation method that makes full use of a given SCM, which we refer to as SC$\\textbf{E}$ ($\\textbf{E}$ standing for explanation). Since SCEs make use of structural information, any causal graph learner can now provide human-readable explanations. We conduct several experiments including a user study with 22 participants to investigate the virtue of SCE as causal explanations of SCMs.",
        "reference": "The authors list 5 technical contributions:  the structural causal explanations algorithm - \"a new algorithm (SCE) for computing explanations from SCM making them truly causal explanations by construction\" an illustration of \"how SCE fixes several of the shortcomings of previous explainers\" an application of \"SCE algorithm to several popular causal inference methods\" (I assume this is referring to the statement in the abstract, \"Since SCEs make use of structural information, any causal graph learner can now provide human-readable explanations.\") a discussion \"using a synthetic toy data set [of] how one could use SCE for improving model learning\" \"a survey with 22 participants to investigate the difference between user and algorithmic SCE\" (I assume this is referring to the statement in the abstract, \"We conduct several experiments including a user study with 22 participants to investigate the virtue of SCE as causal explanations of SCMs.\")  The authors also make the following claims in their abstract and introduction (and in the main body of the paper), which appear to be distinct from the claims listed in their technical contributions section:  SCE is a step forwards in terms of (causal) explanatory interactive learning (XIL). \"In explanatory interactive learning (XIL) the user queries the learner, then the learner explains its answer to the user and finally the loop repeats. [...] Thus as a step towards causal XIL, we propose a solution to the lack of causal explanations.\" CXPlain does not output \"truly causal\" explanations. \"[...] we propose a solution to the lack of causal explanations. Specifically, we use the popular, proclaimed causal explanation method CXPlain to illustrate how the generated explanations leave open the question of truly causal explanations. [...] as we show in this work [CXPlain] still leave[s] open the question of truly causal explanations\"  The explanations generated by SCE are human-understandable because they can be expressed using natural language. \"We provide a new, natural language expressible (thus, human understandable) explanation algorithm with SCE.\" \"Throughout this paper we provided several arguments in advocacy of Causal XIL as the key paradigm of interest for future research and application.\" By using SCE, any causal graph learner can provide human-readable explanations on any query of interest, and this was not possible before. \"Since SCEs make use of structural information, any causal graph learner can now provide human-readable explanations. [...] [Theorem 1] tells us that any causal graph learner ever invented and that will ever be invented can provide causal explanations on any query of interest consistent with the learned model thus reflecting the learnt. [sic]\" Strengths The paper's topic is very relevant to the field of explainable AI and the particular subfield of explanatory interactive learning (XIL). The concept of an algorithm which recursively traces the ancestors of a variable which the algorithm is explaining to construct an explanation, as in Definition 4, is interesting and novel to my knowledge. The concept exists in Hall's notion of production but it hasn't been used to algorithmically construct explanations to my knowledge. Weaknesses Overall, it seems that very strong claims are made in the introduction and abstract, and the claims are significantly weakened when the technical contributions are listed. Because these strong claims are not supported by the paper, they should not be present. I address these claims first, as they seem to be the strongest.  SCE leads to improvements in (causal) XIL. XIL is mentioned heavily in the abstract/introduction and in the discussion of Figure 1, which illustrates SCE being used in an XIL setup (a setup where users repeatedly query a system for explanations to better understand it). However, it is not shown in the paper that SCE actually leads to a better user experience in XIL. XIL and the setup in Figure 1 is not mentioned again in the body of the paper until section 4.1, where the authors argue CXPLain (the prior method) has shortcomings with respect to SCE, and in section 4.4, where the authors attempt to answer the question \"What does SCE explain about the causal intuition that humans have that could provide for Causal XIL?\". I do not see any mention of XIL in the arguments for SCE over CXPlain in section 4.1 or the analysis of the study in section 4.4. A human study would be useful to demonstrate this is the case. CXPlain does not output \"truly causal\" explanations. Already, \"truly causal\" is a vague term which is not defined in the paper. Section 4.1 argues that SCE is superior to CXPlain because it provides additional information, but it is not shown that CXPlain's output is wrong or not \"truly causal\". In the example given (Figure 2), it seems reasonable to me to state that Hans' age is the main contributing factor to his (lack of) mobility, the explanation CXPlain seems to give, and the paper doesn't explain why this might be wrong. Thus, the claim that CXPlain does not output \"truly causal\" explanations doesn't seem to be supported in the paper. SCE explanations are human-understandable because they can be expressed using natural language. First, I examine the claim that SCE explanations can be expressed using natural language. It seems that this is true for rules 1 and 2 (excitation and inhibition). However, rule 3 doesn't actually seem to correspond to the phrase \"Y is mostly because of [...]\"; it seems that it would be more accurate to say \"[...] contributes the most to Y\", and this may not constitute an explanation. For instance, it seems wrong to say \"The amount of money a charity received was high mostly because of Alfred, who donated 2 dollars\" while 500 other people each donated 1 dollar. So I conclude that SCE explanations can indeed be expressed using natural language, but these expressions may be misleading and may not align with what we intuitively consider an explanation. Further discussion of this point in the paper is warranted. Next, I examine the claim that SCE explanations are human-understandable due to their ability to be expressed using natural language. I could not find any content in the paper supporting this claim (including the human study in section 4.4, which did not show SCE explanations to humans); a human study should be used to support this claim. The paper argues in advocacy of Causal XIL. I am unable to find any such arguments after reading the paper and searching for the term \"XIL\" in the paper in or before section 4.4, where the claim is made. Any causal graph learner can now provide human-readable explanations on any query of interest. First, I examine the claim that this was not possible before (implied by the usage of the word \"now\"). I agree that causal graph learners' purpose is not to provide explanations. Next, I examine the claim that this is possible now. First, as discussed in point (8), there are issues with claiming that the generated explanations are now human-readable due to misalignment with human intuition. Second, it is not shown that explanations on larger graphs (for instance, 50-100 nodes) are human-readable. Third, causal graphs learners generally do not output a single graph but an equivalence class of graphs, and SCE doesn't operate on equivalence classes of graphs. Fourth, SCE is limited to queries comparing variables' values to their means (Def 1); this seems far from \"any query of interest\". Thus, there is still a large gap between causal discovery algorithms and human-readable explanations on any query of interest, which SCE does not bridge.  Next, I address the listed technical contributions. Overall, contributions 3-5 are not clearly stated, and their significance is unclear even with the context of the introduction and abstract. Contribution 1 requires more engagement with the prior literature on explanations and a clear statement + discussion of its limitations and assumptions. Contribution 2 relies on two qualitative arguments in section 4.1 which I do not think are sound.  The structural causal explanations (SCE) algorithm + the claim that SCE explanations are \"truly causal\" Regarding the construction of the SCE framework, there seems to be very little justification for Definitions 1-3 when taking into account prior literature.  1.1. Regarding Def 1, why are \"Why\" questions constrained to variables' relationships with their means (Def 1)? Why aren't all kinds of why questions allowed, as in (Halpern, 2016)? It is stated after Explanation 1 that our \"Why\" questions are asked about individuals; if this is the case, are the unobserved variables in the SCM fixed?  1.2. Regarding Def 2, how is each causal effect \u03b1X\u2192Y specifically defined? Is this a unit-level causal effect specific to a setting of unobserved variables U, or an average causal effect (a.k.a. average treatment effect)? If it is a unit-level causal effect, how is this supposed to be obtained, given that it is not possible to observe each variable's counterfactuals without the SCM? Is the assumption that we have the full SCM and the specific unobserved variable setting? If so, this should be stated explicitly in the contributions and the abstract, as it is even stronger than the assumption of just knowing the full SCM. And is the full SCM required? Or just the causal graph? If so, a causal graph is not sufficient to deduce all interventional effects from observational data, unless the graph is Markovian; if Markovianity is assumed, it should be explicitly stated as an assumption in the abstract/contribution section.  1.3. Regarding Def 3., what is the intuitive justification for each rule? Examples in cases of linear SCMs are given. As mentioned in my comments on claim (8), we can obtain unintuitive behavior with rule 3 even in linear cases. I think we can also obtain unintuitive behavior with rules 1 and 2 if the causal effect of increasing & decreasing is on average 0, but there are specific regions of increase/decrease where there is an effect of increasing/decreasing the variable (e.g., a sine curve). How does this change the justification for each rule? If the assumption is that each function is linear and/or monotonic, this should be stated explicitly in the contributions and the abstract; in addition, justification for each rule should be given, as well as discussion of the rules' limitations.  1.4. Regarding Def 4., it seems like a causal scenario (\u03b1X\u2032\u2192Y\u2032,x\u2032,y\u2032,\u03bcx\u2032,\u03bcy\u2032) is required for every pair of variables among the ancestors of the explained variable X. Is this correct? This isn't made clear in Def 2, and the causal scenario is referred to as a \"single tuple\", which makes it seem that there is only one causal scenario inputted to the SCE algorithm. In addition, why is the dataset D required for the SCE algorithm if the causal effects \u03b1Z\u2192X are already known from the causal scenario? Is there some additional step of transforming a dataset to a causal effect which is not included among the definitions? And from what distribution (observational, interventional, etc.) is the data in the dataset drawn?  1.5. Finally, it's not clear what \"truly causal\" means (as opposed to \"causal\" or \"purportedly causal\"), as this is not defined in the paper. The paper argues that SCE's outputs are \"truly causal\" by construction; it is not clear to me why this is the case.  SCE fixes several of the shortcomings of previous explainers. Section 4.1 argues that SCE is superior to CXPlain because 1) some aspects of the causal graphical structure (variables with direct vs indirect effects) cannot be distinguished using CXPlain's outputs and 2) CXPlain's explanation does not give information on how changing each input variable will change the outcome.  2.1. The observation seems to be true, as CXPlain does not output any kind of recursive or graphical structure. But why is this a shortcoming? Is the explanation given by CXPlain wrong? Can't users look at the causal graph (which is required as an input to SCE) to obtain this information?  2.2. This seems to be a valid point about CXPlain. But is it true that SCE does give this information in general? It is certainly true in linear cases, where the causal effect can be modelled with a single value. What about in non-linear cases, such as the sine wave case mentioned in my comments in (1.3)?  This seems to be an action the authors took rather than a result. Why are the applications mentioned in (3) novel/significant?   Improving model learning using SCE. It's not quite clear what \"model learning\" means here; this point should be clarified. If this refers to section 4.3, then there are some points to clarify. What is the exact definition of SCE regularization? How is error computed? Is the difference in Fig 3 statistically significant? Doesn't SCE already take in the true causal graph, so isn't including an SCE-based regularization equivalent to passing the graph learning method additional information on the graph (this of course depends on what the definition of SCE regularization is)? All of these question would need to be answered to assess the benefit of SCE regularization to causal discovery.  This seems to be an action the authors took rather than a result. Do the results of the study mentioned in (5) study support any novel/significant claims about SCE?   Actionable feedback:  Either remove claims 6-10 from the paper or include them in the listed contributions section and include supporting evidence for them in the body of the paper as suggested above. For instance, XIL seems interesting, but the connection to SCE doesn't seem to be made; it would strengthen the paper to make this connection. In general, please do not include strong claims in the introduction/abstract which are then weakened in the listed contributions section. Remove claims 3-5 from the paper, as they do not seem to be contributions in themselves, and optionally replace them with novel/noteworthy conclusions from the listed application, discussion, and survey. Claim 5 could possibly be replaced with \"analysis of a user survey of 22 participants on 4 explanation settings that explores challenges for explanation methods to tackle in the future\". Regarding claim 2, highlight specific cases where CXPlain fails at its stated goal. For instance, construct two examples where CXPlain outputs the same set of scores but SCE outputs different explanations. Conduct a human study to show that humans find explanations generated by SCE more useful for performing a task (or higher-rated according to some other criteria) than explanations generated by CXPlain. Regarding claim 1, connect with the literature in greater depth to clearly state the assumptions/limitations of Definitions 1-3 in the abstract and contributions sections (or to change Def. 1-3 to encompass explanations as explored in prior work); some pointers include Halpern's approach to defining explanations and causation, which is cited in the paper (Halpern, 2016), and (MIller, 2017 - https://arxiv.org/abs/1706.07269). Clarify answers to the questions above in 1.1-1.4. Remove usage of the phrase \"truly causal\" or define it specifically. Clarity: As mentioned above, stronger contributions are listed in the introduction and abstract than in the technical contributions section after \"we make several contributions:\", so it is hard to determine exactly what the claimed contributions are. Furthermore, the listed contributions are vague; for instance, \"(III) we apply the SCE algorithm [...]\", \"(IV) we discuss [...] how [...] [to] use SCE for improving model learning\", and \"(V) we perform a survey [...] to investigate the difference between user and algorithmic SCE\" do not seem to be results in themselves and require the reader to guess what the authors' actions are demonstrating based on what has been stated in the abstract/introduction. The notation in Def 3 is unclear and hard to read. What are R1 and R2? It seems there should be an exists statement in ER1 and ER2 on R1 and R2. Why is s necessary? Why is the output of ERi(\u22c5)\u2208\u22121,0,1 rather than a binary output? Several other important technical points are unclear because it is not clear how the specifications in text translate to the notation (among others mentioned in the weaknesses section: is the paper examining individual explanations or global/SCM-level explanations, or something in between with population-level explanations?). Quality: The content of the paper does not support its claimed contributions. See the weaknesses section. Originality: The concept of an algorithm which recursively traces the ancestors of a variable which the algorithm is explaining to construct an explanation, as in Definition 4, is interesting and novel to my knowledge. The concept exists in Hall's notion of production but it hasn't been used to algorithmically construct explanations to my knowledge. Reproducibility: The authors released their paper's code. While the paper is targeted towards an important field and introduces an interesting general concept of how to construct explanations, the content of the paper does not support its claimed contributions. The paper also needs reorganization to make its contributions clear, as it states stronger and distinct contributions in its introduction/abstract when compared to those that are listed in its contributions section. The paper also needs to clearly state the assumptions and limitations underlying its method, as described in the weaknesses section (my comments from 1.1.-1.4). Therefore, I recommend that the paper be rejected. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 1: The contributions are neither significant nor novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 169,
        "instruction": "Multi-Agent Reinforcement Learning with Shared Resources for Inventory Management: In this paper, we consider the inventory management (IM) problem where we need to make replenishment decisions for a large number of stock keeping units (SKUs) to balance their supply and demand. In our setting, the constraint on the shared resources (such as the inventory capacity) couples the otherwise independent control for each SKU. We formulate the problem with this structure as Shared-Resource Stochastic Game (SRSG) and propose an efficient algorithm called Context-aware Decentralized PPO (CD-PPO). Through extensive experiments, we demonstrate that CD-PPO can accelerate the learning procedure compared with standard MARL algorithms.",
        "reference": "In this paper, the authors have considered the problem of managing inventory on the replenishments for a number of SKUs to handle the supply and demand. This paper proposes Context-aware Decentralized PPO (CD-PPO) by formulating the problem as a Shared-Resource Stochastic Game (SRSG). The authors have conducted rigorous experiments that CD-PPO learns quickly and achieve a good performance with MARL algorithms. Strengths:  Interesting to address inventory management problem through a Shared-resource Stochastic Game (SRSG)  Introduction of context awareness to avoid non-stationary problem and acceleration of sampling process in estimating and optimizing objective function. Experimentation set up and baselining methods used in the study  Weakness:  Although the authors claim that the CD-PPO algorithm has outperformed MARL but it was indicated that it is due to the avoidance of non-stationarity by introducing the context but there are other methods like IPPO-IR with context showed a poor performance. This needs to be explored in detail. The problem space considered was too small to be accepted in a premier conference like ICLR. Clear standing and providing relevant real-world applications with shared-resource structure will enhance the readability of this article. Rationale for the selection of MARL algorithms needs to be explored in detail. There are several areas where the usage of language needs to be improved considerably. Some of the example are as follows: a. In our setting, the constraint on the shared resources (such as the inventory capacity) couples the otherwise independent control for each SKU. Overall the authors justified the formulation of the problem and conducted experiments to showcase the superiority of the proposed CD-PPO methodology but at several places clarity was not provided which has been showcased in the weakness section. Results and discussion section need to be explored in detail showcasing the superior performance of CD-PPO with the parameters used in the MARL and the proposed algorithms. Especially with respect to sample efficiency, there are not enough evidences /rationale supporting its superior performance. The authors have provided enough information to have the reproducibility. Overall a good research article but needs a lot of improvement to be considered for publication in ICLR 2023. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 1: The contributions are neither significant nor novel. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 170,
        "instruction": "Explicitly Maintaining Diverse Playing Styles in Self-Play: Self-play has proven to be an effective training schema to obtain a high-level agent in complex games through iteratively playing against an opponent from its historical versions. However, its training process may prevent it from generating a well-generalised policy since the trained agent rarely encounters diversely-behaving opponents along its own historical path. In this paper, we aim to improve the generalisation of the policy by maintaining a population of agents with diverse playing styles and high skill levels throughout the training process. Specifically, we propose a bi-objective optimisation model to simultaneously optimise the agents' skill level and playing style. A feature of this model is that we do not regard the skill level and playing style as two objectives to maximise directly since they are not equally important (i.e., agents with diverse playing styles but low skill levels are meaningless). Instead, we create a meta bi-objective model to enable high-level agents with diverse playing styles more likely to be incomparable (i.e. Pareto non-dominated), thereby playing against each other through the training process. We then present an evolutionary algorithm working with the proposed model. Experiments in a classic table tennis game Pong and a commercial role-playing game Justice Online show that our algorithm can learn a well generalised policy and at the same time is able to provide a set of high-level policies with various playing styles.",
        "reference": "The paper introduces a bi-objective optimization method for learning strategies for playing games from a pool of diverse agents. The method is population-based in the sense that it maintains a population of agents that attempts to optimize for a bi-objective function. Such a function accounts for the diversity of play, which is game-dependent, and winning rate.  The system was evaluated in the games of Pong and Justice Online, where the proposed method was competitive in some settings and stronger in others with respect to other population-based agents and RL baselines. Strength This paper deals with a challenging topic, which is the one of learning strategies for playing complex games. The paper works with the underlying hypothesis that one is able to learn stronger strategies while keeping a population of diverse agents. While other population-based works dealt with similar research question, the paper does a good job reviewing some of these works. The experiments in Pong and Justice Online seem to support this underlying assumption. Weaknesses A key weakness of the paper is to not discuss the paper \"A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\" by Lanctot et al (2017). The paper talks vaguely about \"self play\" without explaining exactly which self-play algorithm they are referring to. Lanctot et al. describe a general framework for multiagent RL that allows one to instantiate algorithms such as Iterated-Best Response, Fictitious Play, and Double Oracle. As far as I know these are all self-play algorithms. In particular, Fictitious Play can be seem as a population-based algorithm and should probably be used as a baseline in the experiments. Why not consider these algorithms as baselines?  The paper is also constructed under the assumption that \"maintaining an agent population does not necessarily mean maintaining diverse playing styles.\" (see Related Work section). However, population-based agents such as AlphaStar were designed to learn a diverse set of playing styles. It isn't clear from the experiments whether the proposed method is able to produce \"more diverse agents\" than algorithms such as AlphaStar.  Another weakness is related to how opponents are sampled to generate the results shown in Table 3. The experiments starts with a pool of agents formed by 1 PPO agent, 1 PPT (this is probably a typo and authors meant PBT) agent, 3 Multi-Reward Agents, 30 EMOGI agents, and 30 BiO agents. Then, an opponent is randomly sampled from this pool of agents for each of the evaluated methods. The agents play a game and the result is stored. The process is repeated 60 times and the average win rate is reported in Table 3.  This experimental design is problematic because it gives an advantage to EMOGI and BiO as they represent a much bigger share of the pool of agents. That way, both EMOGI and BiO are likely to face, during test, agents they were trained with. Despite this significant disadvantage, PBT is still competitive with BiO. This makes me wonder whether BiO is bringing something new in comparison to other PBT agents. The paper is clear and mostly well written. I would argue that adding PPO's equation doesn't  help readers who aren't familiar with the method and it doesn't teach anything new to readers already familiar with the method. The paper would be better off without Equation 1.  The appendix provides enough information to possibly reproduce the results from the paper. Paper deals with a challenging and important problem, but misses an important part of the literature that could be used as baseline in the experiments. The empirical design seems to be problematic as it might be giving advantage to the method introduced in this paper. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 171,
        "instruction": "Toward Learning Geometric Eigen-Lengths Crucial for Robotic Fitting Tasks: Some extremely low-dimensional yet crucial geometric eigen-lengths often determine whether an object can be fitted in the environment or not. For example, the {\\em height} of an object is important to measure to check if it can fit between the shelves of a cabinet, while the {\\em width} of a couch is crucial when trying to move it through a doorway. Humans have materialized such crucial geometric eigen-lengths in common sense since they are very useful in serving as succinct yet effective, highly interpretable, and universal object representations. However, it remains obscure and underexplored if learning systems can be equipped with similar capabilities of automatically discovering such key geometric quantities in doing robotic fitting tasks. In this work, we therefore for the first time formulate and propose a novel learning problem on this question and set up a benchmark suite including the tasks, the data, and the evaluation metrics for studying the problem. We explore potential solutions and demonstrate the feasibility of learning such eigen-lengths from simply observing successful and failed fitting trials. We also attempt geometric grounding for more accurate eigen-length measurement and study the reusability of the learned geometric eigen-lengths across multiple tasks. Our work marks the first exploratory step toward learning crucial geometric eigen-lengths and we hope it can inspire future research in tackling this important yet underexplored problem. \n\n",
        "reference": "This paper presents an early attempt at learning geometric lengths from trails and explores how to transfer the learned geometric knowledge to solve a different task. Besides offering a problem formulation, this paper presents an evaluation framework based on finding evaluation correlations against ground truth lengths. Strengths Ideas are clearly elaborated through the paper with adequate support of intuition in explanations. Elaboration of experimentations is adequate for an early exploration of learning object lengths.  Initial results are promising on the capability of learning lengths from trials. Weaknesses This paper is an early exploration of the topic. Consequently, the experimentation is limited to a single dataset and simplistic tasks. Can this methodology be used to improve challenging tasks such as relative pose estimation and active visual navigation? There are no major issues related to the clarity of this paper as the idea is simple and intuitive. Overall this is initial research. My main criticism is the lack of exploration or at least discussion on how to use this idea in more complex problems. I would like to see at least some discussion in this regard. 4: All of the claims and statements are well-supported and correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 172,
        "instruction": "Learning Test Time Augmentation with Cascade Loss Prediction: Data augmentation has been a successful common practice for improving the performance of deep neural network during training stage. In recent years, studies on test time augmentation (TTA) have also been promising due to its effectiveness on improving the robustness against out-of-distribution data at inference. Instead of simply adopting pre-defined handcrafted geometric operations such as croping and flipping, recent TTA methods learn predictive transformations which are supposed to provide the best performance gain on each test sample. However, the desired iteration number of transformation is proportional to the inference time of the predictor, and the gain by ensembling multiple augmented inputs still requires additional forward pass of the target model. In this paper, we propose a cascade method for test time augmentation prediction. It only requires a single forward pass of the transformation predictor, while can output multiple desirable transformations iteratively. These transformations will then be adopted sequentially on the test sample at once before the target model inference. The experimental results show that our method provides a better trade-off between computational cost and overall performance at test time, and shows significant improvement compared to existing methods.",
        "reference": "This paper presents a model for selecting test-time data augmentation to boost a classification model. The proposed method is based on RNN to gradually output multiple augmentations, selected from a predefined augmentation set. The idea is based on loss prediction for finding suitable augmentations. In each step of RNN prediction, the model outputs a predicted loss value and then select the best augmentation. To show the effectiveness of the proposed method, this paper conducts experiments on CIFAR-100 and ImageNet datasets, and compare this method with many other test-time augmentation strategies. Strength:  This paper presents a method, which is able to produce multiple data augmentations with single forward pass.  Weakness:  The novelty of this work is not surprisingly strong. The proposed model is different to previous architecture, however, applying RNN to produce multiple output is a common method.  From the experimental results, it is hard to observe clearly better results than other TTA methods, for example Horizontal-Flip.   How do you train the loss predictor. This is unclear how to train a loss predictor on specific datasets.  How to select the predefined candidate transformations. For individual instances, different transformations are required. How do you prepare the transformation set for the best performance for various datasets.   Need citation after the first sentence in section 3. The overall quality of this paper is not strong enough. There are some issues unclear to me. I don't know how to train the loss predictor and cannot reproduce the results after reading this paper. This paper does not have a clear merit to be accepted by ICLR. They present an architecture for test-time augmentation, however, no clear benefit from the proposed method can be achieved. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 1: The contributions are neither significant nor novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 173,
        "instruction": "Adaptive Computation with Elastic Input Sequence: When solving a problem, human beings have the adaptive ability in terms of the type of information they use, the procedure they take, and the amount of time they spend approaching and solving the problem. However, most standard neural networks have the same function type and fixed computation budget on different samples regardless of their nature and difficulty. Adaptivity is a powerful paradigm as it not only imbues practitioners with flexibility pertaining to the downstream usage of these models but can also serve as a powerful inductive bias for solving certain challenging classes of problems. In this work, we propose a new strategy, AdaTape, that enables dynamic computation in neural networks via adaptive tape tokens. AdaTape employs an elastic input sequence by equipping an existing architecture with a dynamic read and write tape. Specifically, we adaptively generate input sequences using tape tokens obtained from a tape bank that can either be trainable or generated from input data. We analyze the challenges and requirements to obtain dynamic sequence content and length, and propose the Adaptive Tape Reader (ATR) algorithm to achieve both objectives. Via extensive experiments on image recognition tasks, we show that AdaTape can achieve better performance while maintaining the computational cost.",
        "reference": "The paper proposes a novel method to add \"on demand\" computation to transformers by selecting additional tokens from a token bank. In particular the authors use a variation of the adaptive computation time algorithm used in universal transformers to add additional tokens in the input sequence. Experiments with image classification shows that the proposed method can indeed make use of the extra computation to improve the classification results. Strengths  Adaptive computation is a very interesting and important subject, both in terms of saving computation by only computing as much as it is necessary as well as mimicking the human thought process of considering a problem again and again and collecting more information. Adding extra tokens from a token bank is a very intuitive way to inject variable sized pieces of information to a transformer model.  Weaknesses  AdaTape is significantly slower than the baselines often being 2 times slower. This significantly reduces the usefulness of the method since the baselines with a few extra layers would probably perform as well. The experimental evaluation is not very clearly written. In particular the information about finetuning the datasets except for JFT-300 is missing. The usefulness of the adaptive computation abilities of AdaTape is put in question from the experiments in section 3.4 . In particular we see that in most cases, not using the adaptive length algorithm actually yields better results. It would also be significantly faster on GPUs and TPUs. The clarity of the paper can be improved. In particular, the experimental evaluation should contain more details regarding the training and evaluation on the datasets other than JFT-300M. The paper seems a great relatively novel idea, however the experimental evaluation is lacking. The comparisons show that using the proposed method is actually significantly slower albeit better in terms of final accuracy. However, not using the adaptive tape reading algorithm actually performs better and would be possibly faster in modern accelerators. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 174,
        "instruction": "Optimizing Data-Flow in Binary Neural Networks: Binary Neural Networks (BNNs) can significantly accelerate the inference time of a neural network by replacing its expensive floating-point arithmetic with bit-wise operations. Most existing solutions, however, do not fully optimize data flow through the BNN layers, and intermediate conversions from 1 to 16/32 bits often further hinder efficiency. We propose a novel training scheme that can increase data flow and parallelism in the BNN pipeline; specifically, we introduce a clipping block that decreases the data-width from 32 bits to 8. Furthermore, we reduce the internal accumulator size of a binary layer, usually kept using 32-bit to prevent data overflow without losing accuracy. Additionally, we provide an optimization of the Batch Normalization layer that both reduces latency and simplifies deployment. Finally, we present an optimized implementation of the Binary Direct Convolution for ARM instruction sets. Our experiments show a consistent improvement of the inference speed (up to $1.77$ and $1.9 \\times$ compared to two state-of-the-art BNNs frameworks) with no drop in accuracy for at least one full-precision model.",
        "reference": "This paper presents a training scheme to increase data pipeline of binary neural networks when implemented on ARM. To this end, a two-step clipping method and 8-bit batch normalization have been proposed. The clipping method insures that the values can fit into 8 bits so that batch normalization can be performed using the same bitwidth. The proposed method has no impact on accuracy performance and its implementation on Raspberry Pi shows up to 1.9x speedup compared to prior works. Strengths: -- The paper provides an insightful challenges about hardware implementation challenges of binary neural networks on ARM. -- It provides a hardware implementation on ARM and yields up to 1.9x speedup compared to prior works. -- The paper is well-written and easy to understand. Weaknesses: -- The two-step clipping method was introduced to accommodate for 8-bit batch normalization. However, it doesn't seem to be necessary for VGG-style block. According to Eq. (2), all we need to compute is sign of the batch norm's output, which can be achieved by comparing the input of batch norm with the threshold tau. Of course, tau can be represented using 8 bits. Then, the input of batch norm doesn't need to be represented using 32 bits anymore; its 8-bit representation suffices. I am also assuming the scaling factor of binarization is merged with BN, is it a right assumption? For the ResNet-style block, the batch norm is mixed-precision not 8-bit according to Fig. 3. Then, what's the point of representing the input using 8 bits? -- 8-bit batch norm has been proposed before in literature (e.g., https://arxiv.org/pdf/1805.11046.pdf), which undermines the novelty of this work. I believe the paper falls short in terms of novelty (see my comments listed as weaknesses). The contribution of this easy to understand but there are some aspects missing such as scaling factor of binarization. I believe this paper suffers from the lack of motivation and also novelty. The statements of this paper don't explain why two-step clipping is required to make the inputs of BN into 8 bits while BN is in mixed-precision (8-bit and 16-bit). Besides, 8-bit batch norm has been introduced before in different works. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 1: The contributions are neither significant nor novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 175,
        "instruction": "SGD Through the Lens of Kolmogorov Complexity: We initiate a thorough study of the dynamics of stochastic gradient descent (SGD) under minimal assumptions using the tools of entropy compression. Specifically, we characterize a quantity of interest which we refer to as the \\emph{accuracy discrepancy}. Roughly speaking, this measures the average discrepancy between the model accuracy on batches and large subsets of the entire dataset. We show that if this quantity is sufficiently large, then SGD finds a model which achieves perfect accuracy on the data in $O(1)$ epochs. On the contrary, if the model cannot perfectly fit the data, this quantity must remain below a \\emph{global} threshold, which only depends on the size of the dataset and batch.\n\nWe use the above framework to lower bound the amount of randomness required to allow (non stochastic) gradient descent to escape from local minimas using perturbations. We show that even if the model is \\emph{extremely overparameterized}, at least a linear (in the size of the dataset) number of random bits are required to guarantee that GD escapes local minimas in polynomial time.",
        "reference": "The paper considers studies dynamics of mini-batch SGD using Kolmogorov complexity. They define a notion of accuracy discrepancy as a KL-divergence between accuracy of the model on previous batches in the current epoch and on the last batch. Using the fact that the random strings used for generating the epoch\u2019s permutation are incompressible, they bound the accuracy discrepancy throughout the algorithm execution. I have the following concerns about the paper: -- As I understand, the discussion near Theorem 2.2 is really important, since it shows that the provess is invertible. Moreover, the algorithm actually relies on the fact that the set of possible values is finite, to find the original point in finite time. However, the conditions in Theorem 2.2 don\u2019t necessarily hold in practice. To give an example, for f(x)=x^2 / 2, with the step size 1e-16, for the double type, the gradient step starting from points 1e16 + 2 and 1e16 produce the same result. While I understand that the example is artificial, it suffices to show that the process is, in general, not invertible (when working in doubles). I\u2019m not sure, but it might be important for the rest of the paper, since you do rely on the invertibility. I think that it should be clarified why the conditions of Theorem 2.2 actually hold. -- \u201cIt is clear that w.h.p we cannot train a model with d parameters that achieves any accuracy better than 1/2 + o(1) on X\u201d - I\u2019m not sure this statement is true. You probably should use non-random labels and counting argument. Minor issues: -- Theorem 4.3: I don\u2019t understand what \u00bd + \u03b8(1) means -- Instead of \u2207fA~ (and others), I would write \u2207f~A. The long tilde looks really weird -- I think \u201cw.h.p.\u201d is commonly defined as 1 - n^-c for an arbitrary large c -- Page 6:\u201d Where in the above we used Stirling\u2019s approximation\u201d and \u201cWhere the efficiency of reconstruction is expressed via \u03c1_i\u201d: should start commas (instead of being new sentences). -- I think the relation between f and acc was never mentioned Solid paper, accept. 4: All of the claims and statements are well-supported and correct. 4: The contributions are significant, and do not exist in prior works. Not applicable NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 176,
        "instruction": "Identifying Phase Transition Thresholds of Permuted Linear Regression via Message Passing: This paper considers the permuted linear regression, i.e., ${\\mathbf{Y}} = {\\mathbf{\\Pi}}^{\\natural}{\\mathbf{X}}{\\mathbf{B}}^{\\natural} + {\\mathbf{W}}$, where ${\\mathbf{Y}} \\in \\mathbb{R}^{n\\times m}, {\\mathbf{\\Pi}}^{\\natural}\\in\\mathbb{R}^{n\\times n}, {\\mathbf{X}} \\in \\mathbb{R}^{n\\times p}, {\\mathbf{B}}^{\\natural}\\in \\mathbb{R}^{p\\times m}$, and ${\\mathbf{W}}\\in \\mathbb{R}^{n\\times m}$ represent the observations, missing (or incomplete) information about ordering, sensing matrix, signal of interests, and additive sensing noise, respectively. As is shown in the previous work, there exists phase transition phenomena in terms of the \\emph{signal-to-noise ratio} ($\\mathsf{snr}$), number of permuted rows, etc. While all existing works only concern the convergence rates without specifying the associate constants in front of them, we give a precise identification of the phase transition thresholds via the message passing algorithm. Depending on whether the signal ${\\mathbf{B}}^{\\natural}$ is known or not, we separately identify the corresponding critical points around the phase transition regimes. Moreover, we provide numerical experiments and show the empirical phase transition points are well aligned with theoretical predictions.",
        "reference": "The authors study phase transitions in linear regression with permuted labels for both the oracle setting (in which the regression coefficients are assumed to be known) and the non oracle setting (in which both the permutation and the regression vector are considered as unknown). The main result of the paper is an estimator for the signal to noise ratio above which recovery of the permutation matrix is possible in the absence of any estimate for the regression coefficients. The result relies on an estimator for the permutation matrix derived in a previous paper (Hang Zhang, Ping Li, Optimal estimator for unlabeled linear regression) and applies the machinery of Semerjian et al 2020 (cavity method) to derive the phase transition. The paper is interesting although poorly written. I have some concern with (1) the incremental nature of the result; Although there is a clear mathematical venture/achievement and the necessary associated hard work, the novelty lies in part in an approximation (based on a Taylor expansion) used to compute the phase transition when considering the non oracle setting; and perhaps more importantly (2) the organization/intelligibility of the paper.  The first part of the paper is very similar to Semerjian et al 2020 and the second part is an analysis based on the estimator provided in Zhang et al 2020. A reader unfamiliar with the former paper will have a very difficult time going through the notations. See below for more details The paper is notationnally heavy and requires the careful reading of other related papers for a proper understanding (For someone who has not read Semerjian et al, the exposition and the notations are quite difficult to follow). The paper should be self contained which it clearly isn't (at least in its current state). I do believe there is more space required for a clear exposition of the mathematics. My suggestions, if you really want to submit to a conference, would be to remove part of the introduction (which can be found exactly in Semerjian et al.) as well as the proof sketch for Theorem 2 (which is unclear). Then use the spared space to expand on the connection between the oracle and the non oracle settings, discuss the experiments in more details and develop on the non-oracle estimator from Zhang et al 2020 which is a key element of the machinery. I don't think the paper can be accepted in its current state. Although the result is interesting, the paper should either be seriously reorganized or submitted to a journal which would allow for a longer and clearer exposition.  Detailed comments can be found below. page 2  The second item when you expose your technical contributions is not very clear  page 3  In figure 1, it is difficult to read the expression in the red square  page 4 I\u2019m in favor of removing the whole section as it appears exactly in this form in Mezard and Montanari Chapter 16. (A couple of comments can be found below)  Section 3.2. When you say \u201chere L and R are used to denote the positions of the nodes\u201d do you mean the row vs column indexing ? This is unclear Your introduction of the messages in (3) is not clear. Honestly the \\cong notation is misleading, why not use the traditional \\leftarrow. What you are introducing here are the messages updates. Also you should really get rid of the \\hat{m} e^{-\\beta \\pi_{i,k} E}\u2026 that appears in the second line and replace it with the RHS from the first update. Expanding the expression only makes things more confusing than they are Do you really need to keep the L and R indices, that makes the notations so much heavier I feel like the second equation between (3) and (4) in which you introduce the log ratio is redundant.  Is \\zeta (above (4)) the same as \\zeta_{i,j} ? if so you should keep the same notation. The whole section on MP has to be rewritten. You introduce variables from nowhere. Below equation (4), what is \\hat{\\pi} ? We can somewhat guess  that it corresponds to the (estimated) permutation operator but for that same operator you used \u2018\\pi\u2019 on page 2 and \u2018pi^\\sharp\u2019 on page 3. Why not just keep pi for the entries in the matrix \\Pi? It is very difficult to follow on your definition of the cavity fields in Equation (5). In fact you should rewrite the transition. At least add a reference to (5) in the paper of Semerjian. For someone who has not read the paper, it is absolutely unclear where the updated BP equations on the cavity fields come from (in particular the min which appears out of nowhere) Honestly, you should provide a clear reference to section (16) in Mezard and Montanari or (15) in Semerjian page 5 When you introduce the random variables H and \\hat{H}, you skip a number of steps and that makes the reading of the paper quite painful. E.g. you again need to zero temperature limit for (7) to hold if I\u2019m not wrong What you call the message flows are the cavity fields? You can introduce the independent copies because you have equality in the distributional sense (again I think you should specify this, otherwise you lose the reader who is not familiar with the cavity method) The equations in (7) are the same as (48a) and (48b) in Semerjian but if you look at (48b) the first update is carried only with probability (1-q). Also look at (29a) (29b) The transition from (4) to the condition H+H\u2019>\\Omega is unclear. Either provide a reference or expand The definitions of the random variables Omega and K are unclear. If I\u2019m not wrong, you never introduce the K that appears in the statement of Theorem 1. If I\u2019m not wrong the K has a precise definition as a lower bound for H following from a simplification of the Recursive Distributional Equations for H. This should be properly introduced. Also your notion of phase transition is unclear. What is meant by phase transition here? Once again, there is too much content and the essentials are missing. If you look at the Semerjian paper, those notion are carefully introduced. E.g. the authors take the time to explain that in the large n (or in your case t) limit, a value of K (and hence H since K lower bounds H) going to infinity implies that the only solution to the BP equations is the solution H = +infty. we are thus interested in the transition from the setting where there are possibly other solution (i.e. K\\neq \\infty) to this last setting. This explanation should appear in your paper In (12) you introduce an SNR but there is no explanation on where that SNR comes from or even what it encodes. We can sort of guess it is related to the sigma in (6) and on page 2 but the connection is unclear  When you introduce the SNR, is that the SNR between PiXB and W at which the transition occurs ? You should elaborate on this. Is the SNR related to sigma?  page 6  Why is assumption 1 a good assumption ? You say \u201cwidely used assumption\u201d but you don\u2019t provide any reference  Section 4.2. is particularly unclear.    In the discussion before section 5, when you discuss subGaussian matrices What is ENER ?  Honestly, your introduction of the two different approaches to derive the threshold SNR is unclear. From what I understand, you first use the solution of inf 1/\\theta * \\log(\\sum_i \\mathbb{E} e^{-\\theta \\Xi_i}) for which you have to compute the average  E{e^{-\\theta \\Xi}} exactly and then you use the taylor expansion approach (which you want to use in the non oracle case). You motivate the second approach by saying that it is not always possible to compute the expectation. Yet in this case (since you restrict to the identity) you just computed this expectation so why use the simplification? You should clearly explain why you discuss both approaches (i.e. add a short sentence such as \u201cSolving the non oracle case requires an approximation on the expectation \\mathbb{E} e^{\\theta \\Xi}. We first motivate such an approximation by showing that it can be related to the exact solution for the SNR, in the particular case of the identity\u201d).   From what I understand the conditions  inf_\\theta (1/\\theta) \\log\\left(\\mathbb{E} Z * \\mathbb{E}e^{-\\theta\\Xi}\\right) and inf_\\theta (1/\\theta) \\log(n \\mathbb{E} e^{-\\theta \\Xi}) are the same. what you do is, in the first approach, you compute the exact expectation, in the other, you use an approximation. But this is very poorly explained  Can you claim that your approximation is sound just because it works for the identity ?   page 7/8  The connection between the oracle and the non oracle case is not very clear. In particular, why can you apply the same machinery in the non oracle case as the one you use in the oracle case? I.e. from the paper it seems straightforward that the phase transition in the non oracle setting occurs when -inf_theta \\frac{1}{\\theta} \\log\\left[\\sum_i \\mathbb{E} e^{-\\theta\\Xi_i}\\right] or inf_\\theta \\log(\\mathbb{E}Z \\mathbb{E}e^{-\\theta\\Xi}). Yet this does not seem straightfoward to me. I.e. when you say \u201csecond, this estimator also exhibits a phase transition phenomenon, which behaves similarly to that in the oracle case\u201d, can you expand a little more ?  You should give a more detailed reference (indicate Algorithm 1?) to your 2020 paper (i.e. Zhang et Li) in which you introduce the estimator for Pi.  How do you go from your estimator (i.e the 2020 one to the matrix Xi? further explanation is needed here). If I understand well, because your estimator does not rely on the regression coefficients, you dcan apply the BP equations to this estimator to study the phase transition. if so you should add that somewhere.   In the statement of Theorem 2, you reference an equation (39) in appendix  I would remove the proof details for Theorem 2 as it is just the computation of an expectation (i.e. everything can be done by means of Isserlis' theorem) and your explanation of the three phases is unclear. I think no one will blame you for not providing the details of the computation of an expectation. But if what you provide is unclear, then a the reader will start to question the whole paper.  In the very last paragraph of page 8 you mention a gap between the theoretical value and the numerical value. What gap are you talking about? Where are the numerical experiments ? Are you talking about The result from table 3, then you should add a reference to that table.   page 9  At the beginning of page 9 (first paragraph), you again indicate that numerical experiments confirm your results while not adding any reference to those experiments. Are you talking about Figure 2/Table 3 because the captions of those do not seem to relate to what you say on page 9 I would consider removing section 6. The section should be expanded and is unclear in its current state. Why would you want to do partial recovery if you can apply the full recovery algorithm. You need to better motivate this (for example in terms of computation) or remove it. The discussion is not clear. From what I understand the SNR already represent the SNR at which the phase transition occurs. Then you indicate that there is a possible phase transition w.r.t tau ? what do you mean? there is a regime in which you can\u2019t predict anything ? i.e. no SNR will enable the recovery? This also appears surprising to me that this regime corresponds to a very narrow subset of tau values (as your plot in Figure 2 seems to indicate). How do you explain this?  As a side note, I find it surprising that the Hamming distance of the permutation matrix to the identity seem to matter for the phase transition but maybe this is a well known fact.  ================================================================ Typos/syntax errors  ================================================================ page 1  fisrt paragraph: \u201cwe have witnessed a revival of this problem due to its board spectrum\u201d \u2014> I guess you mean \u201cbroad\u201d   The result introduced in Zhang et Li 2020 is indeed very similar to the work of the paper page 2 When you say \u201cthe maximum allowed permuted rows\u201d, do you mean the \u201cmaximum allowed number of permutations\u201d? When you introduce your technical contributions, \u201cBy removing all function nodes except that corresponds to that index\u201d \u2014> \u201cexcept the one that corresponds\u201d Again, when you introduce your technical contributions : \u201can algorithm that converge in one step \u201c \u2014> \u201cthat convergeS\u201d \u201cits performance almost match\u201d \u2014> \u201cits performance almost matchES\u201d  page 4  Before Equation 5: \u201cwe can thus simply the MP update equation as\u201d \u2014> \u201cwe can thus simply write\u201d? Before Equation 6: \u201cwhere B^ sharp is given a prior\u201d \u2014> \u201cB^\\sharp is given as a prior\u201d ?  page 6  section 4.2. \u201cto obtain a closed-formula\u201d \u2014> \u201cA closed form expression\u201d? First paragraph of section 4.2. \u201cThe computation of phase transition threshold is by setting\u201d  \u2014> \u201cThe phase transition can be estimated from\u201c ? In the title of section 4.2. : \u201cAPPROXIMATED COMPUTATION \u201d \u2014> \u201cApproximate computation\u201d  page 7  Computation of the mean: \u201ceasily we can verify\u201d \u2014> \u201cwe can easily verify\u201d  page 8  Phase 1 of the computation of the variance : \u201cThis decomposition is in the same spirit of the leave-one-out technique\u201d \u2014> \u201cin the same spirit as \u2026\u201d Right before the subsection \u201cComputation of Var \\Xi\u201d: \u201cwhich consists the calculation of the following six terms\u201d \u2014> \u201cwhich consists OF the calculation of\u2026\u201d Phase 2 of the computation of the variance : \u201cwe can exploit the independence among rows\u201d \u2014> \u201camong the rows\u201d Phase 2 of the computation of the variance: \u201cwhich contains majority of terms\u201d \u2014> \u201cwhich contains THE majority of THE terms\u201d Same phase: \u201creduce the order of Gaussian random variables\u201d \u2014> \u201cof THE Gaussian variables\u201d Same phase: \u201cby separately taking expectation\u2019 \u2014> \u201cby taking THE expectation\u201d When you explain phase 1 for the computation of the variance, \u201cand leave the rest terms to matrix\u201d \u2014> \u201cthe rest OF the terms\u201d or even better \u201cthe remaining terms\u201d In the discussion paragraph at the end of the page: \u201cwe notice a increasing gap\u201d \u2014> \u201cAN increasing gap\u201c In the caption of table 3, \u201crate drops blow 0.05 \u201d \u2014> \u201cdrops below\u201d?  Page 9  At the beginning of the first paragraph you say \u201cNumerical experiments confirm our prediction \u201d. Which numerical experiments are you talking about ? In the conclusion: \u201cFor the oracle case where the signal B is given a prior \u201d \u2014> \u201cis given AS a prior\u201d 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 177,
        "instruction": "A Hierarchical Bayesian Approach to Federated Learning: We propose a novel hierarchical Bayesian approach to Federated learning (FL), where our models reasonably describe the generative process of clients' local data via hierarchical Bayesian modeling: constituting random variables of local models for clients that are governed by a higher-level global variate. Interestingly, the variational inference in our Bayesian model leads to an optimization problem whose block-coordinate descent  solution becomes a distributed algorithm that is separable over clients and allows them not to reveal their own private data at all, thus fully compatible with FL. We also highlight that our block-coordinate algorithm has particular forms that subsume the well-known FL algorithms including Fed-Avg and Fed-Prox as special cases. That is, we not only justify the previous Fed-Avg and Fed-Prox algorithms whose learning protocols look intuitive but theoretically less underpinned, but also generalise them even further via principled Bayesian approaches. Beyond introducing novel modeling and derivations, we also offer convergence analysis showing that our block-coordinate FL algorithm converges to an (local) optimum of the objective at the rate of $O(1/\\sqrt{t})$, the same rate as regular (centralised) SGD, as well as the generalisation error analysis where we prove that the test error of our model on unseen data is guaranteed to vanish as we increase the training data size, thus asymptotically optimal.",
        "reference": "This paper proposes a novel hierachical Bayesian approach to FL, with variational inference. The idea is to use two levels of random variables, the higher level \u03d5 which is shared among clients, and lower level \u03b8i's for each client i. This allows flexibility of conditional independence and personalization. There are two main methods: Normal-inverse-Wishart (NIW) and Mixture Model (Mix.). The methods are supported by convergence and generalization proofs, as well as experiments on two datasets.  --post rebuttal-- Thanks to the authors for the careful response to my questions. I have updated my score accordingly. After reading the response, I think the paper indeed improves, but there are still some weaknesses to address. It seems that on the theoretical side, the only main novelty is the generalization bound, as the authors admit. However, after checking the proof, I found this generalization bound shares a lot of similarities with Bai et al. 2020. Moreover, in the main paper, Theorem 4.2 needs a lot of discussions, such as the meaning of each term (e.g. rn, \u03f5n), the implication of this theorem, and comparison with existing results (e.g. Bai et al). On the algorithmic side, since hierarchical Bayesian is proposed before, what is the main novelty of this paper? Is it simply an application for personalized FL? Finally, on the experimental side, after adding more baselines, the improvement over baselines is not significant. For instance, for MNIST the improvement is less than 1%. Even for CIFAR-100, for \u03c4=10 the improvement is much less than \u03c4=1. This casts doubt on the practicability of the proposed algorithms. Strengths:  The Bayesian approach to FL is interesting and seems suitable for personalization. Using the hierachical approach and variational inference, the authors propose two different choices of hierachical models, that seem to be work well in practice. The proposed Bayesian FL algorithms are supported by convergence analysis and generalization error bound. The method is tested on CIFAR-100 and CIFAR-C-100 and compared with baseline algorithms, with noticeable improvement.  Weaknesses: Although this hierachical Bayesian approach seems interesting, and it is supported by some theoretical analysis as well as experimental comparison, I feel this paper is currently not ready for publication due to several reasons:  The motivation is not very clear and some claims are not well-supported. For example:   Sec 1, paragraph 1: \"FedAvg and FedProx are well-known to suffer convergence issues\". Any reference? What are the assumptions? Why would personalization resolve the convergence issues?  Sec 1, paragraph 2: why would Bayesian perspective be good for personalization? Is the current method complete or principled? Despite this being a bit subjective, I think at least the choices of NIW and Mix. are not principled and ad hoc. What is the meaning of the higher level random variable \u03d5? Is it like some prior or shared knowledge? More intuitive explanation is needed. Figure 1: what are yp and xp?  Sec 2.1: reference of ELBO and the original ref of block coordinate optimization are needed. Sec 2.2: the notation pp seems a bit weird as the two p's have different meanings. eq.10: what does the approximation sign mean? In which cases does the equality hold? Sec 3: how the two models are proposed does not seem straightforward. Why are NIW and Mix. good choices and how are they proposed?   Convergence analysis. In Appendix D the authors added 3 assumptions. In Assumption 1, the definitions of \"locally convex\" are not quite clear. Assumptions 2 and 3 have some overlap regarding the Lipschitzness and the radius of the domain needs to be justified.   The convergence of the proposed methods has O(1/T) convergence rate. This is at the same level as FedAvg (see e.g. [1]). Therefore the proposed method has no advantage over FedAvg which the authors claim to have some convergence issues.   The generalization error bound is not well explained. For example, what is \u03f5n? What does \u03bbi\u2217 mean? Why do you use Hellinger distance? How is the error bound compared to (Mohri et al 2019)? The empirical evalution can be improved. The dataset is mainly CIFAR-100 and its variant, and only 4 baselines are compared. FedAvg and FedProx are pretty similar and more Bayesian approaches should be compared, such as FedPA, FedBE, FedEM, and PredictiveBayes [2].  [1] Adaptive federated optimization. Reddi et al, ICLR 2021.  [2] Robust One Round Federated Learning with Predictive Space Bayesian Inference, Hasan et al, https://arxiv.org/pdf/2206.09526v1.pdf. At the current stage the paper is not clearly written and the quality cannot be easily judged. I believe there is some novelty in this work. About the reproducibility, the authors attached the code but it cannot be run. There is no package imported and the data collection process is not clear. Therefore, the reproducibility of this paper cannot be checked. In summary, this paper proposes hierachical Bayesian FL using variational inference which looks interesting. However, the paper largely suffers from presentation, and it is not ready for acceptance till it's more clearly presented. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 178,
        "instruction": "Neural Representations in Multi-Task Learning guided by Task-Dependent Contexts: The ability to switch between tasks effectively in response to external stimuli is a hallmark of cognitive control. Our brain is able to filter and to integrate external information to accomplish goal-directed behavior. Task switching occurs rapidly and efficiently, allowing us to perform multiple tasks with ease. In a similar way, deep learning models can be tailored to exhibit multi-task capabilities and achieve high performance across domains. Still, understanding how neural networks make predictions is crucial in many real-world applications.\n\nIn this study, we delve into neural representations learned by multi-tasking architectures. Concretely, we compare individual and parallel networks with task switching networks. Task-switching networks leverage task-dependent contexts to learn disentangled representations without hurting the overall task accuracy. We show that task-switching networks operate in an intermediate regime between individual and parallel. In addition, we show that shared representations are produced by the emergence neurons encoding multiple tasks. Furthermore, we study the role of contexts across network processing and show its role at aligning the task with the relevant features. Finally, we investigate how the magnitude of contexts affects the performance in task-switching networks.",
        "reference": "This paper studies the problem of neural representation learning in multi-task learning. The authors compared three types of model architectures: individual network, parallel network and task switching network, on MNIST dataset with a multitask learning setting. To be more specific, the authors conducted analyses on the representations learned from these model architectures. In the discussion, the authors pointed out that task-switching networks can encode congruency to resolve multi-tasking, and the context of tasks can improve multi-task learning differently when being used at different positions (layer depth). Strengths  This paper studies a very meaningful research problem of understanding the representation learned for multi-task models.  The authors utilized different statistical tools to conduct analyses on understanding the representation learned and importance of context for different multi-task learning model architectures.  Weaknesses  Presentation. The presentation and organization of this paper can be improved. I found it hard to connect different analyses done in section 3. It might be good to have an overview of different research hypotheses or questions being answered in each of the analyses.  Novelty. The technical novelty of this paper is relatively limited. The model architectures used in this paper to conduct analysis don\u2019t reflect recent advances in multi-task deep learning. Also the analyses being done here is similar to other existing work that studies the understanding of multi-task deep learning. I also want to see if the authors can propose some novel improved techniques based on their findings.   Related work. This paper doesn\u2019t include recent advances in multi-task deep learning, either in understanding MTL or MTL model architectures, which limits its contribution and significance. For example, [1] conduct analysis on multi-task deep learning for its generalization ability related to representation learning and model capacity. Also, besides the task switching network, Mixture-of-experts [2,3] based approaches, and attention modules [4] have been widely used for multi-task learning.   [1] https://arxiv.org/abs/2005.00944 [2] https://proceedings.neurips.cc/paper/2021/hash/f5ac21cd0ef1b88e9848571aeb53551a-Abstract.html [3] https://dl.acm.org/doi/pdf/10.1145/3219819.3220007 [4] https://arxiv.org/abs/1910.10683  Experiment dataset. The dataset used in conducting the experiment is quite small. Multi-task learning generalization benefits can be better evaluated in different scenarios, such as: (1) tasks have similar amounts of data, (2) some tasks have much fewer data (3) few-shot tasks. These scenarios cannot be fully captured by MNIST dataset. The clarity and organization of this paper needs to be improved. Quality and technical novelty of this paper is relatively limited. This paper conducted experiments and analyses on understanding neural representations in multi-task learning networks. There are some very interesting results and discussions. However, I think this paper can be further improved by including more recent advances in multi-task deep learning and discussion related to their findings. Meanwhile, the experiment section can be improved by including larger benchmark datasets. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 179,
        "instruction": "MCTransformer: Combining Transformers And Monte-Carlo Tree Search For Offline Reinforcement Learning: Recent studies explored the framing of reinforcement learning as a sequence modeling problem, and then using Transformers to generate effective solutions. In this study, we introduce MCTransformer, a framework that combines Monte-Carlo Tree Search (MCTS) with Transformers. Our approach uses an actor-critic setup, where the MCTS component is responsible for navigating previously-explored states, aided by input from the Transformer. The Transformer controls the exploration and evaluation of new states, enabling an effective and efficient evaluation of various strategies. In addition to the development of highly effective strategies, our setup enables the use of more efficient sampling compared to existing MCTS-based solutions. MCTransformer is therefore able to perform a small number of evaluations for each newly-explored node, and to do so without degrading its performance. Our evaluation, conducted on the challenging and well-known problem of SameGame, shows that our approach outperforms both Transformer-based and MCTS-based solutions.",
        "reference": "This work proposes to combine Decision/Trajectory Transformers (DTs) with MCTS, in order to improve the performance of DTs and also enable better exploration during online evaluation. The paper uses transformers as a prior policy for MCTS during the expansion phase, and also uses the same transformer to perform rollouts. The experiments are performed in the SameGame domain, which is a fully-observable Tetris like game with state-based inputs. The experiments show benefit over using using vanilla MCTS that learns only from monte-carlo returns. Strengths:  The paper tries to fix a key issue with Decision Transformers (DTs; transformers for sequence modelling of RL trajectories) like methods which is that they can't adapt easily at test time to explore more or improve their performance. The particular setup of combining DTs and MCTS is novel, and I could see follow-up working building further on it.  Weaknesses:  The writing and presentation is the paper is extremely unclear. The exposition of the method could be a lot better. See the below section for some suggestions.  The evaluation is extremely limited and non-standard, both in terms of the domains and algorithms. This makes is hard to assess if the original pitch of the paper empirically holds.  The experiments performed are on a single, non-standard domain of SameGame which is relatively toy-ish as well. I would have liked seeing comparisons on more well-benchmarked domains like MiniGo or Atari. Similarly, there were no comparisons to DecisionTransformers or Online DTs which the paper supposedly improves upon. I am also not sure what to make of the MCTS comparisons, since from my understanding the paper doesn't use a learned prior policy or value function in the MCTS experiments (similar to AG/AGZ/Muzero), which makes the comparisons a bit unfair IMO. Clarity:  The paper lacks clarity in exposition, and requires multiple passes to grok and connect ideas.  Figure 1 should be self-explanatory with an elaborate caption. Nits: The font-sizes in the figure are extremely small. Some things like the rollout component in the figure are left unexplained (what's rtg\u03c0(st)?) The MCTS baseline needs more explanation as well.  Consider using citep for inline citations so they land inside brackets.  The word transformers is used very liberally, but remember that transformers as just an architecture and can be used in RL in a number of ways. This paper uses transformers to do sequential modelling of trajectories, which should be explicit rather than implicit. Consider replacing transformers by decision transformers or trajectory transformers to avoid the ambiguity.  Quality:  The paper starts off with a great idea, but lacks in execution and clarity.  Originality:  The idea and method itself are pretty original to my knowledge. This paper proposes an elegant idea to combine MCTS with Trajectory Transformers to enable the latter to perform online improvement / exploration. While the idea itself is promising, the paper lacks in empirical execution and evidence (it terms of the domains and compared algorithms). The writing and exposition itself need some work. Owing to these factors, I don't think the paper is ready for acceptance at this time, but I do see a lot of legroom in the idea itself so this could be a great paper once the issues are fixed. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 180,
        "instruction": "One-Step Estimator for Permuted Sparse Recovery: This paper considers the unlabeled sparse recovery under multiple measurements, i.e., ${\\mathbf{Y}} = {\\mathbf{\\Pi}}^{\\natural}{\\mathbf{X}} {\\mathbf{B}}^{\\natural} + {\\mathbf{W}}$, where ${\\mathbf{Y}} \\in \\mathbb{R}^{n\\times m}, {\\mathbf{\\Pi}}^{\\natural}\\in \\mathbb{R}^{n\\times n}, {\\mathbf{X}} \\in \\mathbb{R}^{n\\times p}, {\\mathbf{B}}^{\\natural}\\in \\mathbb{R}^{p\\times m}, {\\mathbf{W}}\\in \\mathbb{R}^{n\\times m}$ represents the observations, missing (or incomplete) correspondence information, sensing matrix, sparse signals, and additive sensing noise, respectively. Different from the previous works on multiple measurements ($m > 1$) which all focus on the sufficient samples regime, namely, $n > p$, we consider a sparse matrix $\\mathbf{B}^{\\natural}$ and investigate the insufficient samples regime (i.e., $n \\ll p$) for the first time. To begin with, we establish the lower bound on the sample number and \\emph{signal-to-noise ratio} (${\\mathsf{SNR}}$) for the correct permutation recovery. Moreover, we present a simple yet effective estimator. Under mild conditions, we show that our estimator can restore the correct correspondence information with high probability.  Numerical experiments are presented to corroborate our theoretical claims.",
        "reference": "In this paper, the authors provide theoretical guarantees for unlabeled sparse recovery with multiple measurements. They first establish the information-theoretic lower bounds with respect to the number of samples and signal-to-noise ratio (SNR) for the reconstruction of both the permutation and signal matrices. Then, they propose a one-step approach for reconstructing the permutation matrix (and the signal matrix can be easily reconstructed by using the estimated permutation matrix and standard compressed sensing techniques). Some experiments on synthetic data are provided to support the theoretical results. Strength: The studied problem is at least of sufficient theoretical interest, and the technical results seem to be novel. Besides, the proposed algorithm is sufficiently simple. Weaknesses:  The theoretical results are not clearly interpreted. For example, the authors mentioned, \"For the sample number n, the lower bound requires n to be at least of order O(klog\u2061p); while Corollary 1 requires n to be k(log\u2061n)(log2\u2061mnp)\" on page 6. Where does the lower bound come from? I guess it is not derived from Theorem 2 in this paper but from the lower bounds established in standard compressed sensing literature. The authors mentioned on page 2 that the lower bound is established with respect to both the number of samples and SNR. But I cannot see it clearly from the statement of Theorem 2. In addition, it is fairer to compare the lower bound with the upper bound for the number of samples established in Theorem 3 (which has a quadratic dependence on k), rather than Corollary 1, which is based on a seemingly restrictive assumption.   It is counter-intuitive to see that no matter how large k is, the thresholding operator used in Eq. (2) that only keeps the element with the largest magnitude in each column works well. I hope that the authors can provide more intuitions/explanations for this, instead of only presenting Figure 1. The authors mentioned that \"In the supplementary material, we give a rigorous explanation of this phenomenon\" on page 5. But I cannot find where is a rigorous explanation. Please specify it.   The authors claimed in the conclusion that \"we considered the unlabeled sparse recovery with multiple measurements for the first time\". Although the studied problem is interesting in theory. From the current submission, I cannot see how practically meaningful it is. I hope that the authors can provide some numerical experiments on real data to back up the theory. This paper is generally well-written. But there is a lack of rigorous/intuitive interpretations of the main theoretical results.  The technical results seem to be novel.  Although this paper only involves synthetic experiments, it is better to upload the code for reproducibility. Some minor problems: (a) Page 2: provide a reference for the linear assignment problem (LAP). (b) Eq. (3): provide clear definition of ||B||1.   Does it mean the sum of entries of B (in magnitude), or the operator norm ||B||1\u21921? (c)  Page 4: the definition of supp(B) should be given. Overall, I think that although this submission is interesting in theory, it is weak in both interpreting the main theoretical results and presenting the practical motivation. Therefore, I am inclined to rejection. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 2: The contributions are only marginally significant or novel. NO.Details Of Ethics Concerns: NA. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 181,
        "instruction": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?: There have been a lot of interest in the scaling properties of Transformer models \\citep{kaplan2020scaling}. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour?  How does this influence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model architectures such as Transformers, Switch Transformers, Universal Transformers, Dynamic convolutions, Performers, and recently proposed MLP-Mixers. Via extensive experiments, we show that (1) architecture is an indeed an important consideration when performing scaling and (2) the best performing model can fluctuate at different scales. We believe that the findings outlined in this work has significant implications to how model architectures are currently evaluated in the community.",
        "reference": "This paper conducts a systematic study of scaling behavior of different model architectures. Strength: Extensive experiments have been done in this paper. Weaknesses: (1). There is a lack of innovation. All of the conclusions drawn from experiments are pretty under expectation and there are no surprises. Majority of findings have been seen before. (2). This paper mainly compares a series of models published by google such as Evolved Transformers, Universal Transformers, Switch Transformers, Performer, Funnel Transformer, Albert, MLP-mixer, and ignores some other state-of-the-art models. For example, although the Switch Transformer was published first, the subsequent models such as BASE layers [1], Hash layers [2] are much better than the Switch Transformer.  As a systematic study paper, the author should choose SOTA models for comparison.  (3). One of the conclusions from this paper is \u201cthe MLP-Mixer is hard to scale\u201d.  I doubt whether this can explain the structure of MLP hard to scale, or because the author only uses mlp-mixer at input encoder part. Actually, MLP structure can also be applied to decoder-only models such as [3].  (4). I wonder why this paper does not compare encoder-decoder model architecture v.s. decoder-only model. The encoder-decoder model seems to be rarely used for large language models. The largest T5 is only 11B, however, decoder only models can easily pass 100B parameters. In addition, I wonder how to scale up encoder-decoder models (increase depth or width? How to balance the encoder part and decoder part). This article systematically compares model structures, but ignores this one. References: [1]. Lewis, Mike, et al. \"Base layers: Simplifying training of large, sparse models.\" International Conference on Machine Learning. PMLR, 2021. [2]. Roller, Stephen, Sainbayar Sukhbaatar, and Jason Weston. \"Hash layers for large sparse models.\" Advances in Neural Information Processing Systems 34 (2021): 17555-17566. [3]. Yu, Ping, et al. \"Efficient Language Modeling with Sparse all-MLP.\" arXiv preprint arXiv:2203.06850 (2022). Clarity: This paper is well written and easy to understand. Novelty: As stated in weaknesses, this paper lacks novelty. In addition, instead of trying to compare SOTA model architectures, most models in this paper come from google. I understand that this will greatly reduce the workload, but as an article of a systematic study, more SOTA model structures should be selected. This article systematically studies model architecture\u2019s influence on scaling law. Extensive experiments are carried out. However, All of the conclusions drawn from experiments are pretty under expectation and there are no surprises. Majority of findings have been seen before. In addition, some SOTA model architectures are not included in this paper. 4: All of the claims and statements are well-supported and correct. 1: The contributions are neither significant nor novel. Not applicable NO. 5: marginally below the acceptance threshold 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 182,
        "instruction": "Multi Task Learning of Different Class Label Representations for Stronger Models: We find that the way in which class labels are represented can have a powerful effect on how well models trained on them learn. In classification, the standard way of representing class labels is as one-hot vectors. We present a new way of representing class labels called Binary Labels, where each class label is a large binary vector. We further introduce a new paradigm, multi task learning on different label representations. We train a network on two tasks. The main task is to classify images based on their one-hot label, and the auxiliary task is to classify images based on their Binary Label. We show that networks trained on both tasks have many advantages, including higher accuracy across a wide variety of datasets and architectures, both when trained from scratch and when using transfer learning. Networks trained on both tasks are also much more effective when training data is limited, and seem to do especially well on more challenging problems. ",
        "reference": "The paper proposes \u2018binary labels\u2019, where each dimension of the embedding space is assigned a 0 or 1 for each class, which is decided randomly at label construction time, before the start of training). Intuitively, at each embedding dimension, this leads to a grouping of classes (those that have a 0 in that dimension, and those that have a 1 there). So training to predict these labels can be thought of incentivizing the model to learn commonalities between different groups of classes. They propose to predict these binary labels as an auxiliary task, jointly with the main task of predicting the usual one-hot labels. They show that in some cases, both for in-distribution generalization and transfer learning, using this auxiliary task improves performance compared to the baseline of using only the main task. They also show that it can improve data efficiency (achieving higher accuracy with fewer updates) compared to that same baseline. Strengths [+] The paper proses an interesting idea and I agree that label representations are an understudied aspect which is orthogonal to where large gains usually come from (improving models and data) [+] In some cases, the proposed method improves results significantly over vanilla training. Weaknesses [-] In some cases, the performance improvement is very small compared to only training with softmax labels, and no intuition or analysis is provided to shed some light on when we expect this approach to help. For instance, is it for datasets with more finegrained labels? What are other factors that affect this? [-] The experimental section is weak. It would have been useful to compare against additional baselines, perform more analyses, explore different datasets and scenarios (concrete details in the below section) [-] The paper has some clarity issues. Clarity The paper discusses \u2018dense labels\u2019 in the introduction without explaining what they are, which makes it hard to follow that discussion. The authors say they use Metabalance for multi-task learning but don\u2019t explain what this means and don\u2019t discuss it in the methods section. The authors say \u201cIn this case, the two tasks are not just related, they are identical \u2026\u201d. This sounds odd since, in principle, it\u2019s not clear why using the same task will be useful. Intuitively, if the tasks used for multi-task learning are redundant, training on them simultaneously won\u2019t give more information than training on a single one.  The authors state that they randomly generate the binary labels for each class. Is this generation happening just once? I imagined that it would happen several times, each time resulting in a different set of classes being grouped together (in each embedding dimension), and I imagined that a different output head would be used for each. But in the experiments, it seems that there are a total of 2 output heads: one for the main task (softmax labels) and one for the (single) auxiliary task. It would be very useful to clarify the exact procedure that the authors propose. Perhaps including an Algorithm box too would be useful.  Novelty The particular auxiliary training task proposed is novel, to the best of my knowledge. However, the authors should be more thorough in discussing related work (see comment below). Reproducibility I don\u2019t think I\u2019d be able to reproduce the results based on reading the paper. I\u2019m not sure what Metabalance is and what is the modification that the authors propose to it. Some other details also weren\u2019t clear to me (see Clarity section). I encourage the authors to make code available, and also to use an Appendix where all implementation details are described. Quality In Section 3, all loss equations sum over the training examples, whereas usually this is an average over the examples (ie. the term dividing by the number of examples is missing) Figure 1 is low resolution, looks like it\u2019s a screenshot. Please update to higher res. The empirical evaluation section is weak. It would be useful to explore few-shot learning tasks at test time, robustness to distribution shifts, etc, as additional experiments. It would also be useful to compare against baselines, different variants of the proposed model (different ways of doing multi-task learning, etc) and compare against dense labels, as well as multi-task variants that include dense labels as an additional auxiliary task. Since the task weighting seems to be important, it would be nice to do a study that shows how different values affect the results. Also, how was this hyperparameter determined for the main results in the paper? The authors allude to results on using binary labels as the only or main task, but these results aren\u2019t shown. This is puzzling as it seems there is plenty of space left in the paper. Is the auxiliary task of binary labels complementary to that of dense labels? Can the two be combined? If the random binary label generation happens exactly once (see comment in Clarity section), have the authors investigated the effect of that on performance? Maybe some random labels are better or worse than others. Related work The authors mention this idea is similar to label smoothing, but they don\u2019t discuss the differences aside from saying it is \u201cquite different\u201d. It would be useful to expand on that.  The proposed approach is closely related to unsupervised meta-learning, where tasks for meta-learning are created without using labels. One way of doing this is clustering, where hopefully different clusters correspond to different semantic classes, which can be used to form tasks (this clustering can be interpreted as defining a different labeling function) [1]. Another idea is randomly grouping examples into \u2018classes\u2019 so that different groups of examples get assigned the same label in each task [2]. This incentivizes the model to discover relationships between different subsets of the data in each task, which has a similar motivation to the binary labels introduced here. Finally, a related idea is provided in [3] where meta-learning happens on tasks produced by different labeling functions. To create each task, a conjunction of attributes (like \u2018smiling and wearing glasses\u2019) is considered as a new label. This can also be thought of as a label transformation to create several tasks for multi-task learning. References [1] Unsupervised Learning via Meta-learning. Hsu et al. ICLR 2019. [2] Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via Random Labels and Data Augmentation. Antoniou et al. 2020. [3] Probing Few-Shot Generalization with Attributes. Ren et al. 2021. Minor comments In section 2.1, \u201ccloses\u201d \u2192 \u201cclosest\u201d Towards the bottom of page 8, \u201ctaks\u201d \u2192 \u201ctask\u201d Overall, the authors propose a really interesting auxiliary task to co-train with the main task of predicting one-hot labels. The results are encouraging in some of the scenarios explored. However, I felt that this exploration is still too premature to meet the bar for publication. Additional baselines, analyses and experimental scenarios should be considered to gain more insights into the performance of the proposed approach. Clarity issues should also be addressed, and related work should be discussed more thoroughly. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 183,
        "instruction": "On the Existence of a Trojaned Twin Model: We study the Trojan Attack problem, where malicious attackers sabotage deep\nneural network models with poisoned training data. In most existing works, the\neffectiveness of the attack is largely overlooked; many attacks can be ineffective\nor inefficient for certain training schemes, e.g., adversarial training. In this paper,\nwe adopt a novel perspective and look into the quantitative relationship between a\nclean model and its Trojaned counterpart. We formulate a successful attack using\nclassic machine learning language. Under mild assumptions, we show theoretically\nthat there exists a Trojaned model, named Trojaned Twin, that is very close to the\nclean model. This attack can be achieved by simply using a universal Trojan trigger\nintrinsic to the data distribution. This has powerful implications in practice; the\nTrojaned twin model has enhanced attack efficacy and strong resiliency against\ndetection. Empirically, we show that our method achieves consistent attack efficacy\nacross different training schemes, including the challenging adversarial training\nscheme. Furthermore, this Trojaned twin model is robust against SoTA\ndetection methods",
        "reference": "This paper studies the backdoor adversarial attack. It theoretically discuss the existence of an attacking counterpart model and corresponding design an attacking algorithm. The proposed method is evaluated on benchmark datasets. Strength  Investigating backdoor attacks under adversarial training is relatively new. The theoretical results seems interesting.  Weakness  The claim that \"existing backdoor attacks cannot resolve adversarial training\" is questionable. The empirical evidences provided by the author do not fully support this claim. The improvement of the proposed method is relatively marginal. This paper is a little bit hard to follow considering the usage of many informal expressions. The quality and novelty are relatively good. I did not check the reproducibility. As in Strength vs Weakness 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 184,
        "instruction": "On Information Maximisation in Multi-View Self-Supervised Learning: The strong performance of multi-view self-supervised learning (SSL) prompted the development of many different approaches (e.g. SimCLR, BYOL, and DINO). A unified understanding of how each of these methods achieves its performance has been limited by apparent differences across objectives and algorithmic details. Through the lens of information theory, we show that many of these approaches are maximising an approximate lower bound on the mutual information between the representations of multiple views of the same datum. Further, we show that this bound decomposes into a ``reconstruction\" term, treated identically by all SSL methods, and an ``entropy\" term, where existing SSL methods differ in their treatment. We prove that an exact optimisation of both terms of this lower bound encompasses and unifies current theoretical properties such as recovering the true latent variables of the underlying generative process (Zimmermann et al., 2021) or or isolating content from style in such true latent variables (Von K\u00fcgelgen et al., 2021). This theoretical analysis motivates a naive but principled objective (EntRec), that exactly optimises both the reconstruction and entropy terms, thus benefiting from said theoretical properties unlike other SSL frameworks. Finally, we show EntRec achieves a downstream performance on-par with existing SSL methods on ImageNet (69.7% after 400 epochs) and on an array of transfer tasks when pre-trained on ImageNet. Furthermore, EntRec is more robust to modifying the batch size, a sensitive hyperparameter in other SSL methods.",
        "reference": "This manuscript provides an information theoretical perspective for multiview self-supervised learning (SSL). In specific, the authors show that a lower bound of mutual information is a useful objective function to learn informative representations. Additionally, optimizing such an objective function could be regarded as a generalization of both learning true latent variables (as in Zimmermann et al. (2021)) and separating content information from the irrelevant (as in Von Kugelgen et al. (2021)), from a generative model perspective. Besides, the work also analyzes the loss functions of different SSL methods, e.g., contrastive learning based and projection reconstruction based ones, and demonstrates that these approaches do not maximize the mutual information lower bound exactly. Strength:  It is an interesting idea to generalize previous model identification results of different generative models by using the mutual information lower bound. The authors also provide detailed analysis showing that many existing multiview SSL algorithms approximately maximize such a lower bound, which could help deepen the understanding of different SSL paradigms. However, the theoretical aspects as well as the empirical ones are not sufficiently clear in its current version. Weaknesses:  The novelty seems not significant since a similar loss has been proposed and analyzed in Wang and Isola (2020) as uniformity and alignment terms. The uniformity is the entropy term here while the alignment corresponds to the reconstruction term (or cross entropy term). Could the authors spell out the differences and the advantages? It is unclear that how Eq. (1) unifies both model identification theorems of Zimmermann et al. (2021) and Von Kugelgen et al. (2021). Specifically, contrastive loss (i.e., InfoNCE loss) is analyzed in Zimmermann\u2019s work while l2 matching loss is used in Von Kugelgen\u2019s work. How is the equivalence between such a loss and (1) established? It is hard to tell how the proof techniques here differ from those of previous works. The convergence rate results of the proposed methods are interesting but not very clear. Could the authors give more details on how the results are derived and perhaps more explanation? It would help readers better understand if the detailed algorithm is listed. It unclear how useful the proposed method is in practice since kernel density estimation is required for each iteration. Is there a wall time or complexity comparison? In terms of experiment results, it might be more straightforward using synthetic experiments to showcase the desired theoretical properties. For example, how well the mutual information is maximized? How does that help identify the true latent variables and/or separate the content from the style, compared to the baselines? Is it necessary to learn good representations only if a tight bound is maximized?  Some minor comments and questions:  Both Zimmermann et al. (2021) and Von Kugelgen et al. (2021) assume that the generative function is the same for each view. As a comparison, in [1], it is shown that the generative function g(\u22c5)'s do not need to be identical (in a more natural multivew setting) for each view in order to recover the true latent variables. Does the proposed criterion lead to similar model identification results under such a setting?  The interpretation in the first paragraph on page 3 is not very clear to the reviewer. Could the authors be more specific about what information can be and is learned is? It seems the entropy term is simply a constrain (could be other alternatives) to avoid degenerated solutions. Why is the proposed method more robust to changes of hyper-parameters, compared to other baselines? How is the parameter of kernel band width selected? Or how the density is estimated? In the Conclusion section, typo \u2018or or isolating\u2019.  [1] Qi Lyu, Xiao Fu, Weiran Wang, and Songtao Lu. \"Understanding latent correlation-based multiview learning and self-supervision: An identifiability perspective.\" In International Conference on Learning Representations. 2021. Overall, the manuscript is well-organized, but some parts can be made clearer (see weakness). The contributions are not significant since similar loss functions have been studied and the main theoretical results are not new. In terms of reproducibility, the detailed algorithm could be given for better clarity. This paper gives an interesting point of view for analyzing SSL using mutual information lower bound and proposes a method based on the criterion. However, both the theoretical and empirical results are not sufficiently clear. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 185,
        "instruction": "Counterfactual Generation Under Confounding: A machine learning model, under the influence of observed or unobserved confounders in the training data, can learn spurious correlations and fail to generalize when deployed. For image classifiers, augmenting a training dataset using counterfactual examples has been empirically shown to break spurious correlations.  However, the counterfactual generation task itself becomes more difficult as the level of confounding increases. Existing methods for counterfactual generation under confounding consider a fixed set of interventions (e.g., texture, rotation) and are not flexible enough to capture diverse data-generating processes. Given a causal generative process, we formally characterize the adverse effects of confounding on any downstream tasks and show that the correlation between generative factors (attributes) can be used to quantitatively measure confounding between generative factors. To minimize such correlation, we propose a counterfactual generation method that learns to modify the value of any attribute in an image and generate new images given a set of observed attributes, even when the dataset is highly confounded. These counterfactual images are then used to regularize the downstream classifier such that the learned representations are the same across various generative factors conditioned on the class label. Our method is computationally efficient, simple to implement, and works well for any number of generative factors and confounding variables. Our experimental results on both synthetic (MNIST variants) and real-world (CelebA) datasets show the usefulness of our approach.",
        "reference": "This paper considers the important confounding issue in image classification problems from the observational data. The authors built the connection between the confounding and the spurious correlation, by assuming that the generating factors are causally independent of each other. This result allows them to add counterfactual data augmentation in CycleGAN to remove the correlation between the target variable and generative factors. Numerical studies show their promising prospect over existing methods. Strength  This paper considers an important confounding issue in image classification from observed data.  The authors provide sound theoretical results to study the relationship between correlation and confounding.  Numerical studies show their promising prospect on well-known benchmarks.  The paper is well-written in general and easy to follow.   Weaknesses  My main concern is that this paper primarily assumed that Zi are causally independent of each other, i.e., there is no causal link among different Zs, in the DAG illustrated in Figure 1a. This means the only correlation among Zs is spurious owing to their common confounders C. In this way, they are able to utilize their proposed CONIC to remove the Effect of confounding edge. Yet, the causal independence among Zs is a quite strong assumption and needs further justification. The authors may also comment on the worse case when this assumption is violated.  The implementation details are not clear. For example, I wonder how the authors weighted the five different loss functions in Equation 4. Or are they equally weighted or using the same strategy in Equation 8? More specifications are required.  The main methodology is built upon CycleGAN. The authors may benefit from decoupling their counterfactual data augmentation part from CycleGAN and show their method is generally applicable to different image classification methods. A very natural question is: how much has the proposed method improved CycleGAN? Why not include CycleGAN for comparison in the experiments of benchmark? The theoretical originality is interesting but with a very strong assumption, and the algorithm is built upon an existing approach.  The authors may consider some proofreading.   Please add punctuation after all the mathematical equations.  Please consider improving the readability of Algorithm 1. Fix typos, e.g., 'of the from' on the top of page 5. The notation I(Zi;Zj) is used without formal definition on page 5.  Add space before ',CutMix (Yun et al., 2019)'. Overall, this paper considers an interesting problem of confounding issues in image classification and is easy to follow. The theoretical results are sound but with a very strong assumption. The algorithm is built upon an existing approach (CycleGAN) while necessary comparison to show the uplifts is missing. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 186,
        "instruction": "SRBGCN: Tangent space-Free Lorentz Transformations for Graph Feature Learning: Hyperbolic graph convolutional networks have been successfully applied to represent complex graph data structures. However, optimization on Riemannian manifolds is nontrivial thus most of the existing hyperbolic networks build the network operations on the tangent space of the manifold, which is a Euclidean local approximation. This distorts the learnt features, limits the representation capacity of the network and makes it hard to optimize the network. In this work, we introduce a fully hyperbolic graph convolutional network (GCN), referred to as SRBGCN, which performs neural computations such as feature transformation and aggregation directly on the manifold, using manifold-preserving Lorentz transformations that include spatial rotation (SR) and boost (B) operations. Experiments conducted on static graph datasets for node classification and link prediction tasks validate the performance of the proposed method.",
        "reference": "This paper proposes a version of hyperbolic GCN based Lorentz model without resorting to the tangent space. Some of the layer operations exist in prior work. Experiments are conducted on standard small-scale citation networks and tree datasets. There are recently many efforts on removing the tangent space of hyperbolic neural networks. This paper used the Lorentz boost and rotation to operate directly on the Lorentz manifold.  Strength. The presentation and logic of the paper are easy to follow.  The ablation study is interesting and shows the learnt structure information from the model.  Weakness.  Lack of novelty and contribution. The Lorentz boost and rotation is already well-characterized and studied in [1], particularly the proposed fully hyperbolic linear layer in [1] contains all Lorentz rotation and boost matrices. In fact, the graph convolution operation is essentially a hyperbolic linear layer. One can just take the linear layer from [1] and make a similar tangent space free Lorentz based hyperbolic GCN, which is pretty much done in the paper.   Limited experiments and lack of reproducibility. Experiments are performed on 4 standard small scale datasets, which are 4 highly overfitted datasets. For example, the disease and airport, they are so small datasets. From my experience, I can always get a very high accuracy, but with a large variance during multiple runs. No code or supplementary is provided for the model. Plus I can get higher accuracies with some baselines, for example, using HGCN on pubmed node classification, 80.2\u00b10.3 with 10 runs. It's really hard to derive meaningful conclusion from the experiments.   ------ 2.1 where is the training hyper-parameter? how many layers of the model, any regularization, hidden dimension? How many epochs? I am aware that many hyperbolic NNs report results of running thousand epochs with early stopping, while standard Euclidean GCNs, SGC report accuracy of 100 or 200 epochs. Is the comparison fair?   Some claims are vague or inaccurate. For example,  \"most of the existing hyperbolic networks build the network operations on the tangent space of the manifold, which is a Euclidean local approximation. This distorts the learnt features, limits the representation capacity of the network\" I don't think this holds true, generally the exp/log map is local approximation of the manifold. However, in the hyperbolic space, they are bijection between the tangent space and the hyperbolic space. Usage of exp map at origin is not a problem with respect to representation capacity, as one can use parallel transport to move it along the manifold, which is already introduced and used in many prior works.  ------ 3.1 It claimed that \"the full Lorentz transformation ... increase the model expressiveness ... with deeper networks.\" I failed to find evidence in the paper supporting this claim, in particular, personally I haven't seen any successful deep hyperbolic GCN style structure to work well even in existing/prior work.   The word and text needs more polish. For example,  \"This makes it hard to optimize deeper networks because of these back and forth mappings between the manifold and the tangent space, and limits the representation capabilities of the hyperbolic networks caused by distortion specially most of these works used the tangent space at the origin. \" [1] Chen et al. 2022, Fully hyperbolic neural networks. The paper/methodology is generally easy to follow and understand. Lack of originality, reproducibility not clear. It's a straightforward usage of the fully hyperbolic NNs paper in GCN applications. Many pieces in the paper are missing and lack of enough novelty, see above. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 1: The contributions are neither significant nor novel. 1: The contributions are neither significant nor novel. NO. 3: reject, not good enough 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
    },
    {
        "id": 187,
        "instruction": "Mitigating the Limitations of Multimodal VAEs with Coordination-Based Approach: One of the key challenges in multimodal variational autoencoders (VAEs) is inferring a joint representation from arbitrary subsets of modalities.  The state-of-the-art approach to achieving this is to sub-sample the modality subsets and learn to generate all modalities from them. However, this sub-sampling in the mixture-based approach has been shown to degrade other important features of multimodal VAEs, such as quality of generation, and furthermore, this degradation is theoretically unavoidable. In this study, we focus on another approach to learning the joint representation by bringing unimodal inferences closer to joint inference from all modalities, which does not have the above limitation. Although there have been models that can be categorized under this approach, they were derived from different backgrounds; therefore, the relation and superiority between them were not clear. To take a unified view, we first categorize them as coordination-based multimodal VAEs and show that these can be derived from the same multimodal evidence lower bound (ELBO) and that the difference in their performance is related to whether they are more tightly lower bounded. Next, we point out that these existing coordination-based models perform poorly on cross-modal generation (or cross-coherence) because they do not learn to reconstruct modalities from unimodal inferences. Therefore, we propose a novel coordination-based model that incorporates these unimodal reconstructions, which avoids the limitations of both mixture and coordination-based models. Experiments with diverse and challenging datasets show that the proposed model mitigates the limitations in multimodal VAEs and performs well in both cross-coherence and generation quality.",
        "reference": "It is challenging to infer a joint representation from arbitrary subsets of multimodalities, and the state-of-the-art approaches (mixture-based multimodal VAEs) attempt to accomplish this by training to generate all modalities from a joint representation inferred from missing modalities, but the quality of modality generation is lower than that of unimodal VAEs, and this limitation is theoretically unavoidable.  Therefore, the authors propose a coordination-based model that brings the representation inferred from each modality closer to that inferred from all modalities. Experiments with diverse and challenging datasets show the advances of the proposed method. Strength:  The authors point out why these existing coordinate-based models perform poorly on cross-model generation, and propose a novel model to fix the issue. The proposed model can mitigate the limitations in multimodal VAEs and performs well in both cross-coherence and generation quality. The paper is well written, and the proposed model is novel. Unfortunately, this paper lies outside of my field of expertise, therefore, my comment may be biased. Area chairs are suggested to seek opinions from other reviewers. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
    },
    {
        "id": 188,
        "instruction": "Incompatibility between Deterministic Policy and Generative Adversarial Imitation Learning: Deterministic policies are widely applied in generative adversarial imitation learning (GAIL). When adopting these policies, some GAIL variants modify the reward function to avoid training instability. However, the mechanism behind this instability is still largely unknown. In this paper, we capture the instability through the underlying exploding gradients theoretically in the updating process. Our novelties lie in the following aspects: 1) By employing multivariate Gaussian policy with small covariance to approximate deterministic policy, we establish and prove the probabilistic lower bound for the exploding gradients, which can describe the degree of instability universally, while the stochastic policy will never suffer from such pathology subsequently. 2) We also prove that the modified reward function of adversarial inverse reinforcement learning (AIRL) can relieve exploding gradients, but at the expense of ``non-confidence''. Experiments and a toy demo support our analysis.",
        "reference": "The paper is about the instability in training when a deterministic learned policy is used in generative adversarial imitation learning (DE-GAIL) and how to alleviate the problem. The authors first show that the training of DE-GAIL is empirically unstable in Mujoco environments that have continuous action and state spaces. Then, they theoretically show that the reward of DE-GAIL becomes zero when the deterministic policy is used in finite MDP and when the cardinality of the state space goes to infinity in Proposition 1. Also, they argue that the instability is mainly due to the reward becoming zero. For MDPs with continuous action spaces, they theoretically show that DE-GAIL suffers the problem of exploding gradients due to a mismatch between actions from the expert and the learned policy. Furthermore, they use Lemma 1 to show that having a mismatch between the actions from the expert and the learned policy is equivalent to having an optimal discriminator function value of one which is also equivalent to having an infinite reward. Lastly, in Proposition 2, they prove that using CR instead of PLR can alleviate the problem. This is done by showing that PLR has a value larger than or equal to that of CR when their discriminator values are the same. Strength  The authors theoretically show that the deterministic learned policy brings instability in training.  The authors theoretically show that the problem can be alleviated by using the modified reward function of AIRL.   Weaknesses and Questions  There are no experiments that validate or support Proposition 1. The experimental results in Fig 1~3 are made from environments with continuous state and action spaces while Proposition 1 is about environments with finite action space. If the authors could empirically validate Proposition 1, that would strengthen the paper.  There are no experiments that validate Proposition 2. The empirical results shown in the paper are only about PLR-DE-GAIL. The results of CR-DE-GAIL should be compared with that of PLR-DE-GAIL.  Need more strong empirical validation for Theorem 1. Fig 2 and 3 may show instability in training DE-GAIL, but they don\u2019t empirically show that the exploding gradients problem actually happened and that the problem is caused by the instability. If the authors could show that the low-performing seeds in Figure 3 indeed suffered from the exploding gradients while the other seeds did not, that would validate Theorem 1 strongly.  The statement In pg 6: \u201cFor any state st, the action at\u2192t+1 occurs 3 times while others occur at most twice in the expert demonstration.\u201d seems to be wrong. If you follow the pseudo-code in Figure 4, when the agent is in s3g4+1,s3g4+2,\u22ef,sg\u22121 the agent never executes the action at\u2192t+1,\u2200t\u22083g4+1,3g4+2,\u22ef,g\u22121.  On pg 7, authors mentioned that \u201cThe learned policy is exposure to be transferred to the mismatching case due to the limited area in the sub-optimal matching compared with the optimal; see seed 1 in Fig. 3(a) for instance\u201d. But I think you cannot guarantee that the result from seed 1 in Fig.3 (a) is such a case. Could you show that the seed 1 experiment shows results similar to Fig 6?  Proposition 1: The derivation of the \u201clength of the expert trajectory\u201d seems to be wrong. In Appendix A.1 in the proof of Proposition 1, the authors derived this equation :  N=2(g4+(g4+1)+\u22ef+(g4+g4\u22121))+2(g2+1)g2\u22121+3g4=11g2+24g\u22121616. I may be wrong, but the derivation is different from mine. For the first for loop in the pseudo-code of Figure 4, the sub-trajectory length is 2(g4+(g4+1)+\u22ef+(g4+g4))+3g4 where the first term comes from executing actions ai\u2192j,aj\u2192i, and the last term comes from executing the action ai\u2192i+1. For the second for loop, since the agent only traverses between s3g4 and sg2,sg2+1,\u22ef,sg\u22121 except s3g/4, the length of sub-trajectory due to the second for loop should be 2((g\u22121)\u2212g/2). For the last line, the sub-trajectory of length 1 comes from executing the action  a3g/4\u2192g. By summing them up, I got (g2+36g)/16.  According to \u201cDefinition 1\u201d on pg 7,   state-action pairs are in a mismatch when ||\u03a3||2\u21920 and at\u2260h(st). And the pairs match when at=h(st). But Fig 6 draws actions in the neighborhood of a_t1 to show the matched case. And the paragraph under Fig 6 mentions that the matched case is made by having actions in the neighborhood of a_t1.  In Appendix A.2 (proof of Theorem 1), the authors derived \u2207h\u03c0h(a\u2223s)=\u03c0h(a\u2223s)\u03ba(s,\u22c5)\u03a3\u22121(a\u2212h(s)). Why is there \u03ba when you define \u03c0h as in the equation at the bottom of pg 6?  In Remark 2, since ||\u03a3||2\u22121||at\u2212h(st)||2\u2265||\u03a3\u22121(at\u2212h(st))||2, shouldn't be the inequality between probabilities (probability mentioned in Theorem 1 and Pr(\u039e)) the other way around? Clarity: poor Below are the questions related to the clarity of the paper:  In Figure 6, adding the legend for the red line and also adding x-axis and y-axis labels will make the clarity of the paper higher. And rather than writing at1 and at2 in the figure, putting markers and adding legends showing that they are optimal and suboptimal actions would be more informative to the readers.  On pg 7, I found this sentence hard to understand: \u201cThe learned policy is exposure to be transferred to the mismatching case due to the limited area in the sub-optimal matching compared with the optimal\u201d. I\u2019d appreciate elaboration on the sentence.  I was unable to find information on how Fig 6 was drawn. Was it drawn as an illustrative example? or, is it an empirical result?  On pg 7, the authors mentioned that \u201cThe threshold of matching is set as 0.035.\u201d. Could you elaborate on how 0.035 was computed? Also, the \u201cthreshold\u201d was never mentioned before. What is the \u201cthreshold\u201d and where is it applied?  In Appendix A.2 (proof of Theorem 1), the authors cited (Guan et al., 2021a) for the gradient of the JS divergence with respect to h, but I was not able to find the contents related to the gradients of JS divergence in the paper. Is the paper cited for policy gradient? If so, elaboration on the derivation of \u2207^hDJ(\u03c1\u03c0h,\u03c1\u03c0E) (DJ: JS divergence) would help the readers to understand.  Is the probabilistic lower bound related to the lower bound in Remark 2? If not, where is it mentioned in the paper?   Reproducibility : good The details of the experiments for Fig 2 and 3 are mentioned in the paper Quality : poor Due to the lack of details in Fig 6 and hard-to-understand sentences. Novelty : good The paper presents a novel theoretical analysis of the instability in the training of DE-GAIL and also theoretically shows how the modified reward function of AIRL can alleviate the problem. However, there seem to be some cases where statements or derivations are wrong, Furthermore, the paper lacks experiments that support propositions and theorems. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 4: The contributions are significant, and do not exist in prior works. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 189,
        "instruction": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation: Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining reasonable efficiency.",
        "reference": "The paper introduces FiD-light, a more efficient variant of the fusion-in-decoder model that maintains/outperforms state-of-the-art performances on the KILT dataset, while drastically increasing the model's efficiency. To achieve this, FiD light compresses the length of input vectors and uses re-ranking to improve the top-ranked provenance precision. Strength  simple and effective solution to improve efficiency clear improvements across datasets and tasks  Weaknesses  some parts of the paper should be better clarified (see my comments below) The paper is clear and well-written. The appendix contains information for reproducibility. Overall I found the paper clear. I do have some concerns regarding some choices made regarding the architecture and a couple of suggestions/questions.  Intro RQ2 - what's unexpected in distribution learned by FiD-light? Sec 3 Decoder efficiency - what's the fk function used to compress vectors? In some sections of the paper (e.g., sec 4.1) you are referring to \"common practices\" without citing papers that follow such approaches. For example, Sec 4.1 mentions that the community compares results to your second oracle scenario. I suggest adding citations to relevant works that do this.  Other works (e.g., KG-FiD) applies re-ranking between the encoder and the decoder of the T5 model. In this work, instead, you focus on the source pointing. I might have missed this part, but it is unclear why you are taking that direction. What's the reason for including source pointing? Sec 4.3 mentions that the model lower the latency by 2x. It would be good to specify with respect to which of the models in table3 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 190,
        "instruction": "Contrastive Learning of Molecular Representation with Fragmented Views: Molecular representation learning is a fundamental task for AI-based drug design and discovery. Contrastive learning is an attractive framework for this task, as also evidenced in various domains of representation learning, e.g., image, language, and speech. However, molecule-specific ways of constructing good positive or negative views in contrastive training under consideration of their chemical semantics have been relatively under-explored. In this paper, we consider a molecule as a bag of meaningful fragments, e.g., functional groups, by disconnecting a non-ring single bond as the semantic-preserving transformation. Then, we suggest to construct a complete (or incomplete) bag of fragments as the positive (or negative) views of a molecule: each fragment loses chemical substructures from the original molecule, while the union of the fragments does not. Namely, this provides easy positive and hard negative views simultaneously for contrastive representation learning so that it can selectively learn useful features and ignore nuisance features. Furthermore, we additionally suggest to optimize the torsional angle reconstruction loss around the fragmented bond to incorporate with 3D geometric structure in the pre-training dataset. Our experiments demonstrate that our scheme outperforms prior state-of-the-art molecular representation learning methods across various downstream molecule property prediction tasks.",
        "reference": "The authors develop a graph neural network model that breaks a module up into fragments over non-ring single bonds, and trains a both a contrastive bag-of-fragments objective as well as prediction of the torsion angle of the respective broken bond. I appreciated the number of baselines presented, and I think the proposed model results are rather good. I really like that the objective is straightforward and pragmatic. The approach is straightforward and more generally applicable. Since the model inputs and objectives are quite interpetable, I think Section E (Table 9) is useful as an ablation study. Questions:  What is the distribution of 3D bond angles used in the objective? Was only a single bond angle used, and where was this generated? Would it be possible to learn using the energetically favorable distribution of bond angles? The loss for the torsion angle is interesting and pragmatic. However, by using a binned loss for the torsion angles, it is not radially symmetric, and may not be ideal. Can the angle loss be parameterized with a von Mises distribution? Figure 1 paragraph description is a bit confusing. It is unclear upon the first read that the words in parenthesis correspond to one another (incomplete / negative). What is the ratio of hard to soft negatives in the objective? How is that determined or optimized? I find the approach novel, pragmatic, and of general interest. I also find the work well written and easy to understand. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 8: accept, good paper 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 191,
        "instruction": "Sharp Convergence Analysis of Gradient Descent for Deep Linear Neural Networks: This paper provides sharp rates of convergence of the gradient descent (GD) method for deep linear neural networks with different random initialization. This study touches upon one major open theoretical problem in machine learning: why deep neural networks trained with GD methods are efficient in many practical applications. While the solution of this problem is still beyond reach for general nonlinear deep neural networks, there have been extensive efforts in the literature in studying relevant questions for deep linear neural networks and there are many interesting results in this research direction. For example, recent results on the loss landscape show that even though the loss function of deep linear neural networks is non-convex, every local minimizer is also a global minimizer. When the GD method is applied to train the deep linear networks, it has been shown in the literature that the convergence behavior of the GD method depends on the initialization. In this paper, we obtain the sharp rate of convergence of GD for deep linear networks, and we show that this rate does not depend on the types of random initialization. Furthermore, we show that the depth of the network does not affect the optimal rate of convergence, provided that the width of each hidden layer is appropriately large. ",
        "reference": "This paper studies the convergence rates of gradient descent (GD) on minimizing the training loss L(WN\u22efW1) of deep linear networks, where L(W) is the convex training loss function of the corresponding linear model (i.e., the loss when the product WN\u22efW1 is collapsed into a single matrix W). We assume that L(W) is strongly convex and smooth in the subspace spanned by the data points. The paper considers the following three initialization schemes  Gaussian initialization; One peak random orthogonal projections and embeddings initialization (which generalizes the orthogonal initialization proposed by Hu et al. (2020)); Special balanced initialization (which is a special case of balanced initialization considered in Arora et al. (2018a)).  For these schemes, the paper proves that, for sufficiently wide networks,  GD converges linearly, and the convergence rate is of the same order as minimizing L(W) as a function of W (Theorems B.1, B.2, and B.3; Theorems 3.1 and 3.2 in the main text are special cases of these theorems). The trajectory of the product WN(t)\u22efW1(t) as we minimize L(WN\u22efW1) in fact stays close to the trajectory of the W(t) as we minimize L(W) with an appropriately rescaled learning rate (Theorem 3.3). Convergence of optimization methods on training linear neural networks is an important area as it can provide valuable intuitions for understanding nonlinear networks. The paper generalizes existing results and presents the main results clearly. The O(\u03balog\u20611/\u03b5) iteration complexity to achieve \u03b5-suboptimality looks indeed sharp because it matches the convergence rate of GD on the convex counterpart. Nevertheless, I should mention that the O(\u03balog\u20611/\u03b5) complexity was also achieved by some previous results such as Du and Hu (2019) and Hu et al. (2020).  The next main result that the trajectory of the product WN(t)\u22efW1(t) closely follows the trajectory of W(t) as we minimize the corresponding convex function L(W) is something that I haven't seen in the literature, unless I missed some existing results. This part is quite intriguing as it establishes that optimizing linear NN follows a similar trajectory as the corresponding convex problem, while the problem itself is nonconvex. This observation can deliver useful insights to the community. While I like the observation made in Theorem 3.3, for the rest of the main results (Theorems B.1, B.2, and B.3), I got the impression that the contributions made by this paper may be somewhat limited. As pointed out above, for Gaussian and orthogonal initializations, the sharp rate O(\u03balog\u20611/\u03b5) was already achieved by Du and Hu (2019) and Hu et al. (2020).  A quick perusal of the proof reveals that the paper also builds on the techniques developed by these two existing papers. Remark 5 states that for the case where all hidden layers have the same width, Theorems B.1 and B.2 recover the main results of these papers. The \"convergence region\" established in Lemma D.1 almost exactly follows the conditions developed in the two papers. Appendix E also seems to follow the flow of the proof in Du and Hu (2019). These observations make me question if Theorems B.1 and B.2 are merely a technical extension of the existing results on \"hidden layers having identical widths\" to just \"hidden layers with general widths.\" I know this could be a false impression as I didn't go through the proof carefully; I would be happy if the authors prove me wrong. Anyway, this concern of novelty makes me hesitate to recommend acceptance at the moment. Importantly, the main text of this paper does not make any precise comparisons against the existing results. Can you elaborate/highlight the technical challenges that had to be overcome in extending the existing results to get Theorems B.1 and B.2? The paper is well-written and delivers the main results clearly. I found the paper quite enjoyable to read.  In terms of novelty, the observation made in Theorem 3.3 (Lemmas D.3\u2013D.5) looks novel to me, but as noted above, the remaining elements of the proof seem to rely upon existing results.  Some minor issues:  In Eq (4), the RHS has to have a leading \u03b22 factor? In Definition 2.1, niIni -> niIni\u22121 and nj\u22121Inj\u22121 -> nj\u22121Inj? In the definition of special balanced initialization, there is no mention on how V1 is defined? Also I thought VN=UN\u22121 should also hold here? In the statements of Theorem 3.1 and 3.2, it looks weird that \u03b4 does not show up anywhere in the stated bound. In Theorem 3.3, it would be useful to mention that q\u2208(0,1) for the step size of interest? The plots in Figure 1 should better be drawn in \"semilogy\" style? I enjoyed reading the paper and I think the results presented in the paper deliver valuable insights. However, at the same time, it may be the case that some of the main theorems rely too heavily on some existing results and hence are of limited novelty. In the rebuttal, it would be very helpful if the authors could clarify the technical barriers that had to be overcome in carrying out the extensions. I would be happy to raise my score if my concerns get resolved. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. Not applicable NO. 5: marginally below the acceptance threshold 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 192,
        "instruction": "Contextualized Generative Retrieval: The text retrieval task is mainly performed in two ways: the bi-encoder approach and the generative approach. The bi-encoder approach maps the document and query embeddings to common vector space and performs a nearest neighbor search. It stably shows high performance and efficiency across different domains but has an embedding space bottleneck as it interacts in L2 or inner product space. The generative retrieval model retrieves by generating a target sequence and overcomes the embedding space bottleneck by interacting in the parametric space. However, it fails to retrieve the information it has not seen during the training process as it depends solely on the information encoded in its own model parameters. To leverage the advantages of both approaches, we propose Contextualized Generative Retrieval model, which uses contextualized embeddings (output embeddings of a language model encoder) as vocab embeddings at the decoding step of generative retrieval. The model uses information encoded in both the non-parametric space of contextualized token embeddings and the parametric space of the generative retrieval model. Our approach of generative retrieval with contextualized vocab embeddings shows higher performance than generative retrieval with only vanilla vocab embeddings in the document retrieval task, an average of 6% higher performance in KILT (NQ, TQA) and 2X higher in NQ-320k, suggesting the benefits of using contextualized embedding in generative retrieval models.",
        "reference": "The proposed model targets a Wikipedia page-level retrieval task: given a query, return the page (or in their case, the Wikipedia title following GENRE). Like GENRE, the authors use a sequence-to-sequence model, take the query text as input, and directly generate the title in an autoregressive manner. The innovation is instead of using static token embeddings in autoregressive decoding, the proposed model uses contextual embeddings, which are acquired by encoding all tokens in possible titles (with the Wikipedia content as context). The embeddings naturally contain both the token information like the static word embeddings and the passage information through the encoder. Arguably, the new model has both the power of autoregressive generation and the external knowledge of Wikipedia through contextual embeddings. The authors explored several variants. First, to compress the huge number of contextual embeddings, they conduct clustering for each token\u2019s corresponding embeddings and keep k=5 clusters. The authors also have different versions where the contextual embeddings are fixed at the beginning (CGR-base), the embeddings are updated by the updated seq2seq model (CGR-async), and the embeddings are jointly trained through contrastive learning (CGR-contra), though the differences are small.  The authors mainly conduct experiments on KILT and NQ320K, and mainly compare their model to the direct baseline, GENRE. The results show that CGR (the proposed model) significantly outperformed GENRE on KILT (especially in-domain datasets NQ and TriviaQA), and outperformed a series of other autoregressive retrieval models on NQ320K. However, one questionable point: GENRE uses BART and CGR uses T5, but the authors only compared to the original GENRE numbers in KILT experiments instead of reproducing GENRE with the same setup, which might lead to unfair comparison. There should also be other reproduced baselines like DPR (or any dense retrieval methods) and SEAL in KILT for a more comprehensive understanding. Strength The proposed idea is novel: it combines the advantage of both autoregressive generation (GENRE, DSI, SEAL) and contextual embeddings of databases (DPR). The use of token contextual embeddings is also smart since it can be compressed by the clustering algorithm, without too much loss in information. The study of different variants of training is comprehensive. Experiment results (if trustworthy) are strong. Weakness The main weakness comes from the experiment setup. The main KILT results only have the original GENRE (using a different pre-trained model, BART) and CGR (even though the authors listed DPR and SEAL, the numbers are not in the same comparable settings). The authors should provide a reproduced GENRE with the same pre-trained model and the same training recipe, as well as other baselines like BM25, a dense retrieval method (like DPR), and, if possible, SEAL. I have some other concerns in Table 2 and Table 3. In Table 2, GENRE, DSI, and SEAL\u2019s performance is so low on NQ-320K. I would appreciate it if the authors could clarify the setting (sorry if I missed it) or maybe retrain the GENRE on NQ-320K in the same setting as CGR (it shouldn\u2019t be difficult since the code should be similar to CGR). The proposed method is novel and intuitive. The authors could improve the clarity \u2014 e.g., the authors can make it clearer what text they put into the encoders for the contextual embeddings (in the paper they only vaguely mention that they are titles). Figure 1 is a bit confusing to me (especially Cape(1), Cape(2)). The experiment setup can be significantly improved. From the current experiments, it is hard to draw a clear conclusion. The proposed method -- using contextual embeddings instead of static token embeddings in decoder for autoregressive retrieval -- is novel, intuitive, and inspiring. However, the experiment has significant flaws (for the most direct and important baseline, the authors took numbers from the original implementation that uses even a different pre-trained model, which leads to unfair comparison). I am leaning toward rejection. However, if the authors can update the results with the reproduced GENRE baseline, I am willing to raise my score. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 193,
        "instruction": "Mirror Training for Input Convex Neural Network:     The input convex neural network (ICNN) aims to learn a convex function from the input to the output by using non-decreasing convex activation functions and non-negativity constraints on the weight parameters of some layers. However, in practice, it loses some representation power because of these non-negativity parameters of the hidden units, even though the design of the ``passthrough'' layer can partially address this problem. To solve issues caused by these non-negativity constraints, we use a duplication input pair trick, i.e., the negation of the original input as part of the new input in our structure. This new method will preserve the convexity of the function from the original input to the output and tackle the representation problem in training. Additionally, we design a mirror unit to address this problem further, making the network Mirror ICNN. Moreover, we propose a recurrent input convex neural network (RICNN) structure to deal with the time-series problems. The recurrent unit of the structure can be ICNN or any other convex variant of ICNN. This structure can maintain convexity by constraining the mapping from the hidden output at time step $t$ to the input of the next time step $t+1$. The experiments can support our design, including the simple numerical curve fitting, power system hosting capacity dataset regression, and the MNIST dataset classification.",
        "reference": "This paper proposes a modified input convex neural network structure and demonstrated the performance of the proposed structure in learning single-input single-output convex functions, hosting capacity dataset, and MNIST. Strength: The topic of studying input convex neural networks is interesting and applies to many real-world applications. Weakness:  The paper is poorly written and many statements are wrong. For example, One of the big motivations for this paper as mentioned in the Introduction part of the paper is \"These non-negative weights can maintain the convexity from the input to the output but also brings the problem of the lack of representation power... To tackle the challenge in representation, Chen et al. (2018) concatenate the negation of the original input with itself, making it the new input of the network. This method can theoretically get more representation power because of the duplication input pair, but the actual training process does not work as expected. \" <-- this statement is wrong.  The original input convex neural network paper [Amos et al. 2017] and the modified ICNN structure paper [Chen et al. 2018] both have the representation power to represent all convex functions. As mentioned by [Chen et al. 2018] page 4, \"our construction of ICNN in Proposition 1 is motivated by [12] but modified to be more suitable to control of dynamical systems... Our construction achieves the exact representation as [12]...\" [Chen et al. 2018] introduced the duplication trick because they not only want the function to be convex, but they need the composition of ICNN functions to be still convex - which is important for dynamical system modeling where the network needs to be \u201crolled out in time\u201d.   If the objective of ICNN is not for dynamical system control - like in all the experiments shown in this paper, from fitting convex curves, fitting the hosting capacity, and MNIST. I don't see a need for not using the original ICNN structure in [Amos et al. 2017]; also the proposed structure does not seem to provide any better representation-wise or training-wise benefits compared to [Amos et al. 2017].  The Experiment section feels pretty sloppy and rushed. All experiments are toy-scale problems not meeting the bar of a top AI conference.   The experiment results are also mysterious and not convincing. For example, For the MNIST experiment 4.3, why NN has a worse performance than ICNN? NN has much better representation power and a simple search on Google will give you a trained NN that can obtain 99% accuracy in the test set. Also, if you look at Fig 6, does the training even converge? Same for Figure 7, the LSTM curve - the training loss is still fast decreasing.  Also, the major content in the Method parts are from previous papers, e.g., Proposition 1 is from [Amos et al. 2017], and the negation method is from [Chen et al. 2018]. The technical contribution of the paper seems quite marginal. The paper has major technical flaws; the major statement for method motivation and contribution is incorrect and the experimental evaluation is flawed and fails to adequately support the main claims. This paper proposes a modified ICNN structure, with empirical results in learning convex functions, power systems and MNIST. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 1: The contributions are neither significant nor novel. 1: The contributions are neither significant nor novel. NO. 1: strong reject 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 194,
        "instruction": "Oscillation Neural Ordinary Differential Equations: Neural ordinary differential equations (NODEs) have received a lot of attention in recent years due to their memory efficiency. Different from traditional deep learning, it defines a continuous deep learning architecture based on the theory of ordinary differential equations (ODEs), which also improves the interpretability of deep learning. However, it has several obvious limitations, such as a NODE is not a universal approximator, it requires a large number of function evaluations (NFEs), and it has a slow convergence rate. We address these drawbacks by modeling and adding an oscillator to the framework of the NODEs. The oscillator enables the trajectories of our model to cross each other. We prove that our model is a universal approximator, even in the original input space. Due to the presence of oscillators, the flows learned by the model will be simpler, thus our model needs fewer NFEs and has a faster convergence speed. We apply our model to various tasks including classification and time series extrapolation, then compare several metrics including accuracy, NFEs, and convergence speed. The experiments show that our model can achieve better results compared to the existing baselines.",
        "reference": "The paper suggests adding jumping terms to the training of Neural ODEs to improve training and computational time. The conduct an analysis of the proposed model and several experimental studies. Strengths:  The paper identifies three main challenges in current ODEs and proposes a method to alleviate them.  Empirically the method seems to work.   Weaknesses:  The figures quality and clarity need to be improved. They do not look professional and are hard to understand.  One of the problems that the authors state that exists in current Neural ODEs is \"The reason for the second limitation is caused by the straightforward optimization approach of the vector field. There is no guarantee that learning the vector field is a better choice than learning the estimated functions directly. Sometimes it will need many function evaluations to optimize the vector field, so the difficulties go beyond learning the estimated functions themselves\" in page 1. However no citations or self explanation is given in the paper.  The authors are lacking reference and discussion of prior work, for example \"Stable Architectures for Deep Neural Networks\".  I am not sure why the authors call the method 'oscillatory'. Typically this is a term that is reserved to describe hyperbolic or wave like dynamical systems. However here the authors propose to introduce 'jumps' into the trajectories.  The experimental scope is rather narrow and it is hard to draw conclusions from experiments on small datasets like CIFAR-10 and CIFAR-100. I believe that experiments with larger and standard datasets like ImageNet can be more convincing. Also, the authors should compare their obtained accuracy with other CNNs that are not necessarily ODE based, for a fair comparison.  A question to the authors: Why do you report the validation accuracy on CIFAR-10 and CIFAR-100 ? the norm is to report the test accuracy. Clarity: The paper is mostly clear but the figures need to be revised. Quality: The discussion of prior work in the field is lacking, and the experiments are rather narrow and not sufficient to draw conclusions. Novelty: The method seems to be new. Reproducibility: From the text of the paper I could not reproduce the results. Crucial details like the chosen architecture and hyper parameters are not provided in the text. The paper identifies problems in current Neural ODEs but fails to discuss relevant prior work and to fully evaluate their method. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 2: The contributions are only marginally significant or novel. 1: The contributions are neither significant nor novel. NO. 3: reject, not good enough 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
    },
    {
        "id": 195,
        "instruction": "Automatically Answering and Generating Machine Learning Final Exams: Can a machine learn machine learning? We propose to answer this question using the same criteria we use to answer a similar question: can a human learn machine learning? We automatically answer final exams in MIT's recent large machine learning course and generate new questions at a human level. Recently, program synthesis and few-shot learning solved university-level problem set questions in mathematics and STEM courses at a human level. In this work, we solve questions from final exams that differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We provide a new dataset and benchmark of questions from machine learning final exams and code for automatically answering these questions and generating new questions. To make our dataset a reproducible benchmark, we use automatic checkers for multiple choice questions, questions with numeric answers, and questions with expression answers, and evaluate a large free language model, Meta\u2019s OPT, and compare the results with Open AI\u2019s GPT-3 and Codex. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning, chain-of-thought prompting, GPT-3 and OPT pre-trained on text and Codex fine-tuned on code on a range of machine learning topics and find that few-shot learning methods perform best. We make our data and code publicly available for the machine learning community.",
        "reference": "The authors provide a new dataset and benchmark of questions from machine learning final exams and code for automatically answering these questions and generating new questions. They build baselines by applying zero-shot and few-shot learning to GPT-3 and Codex, adding chain-of-thought prompting for GPT-3. Strength:  A quite interesting dataset Solid baselines  Weakness:  Seems like one more dataset from BIG-bench, https://github.com/google/BIG-bench . Not too much novelty The paper is not well-written. Not sure why Figure 1 is shown in the paper. It only shows students taking the final exam which is not related to the dataset. What's the difference between the proposed dataset and the BIG-bench datasets? I think this is an interesting dataset, but it is like one dataset from BIG-bench. I think we don't need every dataset from BIG-bench to be published. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 196,
        "instruction": "CAT: Collaborative Adversarial Training: Adversarial training can improve the robustness of neural networks. Previous adversarial training methods focus on a single training strategy and do not consider the collaboration between different training strategies. In this paper, we find different adversarial training methods have distinct robustness for sample instances. For example, an instance can be correctly classified by a model trained using standard adversarial training (AT) but not by a model trained using TRADES, and vice versa.  Based on this phenomenon, we propose a collaborative adversarial training framework to improve the robustness of neural networks. Specifically, we simultaneously use different adversarial training methods to train two robust models from scratch. We input the adversarial examples generated by each network to the peer network and use the logit of the peer network to guide the training of its network. Collaborative Adversarial Training (CAT) can improve both robustness and accuracy. Finally, Extensive experiments on CIFAR-10 and CIFAR-100 validated the effectiveness of our method.\nCAT achieved new state-of-the-art robustness without using any additional data on CIFAR-10 under the Auto-Attack benchmark.",
        "reference": "The authors made an observation that different adversarial defense strategies make different mistakes. Based on this observation, they proposed collaborative adversarial training, where simultaneously train two robust models. The objective of collaborative adversarial training is to minimize the symmetric KL-divergence between the logits of the first and second models. In the experiments, the authors showed improved robustness against AutoAttack on CIFAR-10 and CIFAR-100 datasets. Strengths  Proposed a framework for collaborative adversarial training, where two robust models are trained jointly. Demonstrated promising experimental results on CIFAR-10 and CIFAR-100 benchmarks.  Weaknesses  The increased computational cost of training two robust models. The lack of a theoretical analysis of the proposed method. It is not clear why the proposed objective will guide the training toward a more robust model compared to the single model, single attack, and single objective training. The lack of more detailed experimental analysis and other baselines for comparison.  Questions  Can collaborative adversarial training be affected by catastrophic/robust overfitting? Does collaborative adversarial training perform better than training one robust model with multiple attacks and/or multiple weighted objectives? Can you include experiments with more than 2 defenses? The paper is easy to understand and follow. The authors should proofread the paper to remove some of the typos (e.g. \"the TARDES-trained network). The idea of combining multiple defenses is not novel. There are several methods for training an ensemble of adversarially-trained models to improve robustness [1, 2], which were not cited nor compared with. The authors should relate the proposed method to ensemble methods for adversarial training. In comparison, the authors should consider using some of the more recent and stronger baselines for comparison.  [1] Tram\u00e8r, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., & McDaniel, P. (2018). Ensemble adversarial training: Attacks and defenses. In , International Conference on Learning Representations (pp. ). : . [2] Pang, T., Xu, K., Du, C., Chen, N., & Zhu, J. (2019). Improving adversarial robustness via promoting ensemble diversity. In K. Chaudhuri, & R. Salakhutdinov, Proceedings of the 36th International Conference on Machine Learning (pp. 4970\u20134979). Long Beach, California, USA: PMLR. The authors proposed a collaborative adversarial training method that combines two defense methods. The approach is interesting and marginally novel. It improves robustness upon TRADES and ALP baselines. However, the paper contains many typos, the empirical comparison does not include more recent SOTA methods, and some citations (such as ensemble adversarial training) are missing. 4: All of the claims and statements are well-supported and correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 197,
        "instruction": "A Benchmark Dataset for Learning from Label Proportions: Learning from label proportions (LLP) has recently emerged as an important technique of weakly supervised learning on aggregated labels. In LLP, a model is trained on groups (a.k.a bags) of feature-vectors and their corresponding label proportions to predict labels for individual feature-vectors. While previous works have developed a variety of techniques for LLP, including novel loss functions, model architectures and their optimization, they typically evaluated their methods on pseudo-synthetically generated LLP training data using common small scale supervised learning datasets by randomly sampling or partitioning their instances into bags.  Despite growing interest in this important task there are no large scale open source LLP benchmarks to compare various approaches. Construction of such a benchmark is hurdled by two challenges a) lack of natural large scale LLP like data, b) large number of mostly artificial methods of forming bags from instance level datasets. \nIn this paper we propose LLP-Bench: a large scale LLP benchmark constructed from   the Criteo Kaggle CTR dataset. We do an in-depth, systematic study of the Criteo dataset and propose a methodology to create a  benchmark as a collection of diverse and large scale LLP datasets. We choose the Criteo dataset since it admits multiple natural collections of bags formed by grouping  subsets of its 26 categorical features. We analyze all bag collections obtained through grouping by one or two categorical features, in terms of their bag-level statistics as well as embedding based distance metrics quantifying the geometric separation of bags. We then propose to include in LLP-Bench a few groupings to fairly represent real world bag distributions.\nWe also measure the performance of state of the art models, loss functions (adapted to LLP) and optimizers on LLP-Bench. We perform a series of ablations and explain the performance of various techniques  on LLP-Bench. To the best of our knowledge LLP-Bench is the first open source benchmark for the LLP task. We hope that the proposed benchmark and the evaluation methodology will be used by ML researchers and practitioners to better understand and hence devise state of art LLP algorithms. ",
        "reference": "This paper focuses on creating a benchmark as a collection of diverse and large-scale LLP datasets based on the Criteo Kaggle CTR dataset. The authors obtained all bag sets grouped by one or two categorical features and applied two filters on the clipped groupings to choose groupings for model training. They estimated the geometric clustering of bags and qualified the tractability of an LLP dataset by the test AUC scores. Pros:  The authors considered establishing a benchmark dataset for LLP, which did not exist in previous studies.  Cons:  It is a bit confusing why to choose the Criteo Kaggle CTR dataset and why the dataset groups in this way. First of all, in the Criteo Kaggle CTR dataset, the semantics of these 26 category features are undisclosed and missing for some samples, so the direct use of category features for subcontracting requires a more reasonable explanation. Secondly, why using at most two categorical features for grouping also needs a reasonable explanation. In LLP, the natural grouping of bags can usually be based on geographic location, time point, age group, etc. It may be more appropriate to select a dataset with these kinds of information. In the study of LLP, the performance of various techniques on bags of different sizes is also very important. The reason for removing bags with a size greater than 2500 or less than 50 needs to be explained. Grouping based on category features for the Criteo Kaggle CTR dataset has existed in the work of Saket et al. (2022). This is slightly insufficient as the innovation point of this paper. The authors used the test AUC score to qualify the tractability of the LLP dataset, but the result did not seem to be ideal (AUC scores were centered around 60-80%). It would be nice to add some experiments that use SOTA methods, such as LLP-VAT (Tsai et al. 2020) or MCM (Scott et al. 2020), to illustrate that the dataset makes sense. The presentation of this paper is not so clear and needs improvement. The purpose of this paper is to build a large-scale benchmark for LLP, which is the innovation of this paper. But the reason for using the Criteo Kaggle CTR dataset and grouping in this way needs a clearer explanation. At the same time, directly using the existing category labels of the Criteo Kaggle CTR dataset for generating bags is insufficiently innovative. Whether this dataset can contribute to the field of LLP requires more experimental verification. The novelty of this paper is insufficient. It still has room for improvement. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 2: The contributions are only marginally significant or novel. NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 198,
        "instruction": "Emergence of Exploration in Policy Gradient Reinforcement Learning via Resetting: In reinforcement learning (RL), many exploration methods explicitly promote stochastic policies, e.g., by adding an entropy bonus. We argue that exploration only matters in RL because the agent repeatedly encounters the same or similar states, so that it is beneficial to gradually improve the performance over the encounters; otherwise, the greedy policy would be optimal. Based on this intuition, we propose ReMax, an objective for RL whereby stochastic exploration arises as an emergent property, without adding any explicit exploration bonus. In ReMax, an episode is modified so that the agent can reset to previous states in the trajectory, and the agent\u2019s goal is to maximize the best return in the trajectory tree. We show that this ReMax objective can be directly optimized with an unbiased policy gradient method. Experiments confirm that ReMax leads to the emergence of a stochastic exploration policy, and improves the performance compared to RL with no exploration bonus.",
        "reference": "This paper introduces a new setting which utilizes resetting to previously visited states in the trajectory and continuing on from there. The objective is then defined as the max return over the resulting trajectory tree. The paper shows that directly optimizing for this objective results in stochastic, exploratory policies. Strengths  The paper shows the emergence of stochastic policies when resetting the agent to previous states in the trajectory and using the ReMax return. This is an interesting contribution, and could lead to future research directions in how to develop exploration policies.  Weaknesses  The motivation of this paper is that directly optimizing for this objective results in a stochastic exploration policy. It would be helpful to see how this policy actually works as an exploration policy used to train another policy that is trying to optimize the original RL objective.  The paper compares against other methods that don't reset the simulator during training. The comparison is done based on the number of steps taken in the environment. I am not sure this is a valid comparison. Resetting the environment involves hacking the environment. Other work that hack the environment in such a fashion do so to study a phenomenon that emerges (Go-explore) which is fine. If you're comparing to other methods that don't hack the environment, you need to make allowances to account for that, since doing so is not really possible for most problems of interest. For example, if the agent resets to state A, it automatically skips the steps that another agent would need to take to get to state A. A non resetting agent would almost necessarily be at a disadvantage in terms of sample complexity if thats how the environment steps were counted. Another useful baseline to compare against could be where the agent is reset, but the return is calculated as normal. This could also show whether the max operator is needed. The paper is fairly clear and novel. The overall quality is also good, but I think the experiments can be improved/reframed. I think this is an interesting idea, and should eventually be published, but I think it is not ready in its current state. Its contributions need to be reframed, certain baselines need to be added, and the way the experimental results are presented need to be changed. 2: Several of the paper\u2019s claims are incorrect or not well-supported. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. Not applicable NO. 3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 199,
        "instruction": "SciRepEval: A Multi-Format Benchmark for Scientific Document Representations: Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models.  We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them.  However, a new approach that learns multiple embeddings per document, each tailored to a different task format, can improve performance.\nWe experiment with task-format-specific control codes and adapters in a multi-task setting and find that they outperform the existing single-embedding state-of-the-art by up to 1.5 points absolute. ",
        "reference": "This paper introduces a new benchmark (SciRepEval) for scientific representation learning consisting of 25 tasks in 4 formats (classification, regression, ranking, and search). It shows that learning a separate document representation for each task format would improve the task performance compared to learning a single representation for all tasks. Strength  The authors provide a comprehensive analysis on 25 tasks in 4 formats.  The authors use strong baseline models to make the results more convincing.  Weakness  There is not enough information on the tasks in the main content. Would be better to provide some high-level info there and left the majority in the Appendix. The paper is well written with high clarity and above-average quality. Some concerns on the Novelty, since the paper doesn't go deep on the technique. This paper is written in above-average quality and provides a new benchmark with a comprehensive analysis 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 2: The contributions are only marginally significant or novel. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. NO. 6: marginally above the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    },
    {
        "id": 200,
        "instruction": "On the convergence of SGD under the over-parameter setting: With the improvement of computing power, over-parameterized models get increasingly popular in machine learning. This type of model is usually with a complicated, non-smooth, and non-convex loss function landscape. However, when we train the model, simply using the first-order optimization algorithm like stochastic gradient descent (SGD) could acquire some good results, in both training and testing, albeit that SGD is known to not guarantee convergence for non-smooth and non-convex cases. Theoretically, it was previously proved that in training, SGD converges to the global optimum with probability $1 - \\epsilon$, but only for certain models and $\\epsilon$ depends on the model complexity. It was also observed that SGD tends to choose a flat minimum, which preserves its training performance in testing. In this paper, we first prove that SGD could iterate to the global optimum almost surely under arbitrary initial value and some mild assumptions on the loss function. Then, we prove that if the learning rate is larger than a value depending on the structure of a global minimum, the probability of converging to this global optimum is zero. Finally, we acquire the asymptotic convergence rate based on the local structure of the global optimum. ",
        "reference": "This paper provides theoretical results for the asymptotic convergence of SGD algorithm under an over-parameterized setting. It shows a set of assumptions that can guarantee the global convergence of SGD almost surely in some non-convex setting. The strengths of this paper are the theoretical results for the global convergence of SGD almost surely. The authors consider the regular sampling scheme and propose a new scheme (sampling noise with global stable guarantee). They prove the asymptotic convergence for SGD under a set of assumptions.  The weaknesses of this paper are:   This paper did not explain the intuition why SGD converge globally very well. The assumptions are not clearly motivated. The authors should explain why they have two set of assumptions on the gradient/ sample gradient of g. One is Assumption 2.1 part 4, line 2 where there is a lower bound on the liminf of gradient, the other is Assumption 2.2 where we put an upper bound on the sample gradient. Please add a discussion why the theory needs these assumptions and make sure they do not contradict each other. In addition, Assumption 2.3 is not natural when it asks that the constants c\u03b8,c^\u03b8 are bounded away from 0 and by a constant of \u03b8.  The presentation of this paper is poor. There are many notations and variables that were mentioned before the authors define them in the draft. For example: M0 and a were referred in Assumption 2.1 but only defined until 2.2, \u2207~g has no definition, 'global stable guarantee' was referred before the explanation,... Most of the time, the sketch proofs are confusing and they did not help to understand the thought process to prove the theorems.  Question: In Theorem 3.4, what is 'the variant of Assumption 2.3 described immediately preceding this statement'? The results of this paper seem to be new and the approach is different from prior work. However, its clarity and presentation is not good. Although this paper show interesting results, I am not fully convinced by the assumptions and the intuition/reasoning behind the proofs. 3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct. 3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work. Not applicable NO. 5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
    }
]